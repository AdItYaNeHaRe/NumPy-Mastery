{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f063c2e-0550-458a-a35d-db492e8f04ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea447266-e5be-4a5a-8d7f-18f1baa71008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 6 7 8] <class 'numpy.ndarray'> int32 (8,)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# NUMPY ARRAY CREATION AND ATTRIBUTES\n",
    "# =============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# FUNCTION: np.array()\n",
    "# -----------------------------------------------------------------------------\n",
    "# ENGLISH:\n",
    "# - What it does: Creates a NumPy array (ndarray) from a Python list, tuple, \n",
    "#   or other array-like objects. This is the most common way to create arrays.\n",
    "# - Why use it: NumPy arrays are faster and more memory-efficient than Python \n",
    "#   lists for numerical operations. They support vectorized operations and \n",
    "#   broadcasting.\n",
    "# - Parameters: \n",
    "#   * object: Array-like input (list, tuple, etc.)\n",
    "#   * dtype: (optional) Data type of array elements (int, float, etc.)\n",
    "# - Returns: ndarray object\n",
    "# - Common use cases:\n",
    "#   * Converting Python lists to NumPy arrays for ML/data processing\n",
    "#   * Creating feature vectors, training data, or weight matrices\n",
    "#   * Performing mathematical operations on collections of numbers\n",
    "# - Example:\n",
    "#   arr1 = np.array([1, 2, 3])           # 1D array\n",
    "#   arr2 = np.array([[1, 2], [3, 4]])    # 2D array (matrix)\n",
    "#   arr3 = np.array([1.5, 2.5], dtype=int)  # Specify data type\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Python list, tuple ya kisi bhi array-jaisi cheez se \n",
    "#   NumPy array (ndarray) banata hai. Array banana ka yeh sabse common tarika hai.\n",
    "# - Kab use karein: NumPy arrays Python lists se zyada fast aur memory-efficient\n",
    "#   hote hain numerical operations ke liye. Yeh vectorized operations aur \n",
    "#   broadcasting support karte hain.\n",
    "# - Parameters:\n",
    "#   * object: Array jaisa input (list, tuple, etc.)\n",
    "#   * dtype: (optional) Array elements ka data type (int, float, etc.)\n",
    "# - Returns: ndarray object\n",
    "# - Common use cases:\n",
    "#   * Python lists ko NumPy arrays mein convert karna ML/data processing ke liye\n",
    "#   * Feature vectors, training data, ya weight matrices banana\n",
    "#   * Numbers ke collections par mathematical operations karna\n",
    "# - Example:\n",
    "#   arr1 = np.array([1, 2, 3])           # 1D array\n",
    "#   arr2 = np.array([[1, 2], [3, 4]])    # 2D array (matrix)\n",
    "#   arr3 = np.array([1.5, 2.5], dtype=int)  # Data type specify karna\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "arr = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ATTRIBUTE: type()\n",
    "# -----------------------------------------------------------------------------\n",
    "# ENGLISH:\n",
    "# - What it does: Returns the class/type of the Python object\n",
    "# - Why use it: To verify that you've created a NumPy array and not a regular\n",
    "#   Python list. Useful for debugging and type checking.\n",
    "# - Returns: <class 'numpy.ndarray'> for NumPy arrays\n",
    "# - Common use cases:\n",
    "#   * Debugging: Checking if conversion from list to array worked\n",
    "#   * Type validation before performing NumPy-specific operations\n",
    "#   * Understanding what kind of object you're working with\n",
    "# - Example:\n",
    "#   lst = [1, 2, 3]\n",
    "#   type(lst)  # Returns: <class 'list'>\n",
    "#   arr = np.array(lst)\n",
    "#   type(arr)  # Returns: <class 'numpy.ndarray'>\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Python object ka class/type return karta hai\n",
    "# - Kab use karein: Yeh verify karne ke liye ki aapne NumPy array banaya hai\n",
    "#   ya regular Python list. Debugging aur type checking ke liye useful hai.\n",
    "# - Returns: NumPy arrays ke liye <class 'numpy.ndarray'>\n",
    "# - Common use cases:\n",
    "#   * Debugging: Check karna ki list se array conversion hua ya nahi\n",
    "#   * NumPy-specific operations karne se pehle type validation\n",
    "#   * Samajhna ki aap kis tarah ke object ke saath kaam kar rahe hain\n",
    "# - Example:\n",
    "#   lst = [1, 2, 3]\n",
    "#   type(lst)  # Returns: <class 'list'>\n",
    "#   arr = np.array(lst)\n",
    "#   type(arr)  # Returns: <class 'numpy.ndarray'>\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ATTRIBUTE: .dtype\n",
    "# -----------------------------------------------------------------------------\n",
    "# ENGLISH:\n",
    "# - What it does: Returns the data type of elements stored in the array\n",
    "# - Why use it: Knowing data types is crucial for memory management and \n",
    "#   numerical precision in ML. Different dtypes use different memory amounts.\n",
    "# - Returns: Data type object (e.g., int64, float64, int32, etc.)\n",
    "# - Common use cases:\n",
    "#   * Memory optimization: int8 uses less memory than int64\n",
    "#   * Ensuring correct precision for calculations (float32 vs float64)\n",
    "#   * Preventing type-related bugs in ML models\n",
    "#   * Checking data types before feeding to neural networks\n",
    "# - Example:\n",
    "#   arr_int = np.array([1, 2, 3])        # dtype: int64 (default on 64-bit)\n",
    "#   arr_float = np.array([1.0, 2.0])     # dtype: float64\n",
    "#   arr_specific = np.array([1, 2], dtype=np.float32)  # dtype: float32\n",
    "#   # int8: -128 to 127 (1 byte), int64: much larger range (8 bytes)\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Array mein stored elements ka data type return karta hai\n",
    "# - Kab use karein: Data types jaanna ML mein memory management aur numerical \n",
    "#   precision ke liye bahut zaroori hai. Different dtypes different memory use \n",
    "#   karte hain.\n",
    "# - Returns: Data type object (jaise int64, float64, int32, etc.)\n",
    "# - Common use cases:\n",
    "#   * Memory optimization: int8, int64 se kam memory use karta hai\n",
    "#   * Calculations ke liye sahi precision ensure karna (float32 vs float64)\n",
    "#   * ML models mein type-related bugs se bachna\n",
    "#   * Neural networks ko data dene se pehle data types check karna\n",
    "# - Example:\n",
    "#   arr_int = np.array([1, 2, 3])        # dtype: int64 (64-bit par default)\n",
    "#   arr_float = np.array([1.0, 2.0])     # dtype: float64\n",
    "#   arr_specific = np.array([1, 2], dtype=np.float32)  # dtype: float32\n",
    "#   # int8: -128 se 127 (1 byte), int64: bahut bada range (8 bytes)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ATTRIBUTE: .shape\n",
    "# -----------------------------------------------------------------------------\n",
    "# ENGLISH:\n",
    "# - What it does: Returns a tuple representing the dimensions of the array\n",
    "# - Why use it: Essential for understanding array structure, especially in ML\n",
    "#   where you need to know dimensions for matrix operations, reshaping, and\n",
    "#   ensuring compatibility between layers in neural networks.\n",
    "# - Returns: Tuple of integers (e.g., (8,) for 1D, (3, 4) for 2D)\n",
    "# - Common use cases:\n",
    "#   * Verifying data dimensions before training ML models\n",
    "#   * Debugging shape mismatches in matrix multiplication\n",
    "#   * Reshaping data for neural network input layers\n",
    "#   * Understanding feature dimensions in datasets\n",
    "# - Example:\n",
    "#   arr_1d = np.array([1, 2, 3, 4])           # shape: (4,)\n",
    "#   arr_2d = np.array([[1, 2], [3, 4]])       # shape: (2, 2)\n",
    "#   arr_3d = np.array([[[1, 2]], [[3, 4]]])   # shape: (2, 1, 2)\n",
    "#   # In ML: X_train.shape might be (1000, 784) meaning 1000 samples, \n",
    "#   # 784 features\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Array ke dimensions ko represent karne wala tuple return\n",
    "#   karta hai\n",
    "# - Kab use karein: Array structure samajhne ke liye bahut zaroori hai, \n",
    "#   khaaskar ML mein jahan aapko matrix operations, reshaping, aur neural \n",
    "#   networks mein layers ke beech compatibility ke liye dimensions jaanna \n",
    "#   padta hai.\n",
    "# - Returns: Integers ka tuple (jaise (8,) for 1D, (3, 4) for 2D)\n",
    "# - Common use cases:\n",
    "#   * ML models train karne se pehle data dimensions verify karna\n",
    "#   * Matrix multiplication mein shape mismatches debug karna\n",
    "#   * Neural network input layers ke liye data reshape karna\n",
    "#   * Datasets mein feature dimensions samajhna\n",
    "# - Example:\n",
    "#   arr_1d = np.array([1, 2, 3, 4])           # shape: (4,)\n",
    "#   arr_2d = np.array([[1, 2], [3, 4]])       # shape: (2, 2)\n",
    "#   arr_3d = np.array([[[1, 2]], [[3, 4]]])   # shape: (2, 1, 2)\n",
    "#   # ML mein: X_train.shape (1000, 784) ho sakta hai matlab 1000 samples,\n",
    "#   # 784 features\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(arr, type(arr), arr.dtype, arr.shape)\n",
    "# Output: [1 2 3 4 5 6 7 8] <class 'numpy.ndarray'> int64 (8,)\n",
    "# Explanation: Prints array values, confirms it's ndarray type, shows int64 \n",
    "# data type, and (8,) shape meaning 1D array with 8 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de2ef3ec-b166-43d9-ba4d-69624f16dd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]] (3, 3)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 2D NUMPY ARRAY (MATRIX) CREATION\n",
    "# =============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONCEPT: 2D Array / Matrix Creation\n",
    "# -----------------------------------------------------------------------------\n",
    "# ENGLISH:\n",
    "# - What it does: Creates a 2-dimensional NumPy array (matrix) using nested \n",
    "#   lists. Each inner list becomes a row in the matrix.\n",
    "# - Why use it: 2D arrays are fundamental in ML for representing datasets,\n",
    "#   images, weight matrices in neural networks, and performing linear algebra\n",
    "#   operations like matrix multiplication, dot products, etc.\n",
    "# - Structure: [[row1], [row2], [row3], ...]\n",
    "#   * Each inner list must have the same length (rectangular shape required)\n",
    "#   * Rows are the first dimension, columns are the second dimension\n",
    "# - Common use cases:\n",
    "#   * Storing tabular data (like CSV data with rows and columns)\n",
    "#   * Representing images (though images are often 3D with color channels)\n",
    "#   * Creating weight matrices for neural network layers\n",
    "#   * Linear algebra operations (matrix multiplication, eigenvalues, etc.)\n",
    "#   * Storing feature matrices in ML (rows=samples, columns=features)\n",
    "# - Example:\n",
    "#   # Dataset with 3 samples and 4 features each\n",
    "#   X = np.array([[1.2, 2.3, 3.4, 4.5],    # Sample 1\n",
    "#                 [5.6, 6.7, 7.8, 8.9],    # Sample 2\n",
    "#                 [9.0, 1.1, 2.2, 3.3]])   # Sample 3\n",
    "#   # Shape: (3, 4) means 3 rows, 4 columns\n",
    "#   \n",
    "#   # Grayscale image (5x5 pixels)\n",
    "#   img = np.array([[0, 50, 100, 150, 200],\n",
    "#                   [25, 75, 125, 175, 225],\n",
    "#                   [50, 100, 150, 200, 250],\n",
    "#                   [75, 125, 175, 225, 255],\n",
    "#                   [100, 150, 200, 250, 255]])\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Nested lists use karke 2-dimensional NumPy array \n",
    "#   (matrix) banata hai. Har inner list matrix mein ek row ban jati hai.\n",
    "# - Kab use karein: 2D arrays ML mein bahut fundamental hain - datasets \n",
    "#   represent karne ke liye, images ke liye, neural networks mein weight \n",
    "#   matrices ke liye, aur linear algebra operations jaise matrix \n",
    "#   multiplication, dot products, etc. ke liye.\n",
    "# - Structure: [[row1], [row2], [row3], ...]\n",
    "#   * Har inner list ki length same honi chahiye (rectangular shape zaroori)\n",
    "#   * Rows pehla dimension hain, columns doosra dimension hain\n",
    "# - Common use cases:\n",
    "#   * Tabular data store karna (jaise CSV data with rows aur columns)\n",
    "#   * Images represent karna (lekin images aksar 3D hote hain color channels ke saath)\n",
    "#   * Neural network layers ke liye weight matrices banana\n",
    "#   * Linear algebra operations (matrix multiplication, eigenvalues, etc.)\n",
    "#   * ML mein feature matrices store karna (rows=samples, columns=features)\n",
    "# - Example:\n",
    "#   # Dataset with 3 samples aur har ek mein 4 features\n",
    "#   X = np.array([[1.2, 2.3, 3.4, 4.5],    # Sample 1\n",
    "#                 [5.6, 6.7, 7.8, 8.9],    # Sample 2\n",
    "#                 [9.0, 1.1, 2.2, 3.3]])   # Sample 3\n",
    "#   # Shape: (3, 4) matlab 3 rows, 4 columns\n",
    "#   \n",
    "#   # Grayscale image (5x5 pixels)\n",
    "#   img = np.array([[0, 50, 100, 150, 200],\n",
    "#                   [25, 75, 125, 175, 225],\n",
    "#                   [50, 100, 150, 200, 250],\n",
    "#                   [75, 125, 175, 225, 255],\n",
    "#                   [100, 150, 200, 250, 255]])\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "arr2D = np.array([[1, 2, 3],    # Row 0 (first row)\n",
    "                  [4, 5, 6],    # Row 1 (second row)\n",
    "                  [7, 8, 9]])   # Row 2 (third row)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ATTRIBUTE: .shape for 2D Arrays\n",
    "# -----------------------------------------------------------------------------\n",
    "# ENGLISH:\n",
    "# - What it does: For 2D arrays, returns a tuple (rows, columns) representing\n",
    "#   the matrix dimensions\n",
    "# - Why use it: Critical for understanding data structure in ML. The shape\n",
    "#   tells you how many samples and features you have, or the dimensions of\n",
    "#   weight matrices that need to match for matrix operations.\n",
    "# - Returns: Tuple (rows, columns) e.g., (3, 3) for a 3x3 matrix\n",
    "# - Shape interpretation in ML:\n",
    "#   * (m, n) where m = number of samples/rows, n = number of features/columns\n",
    "#   * For images: (height, width) or (height, width, channels)\n",
    "#   * For weight matrices: (input_features, output_features)\n",
    "# - Common use cases:\n",
    "#   * Verifying dataset dimensions: \"Do I have 1000 samples with 20 features?\"\n",
    "#   * Checking matrix multiplication compatibility: A(m,n) @ B(n,p) = C(m,p)\n",
    "#   * Debugging reshape operations\n",
    "#   * Ensuring data matches model input requirements\n",
    "# - Example:\n",
    "#   X_train = np.array([[1, 2, 3], [4, 5, 6]])  # shape: (2, 3)\n",
    "#   # 2 samples (rows), 3 features (columns)\n",
    "#   \n",
    "#   weights = np.array([[0.1, 0.2],\n",
    "#                       [0.3, 0.4],\n",
    "#                       [0.5, 0.6]])  # shape: (3, 2)\n",
    "#   # Can multiply: X_train @ weights because (2,3) @ (3,2) = (2,2) ✓\n",
    "#   \n",
    "#   img_gray = np.array([[0, 255], [128, 64]])  # shape: (2, 2)\n",
    "#   # 2x2 pixel grayscale image\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: 2D arrays ke liye, tuple (rows, columns) return karta\n",
    "#   hai jo matrix dimensions represent karta hai\n",
    "# - Kab use karein: ML mein data structure samajhne ke liye bahut zaroori hai.\n",
    "#   Shape aapko batata hai ki kitne samples aur features hain, ya weight \n",
    "#   matrices ke dimensions kya hain jo matrix operations ke liye match hone \n",
    "#   chahiye.\n",
    "# - Returns: Tuple (rows, columns) jaise (3, 3) for a 3x3 matrix\n",
    "# - ML mein shape interpretation:\n",
    "#   * (m, n) jahan m = samples/rows ki sankhya, n = features/columns ki sankhya\n",
    "#   * Images ke liye: (height, width) ya (height, width, channels)\n",
    "#   * Weight matrices ke liye: (input_features, output_features)\n",
    "# - Common use cases:\n",
    "#   * Dataset dimensions verify karna: \"Kya mere paas 1000 samples hain with 20 features?\"\n",
    "#   * Matrix multiplication compatibility check karna: A(m,n) @ B(n,p) = C(m,p)\n",
    "#   * Reshape operations debug karna\n",
    "#   * Data ko model input requirements se match karna\n",
    "# - Example:\n",
    "#   X_train = np.array([[1, 2, 3], [4, 5, 6]])  # shape: (2, 3)\n",
    "#   # 2 samples (rows), 3 features (columns)\n",
    "#   \n",
    "#   weights = np.array([[0.1, 0.2],\n",
    "#                       [0.3, 0.4],\n",
    "#                       [0.5, 0.6]])  # shape: (3, 2)\n",
    "#   # Multiply kar sakte hain: X_train @ weights kyunki (2,3) @ (3,2) = (2,2) ✓\n",
    "#   \n",
    "#   img_gray = np.array([[0, 255], [128, 64]])  # shape: (2, 2)\n",
    "#   # 2x2 pixel grayscale image\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(arr2D, arr2D.shape)\n",
    "# Output: \n",
    "# [[1 2 3]\n",
    "#  [4 5 6]\n",
    "#  [7 8 9]] (3, 3)\n",
    "# \n",
    "# Explanation: \n",
    "# - Prints the 2D array showing all 9 elements arranged in 3 rows and 3 columns\n",
    "# - Shape (3, 3) confirms it's a 3x3 square matrix\n",
    "# - First 3 means 3 rows, second 3 means 3 columns\n",
    "# - Total elements = 3 × 3 = 9\n",
    "\n",
    "# =============================================================================\n",
    "# ADDITIONAL IMPORTANT CONCEPTS FOR 2D ARRAYS\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Row vs Column Access:\n",
    "# - arr2D[0] gives first row: [1, 2, 3]\n",
    "# - arr2D[:, 0] gives first column: [1, 4, 7]\n",
    "# - arr2D[1, 2] gives element at row 1, column 2: 6\n",
    "#\n",
    "# Matrix Operations:\n",
    "# - arr2D.T or arr2D.transpose() for transpose (flip rows/columns)\n",
    "# - arr2D @ arr2D for matrix multiplication\n",
    "# - arr2D * arr2D for element-wise multiplication\n",
    "#\n",
    "# HINGLISH:\n",
    "# Row vs Column Access:\n",
    "# - arr2D[0] pehli row deta hai: [1, 2, 3]\n",
    "# - arr2D[:, 0] pehla column deta hai: [1, 4, 7]\n",
    "# - arr2D[1, 2] row 1, column 2 par element deta hai: 6\n",
    "#\n",
    "# Matrix Operations:\n",
    "# - arr2D.T ya arr2D.transpose() transpose ke liye (rows/columns flip)\n",
    "# - arr2D @ arr2D matrix multiplication ke liye\n",
    "# - arr2D * arr2D element-wise multiplication ke liye\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17120e97-1a45-4753-8202-f6ef390fe9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4)\n",
      "12\n",
      "int32\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# NUMPY ARRAY PROPERTIES - COMPREHENSIVE OVERVIEW\n",
    "# =============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CREATING A NON-SQUARE 2D ARRAY FOR PROPERTY EXPLORATION\n",
    "# -----------------------------------------------------------------------------\n",
    "# ENGLISH:\n",
    "# - What it does: Creates a 2D array (matrix) with different number of rows\n",
    "#   and columns (3 rows × 4 columns = non-square matrix)\n",
    "# - Why use it: Non-square matrices are very common in ML - your dataset\n",
    "#   usually has different number of samples than features (e.g., 1000 samples\n",
    "#   with 784 features for MNIST digit images)\n",
    "# - Structure: This is a 3×4 matrix (3 rows, 4 columns)\n",
    "# - Common use cases:\n",
    "#   * Storing datasets where rows=samples, columns=features\n",
    "#   * Weight matrices connecting layers of different sizes in neural networks\n",
    "#   * Transformation matrices that change dimensionality\n",
    "# - Example:\n",
    "#   # Customer data: 3 customers, 4 features (age, income, score, tenure)\n",
    "#   customers = np.array([[25, 50000, 720, 2],\n",
    "#                         [35, 75000, 680, 5],\n",
    "#                         [45, 90000, 750, 10]])\n",
    "#   # Shape: (3, 4) - 3 customers, 4 features each\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Ek 2D array (matrix) banata hai jismein rows aur \n",
    "#   columns ki sankhya alag hai (3 rows × 4 columns = non-square matrix)\n",
    "# - Kab use karein: Non-square matrices ML mein bahut common hain - aapka\n",
    "#   dataset mein usually samples aur features ki sankhya alag hoti hai\n",
    "#   (jaise 1000 samples with 784 features for MNIST digit images)\n",
    "# - Structure: Yeh ek 3×4 matrix hai (3 rows, 4 columns)\n",
    "# - Common use cases:\n",
    "#   * Datasets store karna jahan rows=samples, columns=features\n",
    "#   * Neural networks mein different sizes ki layers ko connect karne ke liye\n",
    "#     weight matrices\n",
    "#   * Transformation matrices jo dimensionality change karti hain\n",
    "# - Example:\n",
    "#   # Customer data: 3 customers, 4 features (age, income, score, tenure)\n",
    "#   customers = np.array([[25, 50000, 720, 2],\n",
    "#                         [35, 75000, 680, 5],\n",
    "#                         [45, 90000, 750, 10]])\n",
    "#   # Shape: (3, 4) - 3 customers, har ek mein 4 features\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "test_arra = np.array([[1, 2, 3, 4],      # Row 0: 4 elements\n",
    "                      [5, 6, 7, 8],      # Row 1: 4 elements\n",
    "                      [9, 10, 11, 12]])  # Row 2: 4 elements\n",
    "\n",
    "# =============================================================================\n",
    "# PROPERTY 1: .shape\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Returns the dimensions of the array as a tuple\n",
    "# - Why use it: Most important property for understanding array structure.\n",
    "#   Essential for debugging shape mismatches in ML operations.\n",
    "# - Returns: Tuple of integers representing size of each dimension\n",
    "#   * For this array: (3, 4) means 3 rows, 4 columns\n",
    "# - How to interpret:\n",
    "#   * 1D array: (n,) - single dimension with n elements\n",
    "#   * 2D array: (m, n) - m rows, n columns\n",
    "#   * 3D array: (l, m, n) - l matrices, each with m rows and n columns\n",
    "#   * nD array: (d1, d2, d3, ..., dn) - n dimensions\n",
    "# - Common use cases:\n",
    "#   * Checking if data matches model input requirements\n",
    "#   * Verifying matrix multiplication compatibility: (m,n) @ (n,p) → (m,p)\n",
    "#   * Understanding data layout before reshaping\n",
    "#   * Debugging \"shape mismatch\" errors in neural networks\n",
    "# - Example:\n",
    "#   arr_1d = np.array([1, 2, 3])                    # shape: (3,)\n",
    "#   arr_2d = np.array([[1, 2], [3, 4], [5, 6]])    # shape: (3, 2)\n",
    "#   arr_3d = np.array([[[1, 2]], [[3, 4]]])        # shape: (2, 1, 2)\n",
    "#   \n",
    "#   # ML example: MNIST dataset\n",
    "#   X_train = np.zeros((60000, 784))  # 60000 images, 784 features (28×28)\n",
    "#   X_train.shape  # Returns: (60000, 784)\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Array ke dimensions ko tuple ke roop mein return karta hai\n",
    "# - Kab use karein: Array structure samajhne ke liye sabse important property.\n",
    "#   ML operations mein shape mismatches debug karne ke liye essential hai.\n",
    "# - Returns: Integers ka tuple jo har dimension ka size represent karta hai\n",
    "#   * Is array ke liye: (3, 4) matlab 3 rows, 4 columns\n",
    "# - Kaise interpret karein:\n",
    "#   * 1D array: (n,) - ek hi dimension with n elements\n",
    "#   * 2D array: (m, n) - m rows, n columns\n",
    "#   * 3D array: (l, m, n) - l matrices, har ek mein m rows aur n columns\n",
    "#   * nD array: (d1, d2, d3, ..., dn) - n dimensions\n",
    "# - Common use cases:\n",
    "#   * Check karna ki data model input requirements se match kar raha hai\n",
    "#   * Matrix multiplication compatibility verify karna: (m,n) @ (n,p) → (m,p)\n",
    "#   * Reshape karne se pehle data layout samajhna\n",
    "#   * Neural networks mein \"shape mismatch\" errors debug karna\n",
    "# - Example:\n",
    "#   arr_1d = np.array([1, 2, 3])                    # shape: (3,)\n",
    "#   arr_2d = np.array([[1, 2], [3, 4], [5, 6]])    # shape: (3, 2)\n",
    "#   arr_3d = np.array([[[1, 2]], [[3, 4]]])        # shape: (2, 1, 2)\n",
    "#   \n",
    "#   # ML example: MNIST dataset\n",
    "#   X_train = np.zeros((60000, 784))  # 60000 images, 784 features (28×28)\n",
    "#   X_train.shape  # Returns: (60000, 784)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(test_arra.shape)\n",
    "# Output: (3, 4)\n",
    "# Meaning: 3 rows (first dimension), 4 columns (second dimension)\n",
    "\n",
    "# =============================================================================\n",
    "# PROPERTY 2: .size\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Returns the total number of elements in the array\n",
    "# - Why use it: Useful for calculating memory usage, understanding total data\n",
    "#   points, and verifying array operations. Size = product of all dimensions.\n",
    "# - Returns: Single integer (total element count)\n",
    "# - Formula: For shape (3, 4), size = 3 × 4 = 12\n",
    "# - Difference from .shape:\n",
    "#   * .shape gives dimensions: (3, 4)\n",
    "#   * .size gives total elements: 12\n",
    "# - Common use cases:\n",
    "#   * Calculating memory consumption: size × dtype.itemsize = bytes\n",
    "#   * Verifying flatten/ravel operations didn't lose data\n",
    "#   * Checking if two arrays have same number of elements\n",
    "#   * Understanding data volume for processing time estimates\n",
    "# - Example:\n",
    "#   arr = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "#   arr.shape  # (2, 3)\n",
    "#   arr.size   # 6 (because 2 × 3 = 6)\n",
    "#   \n",
    "#   # Memory calculation\n",
    "#   arr.size * arr.dtype.itemsize  # 6 × 8 = 48 bytes (for int64)\n",
    "#   \n",
    "#   # 3D array\n",
    "#   arr_3d = np.zeros((2, 3, 4))\n",
    "#   arr_3d.shape  # (2, 3, 4)\n",
    "#   arr_3d.size   # 24 (because 2 × 3 × 4 = 24)\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Array mein total elements ki sankhya return karta hai\n",
    "# - Kab use karein: Memory usage calculate karne ke liye, total data points\n",
    "#   samajhne ke liye, aur array operations verify karne ke liye useful hai.\n",
    "#   Size = sabhi dimensions ka product.\n",
    "# - Returns: Single integer (total element count)\n",
    "# - Formula: Shape (3, 4) ke liye, size = 3 × 4 = 12\n",
    "# - .shape se difference:\n",
    "#   * .shape dimensions deta hai: (3, 4)\n",
    "#   * .size total elements deta hai: 12\n",
    "# - Common use cases:\n",
    "#   * Memory consumption calculate karna: size × dtype.itemsize = bytes\n",
    "#   * Flatten/ravel operations verify karna ki data loss nahi hua\n",
    "#   * Check karna ki do arrays mein same number of elements hain\n",
    "#   * Processing time estimate ke liye data volume samajhna\n",
    "# - Example:\n",
    "#   arr = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "#   arr.shape  # (2, 3)\n",
    "#   arr.size   # 6 (kyunki 2 × 3 = 6)\n",
    "#   \n",
    "#   # Memory calculation\n",
    "#   arr.size * arr.dtype.itemsize  # 6 × 8 = 48 bytes (int64 ke liye)\n",
    "#   \n",
    "#   # 3D array\n",
    "#   arr_3d = np.zeros((2, 3, 4))\n",
    "#   arr_3d.shape  # (2, 3, 4)\n",
    "#   arr_3d.size   # 24 (kyunki 2 × 3 × 4 = 24)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(test_arra.size)\n",
    "# Output: 12\n",
    "# Meaning: Total 12 elements in the array (3 rows × 4 columns = 12)\n",
    "\n",
    "# =============================================================================\n",
    "# PROPERTY 3: .dtype\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Returns the data type of array elements\n",
    "# - Why use it: Critical for memory optimization and numerical precision in ML.\n",
    "#   Different dtypes consume different memory and have different precision.\n",
    "# - Returns: Data type object (int8, int16, int32, int64, float32, float64, etc.)\n",
    "# - Common data types:\n",
    "#   * int8: -128 to 127 (1 byte) - good for small integers, saves memory\n",
    "#   * int16: -32768 to 32767 (2 bytes)\n",
    "#   * int32: ~-2 billion to 2 billion (4 bytes)\n",
    "#   * int64: very large range (8 bytes) - default for integers\n",
    "#   * float32: ~7 decimal digits precision (4 bytes) - common in deep learning\n",
    "#   * float64: ~15 decimal digits precision (8 bytes) - default for floats\n",
    "#   * bool: True/False (1 byte)\n",
    "# - Memory impact:\n",
    "#   * Array with 1 million int8 elements = 1 MB\n",
    "#   * Same array with int64 elements = 8 MB (8× larger!)\n",
    "# - Common use cases:\n",
    "#   * Reducing model memory footprint by using float32 instead of float64\n",
    "#   * Ensuring correct data type before feeding to neural networks\n",
    "#   * Converting data types to prevent overflow/underflow\n",
    "#   * Matching data types for operations (can't mix int and float sometimes)\n",
    "# - Example:\n",
    "#   arr_int = np.array([1, 2, 3])           # dtype: int64 (default)\n",
    "#   arr_float = np.array([1.0, 2.0, 3.0])  # dtype: float64 (default)\n",
    "#   arr_int8 = np.array([1, 2, 3], dtype=np.int8)      # dtype: int8\n",
    "#   arr_float32 = np.array([1.0, 2.0], dtype=np.float32)  # dtype: float32\n",
    "#   \n",
    "#   # Type conversion\n",
    "#   arr_int.astype(np.float32)  # Convert int64 to float32\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Array elements ka data type return karta hai\n",
    "# - Kab use karein: ML mein memory optimization aur numerical precision ke \n",
    "#   liye bahut important hai. Different dtypes alag memory consume karte hain\n",
    "#   aur alag precision dete hain.\n",
    "# - Returns: Data type object (int8, int16, int32, int64, float32, float64, etc.)\n",
    "# - Common data types:\n",
    "#   * int8: -128 se 127 (1 byte) - small integers ke liye, memory bachata hai\n",
    "#   * int16: -32768 se 32767 (2 bytes)\n",
    "#   * int32: ~-2 billion se 2 billion (4 bytes)\n",
    "#   * int64: bahut bada range (8 bytes) - integers ke liye default\n",
    "#   * float32: ~7 decimal digits precision (4 bytes) - deep learning mein common\n",
    "#   * float64: ~15 decimal digits precision (8 bytes) - floats ke liye default\n",
    "#   * bool: True/False (1 byte)\n",
    "# - Memory impact:\n",
    "#   * 1 million int8 elements wala array = 1 MB\n",
    "#   * Same array with int64 elements = 8 MB (8× zyada bada!)\n",
    "# - Common use cases:\n",
    "#   * float32 use karke float64 ki jagah model memory footprint kam karna\n",
    "#   * Neural networks ko data dene se pehle sahi data type ensure karna\n",
    "#   * Overflow/underflow se bachne ke liye data types convert karna\n",
    "#   * Operations ke liye data types match karna (kabhi int aur float mix nahi kar sakte)\n",
    "# - Example:\n",
    "#   arr_int = np.array([1, 2, 3])           # dtype: int64 (default)\n",
    "#   arr_float = np.array([1.0, 2.0, 3.0])  # dtype: float64 (default)\n",
    "#   arr_int8 = np.array([1, 2, 3], dtype=np.int8)      # dtype: int8\n",
    "#   arr_float32 = np.array([1.0, 2.0], dtype=np.float32)  # dtype: float32\n",
    "#   \n",
    "#   # Type conversion\n",
    "#   arr_int.astype(np.float32)  # int64 ko float32 mein convert karna\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(test_arra.dtype)\n",
    "# Output: int64\n",
    "# Meaning: Each element is a 64-bit integer (8 bytes per element)\n",
    "# Total memory for this array: 12 elements × 8 bytes = 96 bytes\n",
    "\n",
    "# =============================================================================\n",
    "# PROPERTY 4: .ndim\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Returns the number of dimensions (axes) of the array\n",
    "# - Why use it: Helps understand array complexity. Essential for knowing if\n",
    "#   you're working with vectors (1D), matrices (2D), tensors (3D+), etc.\n",
    "# - Returns: Integer representing number of dimensions\n",
    "#   * 0D: scalar (single value) - rare, usually just numbers\n",
    "#   * 1D: vector (list of values) - feature vector, time series\n",
    "#   * 2D: matrix (table of values) - datasets, images (grayscale)\n",
    "#   * 3D: tensor (cube of values) - RGB images, video frames, batches\n",
    "#   * 4D+: higher-order tensors - video batches, medical scans\n",
    "# - Relationship with .shape:\n",
    "#   * .ndim = length of .shape tuple\n",
    "#   * shape (3, 4) → ndim = 2\n",
    "#   * shape (10, 28, 28, 3) → ndim = 4\n",
    "# - Common use cases:\n",
    "#   * Checking if data needs reshaping before processing\n",
    "#   * Validating input dimensions for neural network layers\n",
    "#   * Determining if data is batched (extra dimension for batch)\n",
    "#   * Understanding data structure complexity\n",
    "# - Example:\n",
    "#   arr_0d = np.array(42)                    # ndim: 0 (scalar)\n",
    "#   arr_1d = np.array([1, 2, 3])            # ndim: 1 (vector)\n",
    "#   arr_2d = np.array([[1, 2], [3, 4]])     # ndim: 2 (matrix)\n",
    "#   arr_3d = np.zeros((10, 28, 28))         # ndim: 3 (tensor)\n",
    "#   arr_4d = np.zeros((32, 28, 28, 3))      # ndim: 4 (batch of RGB images)\n",
    "#   \n",
    "#   # ML example: Image data\n",
    "#   single_image = np.zeros((28, 28, 3))     # ndim: 3 (height, width, channels)\n",
    "#   batch_images = np.zeros((100, 28, 28, 3)) # ndim: 4 (batch, height, width, channels)\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Array ke dimensions (axes) ki sankhya return karta hai\n",
    "# - Kab use karein: Array complexity samajhne mein madad karta hai. Yeh\n",
    "#   jaanne ke liye essential hai ki aap vectors (1D), matrices (2D), ya \n",
    "#   tensors (3D+) ke saath kaam kar rahe hain.\n",
    "# - Returns: Integer jo dimensions ki sankhya represent karta hai\n",
    "#   * 0D: scalar (single value) - rare, usually sirf numbers\n",
    "#   * 1D: vector (values ki list) - feature vector, time series\n",
    "#   * 2D: matrix (values ki table) - datasets, images (grayscale)\n",
    "#   * 3D: tensor (values ka cube) - RGB images, video frames, batches\n",
    "#   * 4D+: higher-order tensors - video batches, medical scans\n",
    "# - .shape ke saath relationship:\n",
    "#   * .ndim = .shape tuple ki length\n",
    "#   * shape (3, 4) → ndim = 2\n",
    "#   * shape (10, 28, 28, 3) → ndim = 4\n",
    "# - Common use cases:\n",
    "#   * Check karna ki processing se pehle data ko reshape karna hai\n",
    "#   * Neural network layers ke liye input dimensions validate karna\n",
    "#   * Determine karna ki data batched hai (batch ke liye extra dimension)\n",
    "#   * Data structure complexity samajhna\n",
    "# - Example:\n",
    "#   arr_0d = np.array(42)                    # ndim: 0 (scalar)\n",
    "#   arr_1d = np.array([1, 2, 3])            # ndim: 1 (vector)\n",
    "#   arr_2d = np.array([[1, 2], [3, 4]])     # ndim: 2 (matrix)\n",
    "#   arr_3d = np.zeros((10, 28, 28))         # ndim: 3 (tensor)\n",
    "#   arr_4d = np.zeros((32, 28, 28, 3))      # ndim: 4 (RGB images ka batch)\n",
    "#   \n",
    "#   # ML example: Image data\n",
    "#   single_image = np.zeros((28, 28, 3))     # ndim: 3 (height, width, channels)\n",
    "#   batch_images = np.zeros((100, 28, 28, 3)) # ndim: 4 (batch, height, width, channels)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(test_arra.ndim)\n",
    "# Output: 2\n",
    "# Meaning: This is a 2-dimensional array (matrix with rows and columns)\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY OF ALL PROPERTIES FOR test_arra\n",
    "# =============================================================================\n",
    "# Array: [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]\n",
    "# \n",
    "# .shape  → (3, 4)   : 3 rows, 4 columns\n",
    "# .size   → 12       : Total 12 elements (3 × 4)\n",
    "# .dtype  → int64    : Each element is 64-bit integer (8 bytes)\n",
    "# .ndim   → 2        : 2-dimensional array (matrix)\n",
    "# \n",
    "# Memory calculation: 12 elements × 8 bytes = 96 bytes total\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# BONUS: HOW THESE PROPERTIES RELATE IN ML\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# In Machine Learning, these properties help you understand your data:\n",
    "# - .ndim tells you the type: 1D=features, 2D=dataset, 3D=images, 4D=batches\n",
    "# - .shape tells you exact dimensions: (samples, features) or (height, width)\n",
    "# - .size tells you total data points: affects memory and processing time\n",
    "# - .dtype tells you precision and memory: float32 vs float64 can halve memory\n",
    "#\n",
    "# Example workflow:\n",
    "# 1. Load data → check .shape to verify dimensions\n",
    "# 2. Check .dtype → convert to float32 if needed for memory\n",
    "# 3. Verify .ndim → add/remove dimensions for model compatibility\n",
    "# 4. Monitor .size → estimate processing time and memory needs\n",
    "#\n",
    "# HINGLISH:\n",
    "# Machine Learning mein, yeh properties aapko data samajhne mein madad karti hain:\n",
    "# - .ndim type batata hai: 1D=features, 2D=dataset, 3D=images, 4D=batches\n",
    "# - .shape exact dimensions batata hai: (samples, features) ya (height, width)\n",
    "# - .size total data points batata hai: memory aur processing time ko affect karta hai\n",
    "# - .dtype precision aur memory batata hai: float32 vs float64 memory half kar sakta hai\n",
    "#\n",
    "# Example workflow:\n",
    "# 1. Data load karo → dimensions verify karne ke liye .shape check karo\n",
    "# 2. .dtype check karo → memory ke liye zaroorat ho to float32 mein convert karo\n",
    "# 3. .ndim verify karo → model compatibility ke liye dimensions add/remove karo\n",
    "# 4. .size monitor karo → processing time aur memory needs estimate karo\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39ea6715-5c5b-4163-a7fd-2f740ea0bb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]] (2, 3)\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]] (3, 2)\n",
      "[1 2 3 4 5 6] (6,)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# NUMPY ARRAY RESHAPING OPERATIONS\n",
    "# =============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CREATING BASE ARRAY FOR RESHAPING DEMONSTRATIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "# ENGLISH:\n",
    "# - What it does: Creates a 2D array with 2 rows and 3 columns (2×3 matrix)\n",
    "# - Why use it: This array will be used to demonstrate various reshaping\n",
    "#   operations which are fundamental in ML for transforming data between\n",
    "#   different formats (e.g., image to vector, batch to single sample)\n",
    "# - Structure: 2 rows, 3 columns, total 6 elements\n",
    "# - Note: Total elements (6) must be preserved during reshaping operations\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: 2 rows aur 3 columns ke saath ek 2D array banata hai\n",
    "#   (2×3 matrix)\n",
    "# - Kab use karein: Is array ko various reshaping operations demonstrate\n",
    "#   karne ke liye use karenge jo ML mein data ko different formats mein\n",
    "#   transform karne ke liye fundamental hain (jaise image to vector, batch\n",
    "#   to single sample)\n",
    "# - Structure: 2 rows, 3 columns, total 6 elements\n",
    "# - Note: Reshaping operations ke dauraan total elements (6) preserve hone chahiye\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "arr_test = np.array([[1, 2, 3],    # Row 0\n",
    "                     [4, 5, 6]])   # Row 1\n",
    "\n",
    "print(arr_test, arr_test.shape)\n",
    "# Output: [[1 2 3]\n",
    "#          [4 5 6]] (2, 3)\n",
    "# Original array: 2 rows × 3 columns = 6 total elements\n",
    "\n",
    "# =============================================================================\n",
    "# OPERATION 1: .reshape()\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Changes the shape/dimensions of an array WITHOUT changing\n",
    "#   the data or total number of elements. Returns a new view of the array.\n",
    "# - Why use it: Critical for preparing data for neural networks. Different\n",
    "#   layers expect different input shapes. You often need to convert between\n",
    "#   formats (matrix → vector, vector → matrix, add batch dimension, etc.)\n",
    "# - Syntax: array.reshape(new_shape) or array.reshape((dim1, dim2, ...))\n",
    "# - CRITICAL RULE: new_shape must have same total elements as original\n",
    "#   * Original: (2, 3) = 6 elements\n",
    "#   * Valid reshapes: (3, 2), (6,), (1, 6), (6, 1), (1, 2, 3), etc.\n",
    "#   * Invalid: (2, 2) = 4 elements ❌ (doesn't match 6)\n",
    "# - Special value -1: NumPy auto-calculates that dimension\n",
    "#   * arr.reshape(-1) → flattens to 1D\n",
    "#   * arr.reshape(3, -1) → 3 rows, auto-calculate columns → (3, 2)\n",
    "#   * arr.reshape(-1, 1) → column vector (6, 1)\n",
    "# - Common use cases:\n",
    "#   * Flattening images: (28, 28) → (784,) for fully connected layers\n",
    "#   * Adding batch dimension: (784,) → (1, 784) for single sample prediction\n",
    "#   * Preparing data for CNNs: (60000, 784) → (60000, 28, 28, 1)\n",
    "#   * Converting between row/column vectors: (n,) → (n, 1) or (1, n)\n",
    "#   * Reshaping model outputs: (batch, classes) → desired format\n",
    "# - reshape() vs resize():\n",
    "#   * reshape() returns new view, original unchanged\n",
    "#   * resize() modifies array in-place (changes original)\n",
    "# - Example:\n",
    "#   # Image preprocessing\n",
    "#   img_flat = np.array([...])  # shape: (784,) - flattened\n",
    "#   img_2d = img_flat.reshape(28, 28)  # shape: (28, 28) - for visualization\n",
    "#   \n",
    "#   # Adding batch dimension\n",
    "#   single_sample = np.array([1, 2, 3, 4])  # shape: (4,)\n",
    "#   batched = single_sample.reshape(1, -1)   # shape: (1, 4) - batch of 1\n",
    "#   \n",
    "#   # Auto-calculate dimension\n",
    "#   arr = np.arange(12)  # shape: (12,)\n",
    "#   arr.reshape(3, -1)   # shape: (3, 4) - auto-calculates 4 columns\n",
    "#   arr.reshape(-1, 2)   # shape: (6, 2) - auto-calculates 6 rows\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Array ka shape/dimensions change karta hai BINA data\n",
    "#   ya total elements ki sankhya change kiye. Array ka naya view return karta hai.\n",
    "# - Kab use karein: Neural networks ke liye data prepare karne mein critical\n",
    "#   hai. Different layers alag input shapes expect karti hain. Aapko aksar\n",
    "#   formats ke beech convert karna padta hai (matrix → vector, vector → matrix,\n",
    "#   batch dimension add karna, etc.)\n",
    "# - Syntax: array.reshape(new_shape) ya array.reshape((dim1, dim2, ...))\n",
    "# - CRITICAL RULE: new_shape mein same total elements hone chahiye jitne original mein\n",
    "#   * Original: (2, 3) = 6 elements\n",
    "#   * Valid reshapes: (3, 2), (6,), (1, 6), (6, 1), (1, 2, 3), etc.\n",
    "#   * Invalid: (2, 2) = 4 elements ❌ (6 se match nahi karta)\n",
    "# - Special value -1: NumPy us dimension ko auto-calculate karta hai\n",
    "#   * arr.reshape(-1) → 1D mein flatten karta hai\n",
    "#   * arr.reshape(3, -1) → 3 rows, auto-calculate columns → (3, 2)\n",
    "#   * arr.reshape(-1, 1) → column vector (6, 1)\n",
    "# - Common use cases:\n",
    "#   * Images flatten karna: (28, 28) → (784,) fully connected layers ke liye\n",
    "#   * Batch dimension add karna: (784,) → (1, 784) single sample prediction ke liye\n",
    "#   * CNNs ke liye data prepare karna: (60000, 784) → (60000, 28, 28, 1)\n",
    "#   * Row/column vectors ke beech convert karna: (n,) → (n, 1) ya (1, n)\n",
    "#   * Model outputs reshape karna: (batch, classes) → desired format\n",
    "# - reshape() vs resize():\n",
    "#   * reshape() naya view return karta hai, original unchanged rehta hai\n",
    "#   * resize() array ko in-place modify karta hai (original change hota hai)\n",
    "# - Example:\n",
    "#   # Image preprocessing\n",
    "#   img_flat = np.array([...])  # shape: (784,) - flattened\n",
    "#   img_2d = img_flat.reshape(28, 28)  # shape: (28, 28) - visualization ke liye\n",
    "#   \n",
    "#   # Batch dimension add karna\n",
    "#   single_sample = np.array([1, 2, 3, 4])  # shape: (4,)\n",
    "#   batched = single_sample.reshape(1, -1)   # shape: (1, 4) - 1 ka batch\n",
    "#   \n",
    "#   # Auto-calculate dimension\n",
    "#   arr = np.arange(12)  # shape: (12,)\n",
    "#   arr.reshape(3, -1)   # shape: (3, 4) - 4 columns auto-calculate\n",
    "#   arr.reshape(-1, 2)   # shape: (6, 2) - 6 rows auto-calculate\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "reshaped = arr_test.reshape((3, 2))\n",
    "# Reshaping from (2, 3) to (3, 2)\n",
    "# Original layout:     New layout:\n",
    "# [[1, 2, 3],     →    [[1, 2],\n",
    "#  [4, 5, 6]]           [3, 4],\n",
    "#                       [5, 6]]\n",
    "# Elements are read in row-major order (C-style) and rearranged\n",
    "\n",
    "print(reshaped, reshaped.shape)\n",
    "# Output: [[1 2]\n",
    "#          [3 4]\n",
    "#          [5 6]] (3, 2)\n",
    "# New array: 3 rows × 2 columns = 6 total elements (same as original)\n",
    "\n",
    "# =============================================================================\n",
    "# UNDERSTANDING RESHAPE ELEMENT ORDERING\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# How reshape reads and rearranges elements:\n",
    "# 1. Elements are read in \"row-major\" (C-style) order: [1,2,3,4,5,6]\n",
    "# 2. They are then written into new shape in same order\n",
    "# 3. Original (2,3): [[1,2,3], [4,5,6]] → flattened: [1,2,3,4,5,6]\n",
    "# 4. New (3,2): Take 2 elements per row → [[1,2], [3,4], [5,6]]\n",
    "#\n",
    "# HINGLISH:\n",
    "# Reshape elements ko kaise read aur rearrange karta hai:\n",
    "# 1. Elements \"row-major\" (C-style) order mein read hote hain: [1,2,3,4,5,6]\n",
    "# 2. Phir same order mein naye shape mein likhe jaate hain\n",
    "# 3. Original (2,3): [[1,2,3], [4,5,6]] → flattened: [1,2,3,4,5,6]\n",
    "# 4. New (3,2): Har row mein 2 elements lo → [[1,2], [3,4], [5,6]]\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# =============================================================================\n",
    "# OPERATION 2: .flatten()\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Converts a multi-dimensional array into a 1D array (vector).\n",
    "#   Always returns a COPY of the data (not a view).\n",
    "# - Why use it: Essential for converting structured data (like images) into\n",
    "#   vectors for machine learning algorithms that expect 1D input (e.g., \n",
    "#   logistic regression, SVMs, fully connected neural network layers)\n",
    "# - Syntax: array.flatten(order='C')\n",
    "# - Parameters:\n",
    "#   * order='C': Row-major (C-style, default) - read rows left-to-right\n",
    "#   * order='F': Column-major (Fortran-style) - read columns top-to-bottom\n",
    "#   * order='A': Use 'F' if array is Fortran-contiguous, else 'C'\n",
    "# - Returns: Always 1D array with shape (n,) where n = total elements\n",
    "# - flatten() vs ravel():\n",
    "#   * flatten() → ALWAYS returns a COPY (new memory allocation)\n",
    "#   * ravel() → returns a VIEW when possible (more memory efficient)\n",
    "#   * Use flatten() when you need independent copy\n",
    "#   * Use ravel() when you want to save memory\n",
    "# - Common use cases:\n",
    "#   * Preprocessing images for traditional ML: (28, 28) → (784,)\n",
    "#   * Converting feature matrices to vectors\n",
    "#   * Preparing data for algorithms expecting 1D input\n",
    "#   * Flattening CNN feature maps before fully connected layers\n",
    "#   * Creating feature vectors from multi-dimensional data\n",
    "# - Example:\n",
    "#   # Image flattening for ML\n",
    "#   image = np.random.rand(28, 28)  # shape: (28, 28) - grayscale image\n",
    "#   features = image.flatten()       # shape: (784,) - feature vector\n",
    "#   \n",
    "#   # RGB image flattening\n",
    "#   rgb_img = np.random.rand(64, 64, 3)  # shape: (64, 64, 3)\n",
    "#   flat_img = rgb_img.flatten()          # shape: (12288,) = 64×64×3\n",
    "#   \n",
    "#   # Different ordering\n",
    "#   arr = np.array([[1, 2], [3, 4]])\n",
    "#   arr.flatten(order='C')  # [1, 2, 3, 4] - row-major\n",
    "#   arr.flatten(order='F')  # [1, 3, 2, 4] - column-major\n",
    "#   \n",
    "#   # MNIST example\n",
    "#   mnist_image = np.zeros((28, 28))  # Single MNIST digit\n",
    "#   flat_digit = mnist_image.flatten()  # (784,) for neural network input\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Multi-dimensional array ko 1D array (vector) mein\n",
    "#   convert karta hai. Hamesha data ki COPY return karta hai (view nahi).\n",
    "# - Kab use karein: Structured data (jaise images) ko vectors mein convert\n",
    "#   karne ke liye essential hai, ML algorithms ke liye jo 1D input expect\n",
    "#   karte hain (jaise logistic regression, SVMs, fully connected neural\n",
    "#   network layers)\n",
    "# - Syntax: array.flatten(order='C')\n",
    "# - Parameters:\n",
    "#   * order='C': Row-major (C-style, default) - rows ko left-to-right padhna\n",
    "#   * order='F': Column-major (Fortran-style) - columns ko top-to-bottom padhna\n",
    "#   * order='A': Array Fortran-contiguous hai to 'F' use karo, warna 'C'\n",
    "# - Returns: Hamesha 1D array with shape (n,) jahan n = total elements\n",
    "# - flatten() vs ravel():\n",
    "#   * flatten() → HAMESHA COPY return karta hai (new memory allocation)\n",
    "#   * ravel() → jab possible ho VIEW return karta hai (zyada memory efficient)\n",
    "#   * flatten() use karo jab aapko independent copy chahiye\n",
    "#   * ravel() use karo jab memory save karni ho\n",
    "# - Common use cases:\n",
    "#   * Traditional ML ke liye images preprocess karna: (28, 28) → (784,)\n",
    "#   * Feature matrices ko vectors mein convert karna\n",
    "#   * 1D input expect karne wale algorithms ke liye data prepare karna\n",
    "#   * Fully connected layers se pehle CNN feature maps flatten karna\n",
    "#   * Multi-dimensional data se feature vectors banana\n",
    "# - Example:\n",
    "#   # ML ke liye image flattening\n",
    "#   image = np.random.rand(28, 28)  # shape: (28, 28) - grayscale image\n",
    "#   features = image.flatten()       # shape: (784,) - feature vector\n",
    "#   \n",
    "#   # RGB image flattening\n",
    "#   rgb_img = np.random.rand(64, 64, 3)  # shape: (64, 64, 3)\n",
    "#   flat_img = rgb_img.flatten()          # shape: (12288,) = 64×64×3\n",
    "#   \n",
    "#   # Different ordering\n",
    "#   arr = np.array([[1, 2], [3, 4]])\n",
    "#   arr.flatten(order='C')  # [1, 2, 3, 4] - row-major\n",
    "#   arr.flatten(order='F')  # [1, 3, 2, 4] - column-major\n",
    "#   \n",
    "#   # MNIST example\n",
    "#   mnist_image = np.zeros((28, 28))  # Single MNIST digit\n",
    "#   flat_digit = mnist_image.flatten()  # (784,) neural network input ke liye\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "flattened = arr_test.flatten()\n",
    "# Converts 2D array [[1,2,3], [4,5,6]] into 1D array [1,2,3,4,5,6]\n",
    "# Reads elements row by row (row-major order)\n",
    "# Creates a COPY (modifying flattened won't affect arr_test)\n",
    "\n",
    "print(flattened, flattened.shape)\n",
    "# Output: [1 2 3 4 5 6] (6,)\n",
    "# 1D array with 6 elements\n",
    "\n",
    "# =============================================================================\n",
    "# COMPARING THE THREE OPERATIONS\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Original arr_test:      (2, 3) shape\n",
    "# [[1, 2, 3],\n",
    "#  [4, 5, 6]]\n",
    "#\n",
    "# After reshape(3, 2):    (3, 2) shape - Same elements, different arrangement\n",
    "# [[1, 2],\n",
    "#  [3, 4],\n",
    "#  [5, 6]]\n",
    "#\n",
    "# After flatten():        (6,) shape - All elements in single row\n",
    "# [1, 2, 3, 4, 5, 6]\n",
    "#\n",
    "# Key differences:\n",
    "# - reshape(): Changes shape while preserving dimensionality concept\n",
    "# - flatten(): Always produces 1D array (loses dimensional structure)\n",
    "# - Both preserve total element count (6 elements throughout)\n",
    "#\n",
    "# HINGLISH:\n",
    "# Original arr_test:      (2, 3) shape\n",
    "# [[1, 2, 3],\n",
    "#  [4, 5, 6]]\n",
    "#\n",
    "# reshape(3, 2) ke baad:  (3, 2) shape - Same elements, alag arrangement\n",
    "# [[1, 2],\n",
    "#  [3, 4],\n",
    "#  [5, 6]]\n",
    "#\n",
    "# flatten() ke baad:      (6,) shape - Saare elements ek hi row mein\n",
    "# [1, 2, 3, 4, 5, 6]\n",
    "#\n",
    "# Key differences:\n",
    "# - reshape(): Shape change karta hai dimensionality concept preserve rakhte hue\n",
    "# - flatten(): Hamesha 1D array produce karta hai (dimensional structure kho jata hai)\n",
    "# - Dono total element count preserve karte hain (throughout 6 elements)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# =============================================================================\n",
    "# ADDITIONAL RESHAPING TECHNIQUES\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Other useful reshaping methods:\n",
    "# \n",
    "# 1. ravel() - Similar to flatten() but returns view (not copy)\n",
    "#    flattened_view = arr_test.ravel()  # More memory efficient\n",
    "#\n",
    "# 2. reshape with -1 (auto-calculate dimension)\n",
    "#    auto_reshaped = arr_test.reshape(-1)     # Same as flatten: (6,)\n",
    "#    col_vector = arr_test.reshape(-1, 1)     # Column vector: (6, 1)\n",
    "#    row_vector = arr_test.reshape(1, -1)     # Row vector: (1, 6)\n",
    "#\n",
    "# 3. transpose() or .T - Swap rows and columns\n",
    "#    transposed = arr_test.T  # (2,3) → (3,2) but different arrangement\n",
    "#\n",
    "# 4. squeeze() - Remove single-dimensional entries\n",
    "#    arr_with_extra_dim = np.array([[[1, 2, 3]]])  # shape: (1, 1, 3)\n",
    "#    squeezed = arr_with_extra_dim.squeeze()        # shape: (3,)\n",
    "#\n",
    "# 5. expand_dims() - Add new axis/dimension\n",
    "#    arr_1d = np.array([1, 2, 3])  # shape: (3,)\n",
    "#    arr_2d = np.expand_dims(arr_1d, axis=0)  # shape: (1, 3) - row vector\n",
    "#    arr_2d = np.expand_dims(arr_1d, axis=1)  # shape: (3, 1) - column vector\n",
    "#\n",
    "# HINGLISH:\n",
    "# Reshaping ke aur useful methods:\n",
    "# \n",
    "# 1. ravel() - flatten() jaisa hai par view return karta hai (copy nahi)\n",
    "#    flattened_view = arr_test.ravel()  # Zyada memory efficient\n",
    "#\n",
    "# 2. -1 ke saath reshape (dimension auto-calculate)\n",
    "#    auto_reshaped = arr_test.reshape(-1)     # flatten jaisa: (6,)\n",
    "#    col_vector = arr_test.reshape(-1, 1)     # Column vector: (6, 1)\n",
    "#    row_vector = arr_test.reshape(1, -1)     # Row vector: (1, 6)\n",
    "#\n",
    "# 3. transpose() ya .T - Rows aur columns swap karna\n",
    "#    transposed = arr_test.T  # (2,3) → (3,2) lekin alag arrangement\n",
    "#\n",
    "# 4. squeeze() - Single-dimensional entries remove karna\n",
    "#    arr_with_extra_dim = np.array([[[1, 2, 3]]])  # shape: (1, 1, 3)\n",
    "#    squeezed = arr_with_extra_dim.squeeze()        # shape: (3,)\n",
    "#\n",
    "# 5. expand_dims() - Naya axis/dimension add karna\n",
    "#    arr_1d = np.array([1, 2, 3])  # shape: (3,)\n",
    "#    arr_2d = np.expand_dims(arr_1d, axis=0)  # shape: (1, 3) - row vector\n",
    "#    arr_2d = np.expand_dims(arr_1d, axis=1)  # shape: (3, 1) - column vector\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# PRACTICAL ML EXAMPLE: MNIST DIGIT PREPROCESSING\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Real-world scenario: Preparing MNIST handwritten digits for neural network\n",
    "#\n",
    "# # Load image (28x28 pixels)\n",
    "# digit_image = np.random.rand(28, 28)  # Simulating grayscale image\n",
    "# print(f\"Original shape: {digit_image.shape}\")  # (28, 28)\n",
    "#\n",
    "# # For CNN (Convolutional Neural Network) - needs (height, width, channels)\n",
    "# cnn_input = digit_image.reshape(28, 28, 1)  # Add channel dimension\n",
    "# print(f\"CNN input shape: {cnn_input.shape}\")  # (28, 28, 1)\n",
    "#\n",
    "# # For Fully Connected Network - needs flattened vector\n",
    "# fc_input = digit_image.flatten()\n",
    "# print(f\"FC input shape: {fc_input.shape}\")  # (784,)\n",
    "#\n",
    "# # For batch processing - add batch dimension\n",
    "# batch_input = digit_image.reshape(1, 28, 28, 1)  # Single image in batch\n",
    "# print(f\"Batch input shape: {batch_input.shape}\")  # (1, 28, 28, 1)\n",
    "#\n",
    "# HINGLISH:\n",
    "# Real-world scenario: Neural network ke liye MNIST handwritten digits prepare karna\n",
    "#\n",
    "# # Image load karo (28x28 pixels)\n",
    "# digit_image = np.random.rand(28, 28)  # Grayscale image simulate kar rahe hain\n",
    "# print(f\"Original shape: {digit_image.shape}\")  # (28, 28)\n",
    "#\n",
    "# # CNN (Convolutional Neural Network) ke liye - (height, width, channels) chahiye\n",
    "# cnn_input = digit_image.reshape(28, 28, 1)  # Channel dimension add karo\n",
    "# print(f\"CNN input shape: {cnn_input.shape}\")  # (28, 28, 1)\n",
    "#\n",
    "# # Fully Connected Network ke liye - flattened vector chahiye\n",
    "# fc_input = digit_image.flatten()\n",
    "# print(f\"FC input shape: {fc_input.shape}\")  # (784,)\n",
    "#\n",
    "# # Batch processing ke liye - batch dimension add karo\n",
    "# batch_input = digit_image.reshape(1, 28, 28, 1)  # Batch mein single image\n",
    "# print(f\"Batch input shape: {batch_input.shape}\")  # (1, 28, 28, 1)\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89ab5217-44ea-49cb-983d-d72a82c1fe88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "7\n",
      "[1 3 5]\n",
      "[3 4 5]\n",
      "[2 4]\n",
      "[1 3 5]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# NUMPY ARRAY INDEXING AND SLICING TECHNIQUES\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# BASIC 1D ARRAY INDEXING\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Creates a 1D array and accesses individual elements using\n",
    "#   their position (index)\n",
    "# - Why use it: Basic element access is fundamental for data manipulation,\n",
    "#   feature extraction, and working with individual data points in ML\n",
    "# - Indexing rules:\n",
    "#   * Indexing starts at 0 (first element is arr[0])\n",
    "#   * Negative indexing: arr[-1] is last element, arr[-2] is second-to-last\n",
    "#   * Out-of-bounds index raises IndexError\n",
    "# - Common use cases:\n",
    "#   * Accessing specific features from feature vectors\n",
    "#   * Getting predictions for specific samples\n",
    "#   * Extracting labels or targets from datasets\n",
    "#   * Debugging by checking individual values\n",
    "# - Example:\n",
    "#   arr = np.array([10, 20, 30, 40, 50])\n",
    "#   arr[0]   # 10 - first element\n",
    "#   arr[2]   # 30 - third element\n",
    "#   arr[-1]  # 50 - last element\n",
    "#   arr[-2]  # 40 - second-to-last element\n",
    "#   arr[4]   # 50 - fifth element (last)\n",
    "#   # arr[5] would raise IndexError (out of bounds)\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: 1D array banata hai aur elements ko unki position\n",
    "#   (index) use karke access karta hai\n",
    "# - Kab use karein: ML mein data manipulation, feature extraction, aur\n",
    "#   individual data points ke saath kaam karne ke liye basic element access\n",
    "#   fundamental hai\n",
    "# - Indexing rules:\n",
    "#   * Indexing 0 se shuru hoti hai (pehla element arr[0] hai)\n",
    "#   * Negative indexing: arr[-1] last element hai, arr[-2] second-to-last hai\n",
    "#   * Out-of-bounds index IndexError raise karta hai\n",
    "# - Common use cases:\n",
    "#   * Feature vectors se specific features access karna\n",
    "#   * Specific samples ke liye predictions lena\n",
    "#   * Datasets se labels ya targets extract karna\n",
    "#   * Individual values check karke debugging karna\n",
    "# - Example:\n",
    "#   arr = np.array([10, 20, 30, 40, 50])\n",
    "#   arr[0]   # 10 - pehla element\n",
    "#   arr[2]   # 30 - teesra element\n",
    "#   arr[-1]  # 50 - last element\n",
    "#   arr[-2]  # 40 - second-to-last element\n",
    "#   arr[4]   # 50 - paanchva element (last)\n",
    "#   # arr[5] IndexError raise karega (out of bounds)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "arr = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "print(arr[0])\n",
    "# Output: 1\n",
    "# Accesses the first element (index 0)\n",
    "\n",
    "# =============================================================================\n",
    "# 2D ARRAY INDEXING (MATRIX INDEXING)\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Accesses elements in multi-dimensional arrays using\n",
    "#   [row_index][column_index] or [row_index, column_index] notation\n",
    "# - Why use it: Essential for working with datasets, images, and matrices in\n",
    "#   ML where you need to access specific rows, columns, or individual cells\n",
    "# - Two indexing notations:\n",
    "#   * arr[row][col] - chained indexing (less efficient, two operations)\n",
    "#   * arr[row, col] - tuple indexing (preferred, single operation, faster)\n",
    "# - Indexing rules:\n",
    "#   * First index = row (vertical position)\n",
    "#   * Second index = column (horizontal position)\n",
    "#   * Both use 0-based indexing\n",
    "#   * Negative indices work for both dimensions\n",
    "# - Common use cases:\n",
    "#   * Accessing specific pixels in images: image[y, x]\n",
    "#   * Getting specific feature values: dataset[sample_idx, feature_idx]\n",
    "#   * Extracting matrix elements for calculations\n",
    "#   * Accessing weights in neural network weight matrices\n",
    "#   * Selecting specific data points from tabular data\n",
    "# - Example:\n",
    "#   matrix = np.array([[10, 20, 30],\n",
    "#                      [40, 50, 60],\n",
    "#                      [70, 80, 90]])\n",
    "#   matrix[0, 0]    # 10 - top-left corner\n",
    "#   matrix[0, 2]    # 30 - first row, third column\n",
    "#   matrix[2, 1]    # 80 - third row, second column\n",
    "#   matrix[-1, -1]  # 90 - bottom-right corner\n",
    "#   matrix[1]       # [40, 50, 60] - entire second row\n",
    "#   matrix[:, 0]    # [10, 40, 70] - entire first column\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Multi-dimensional arrays mein elements ko\n",
    "#   [row_index][column_index] ya [row_index, column_index] notation use\n",
    "#   karke access karta hai\n",
    "# - Kab use karein: ML mein datasets, images, aur matrices ke saath kaam\n",
    "#   karte waqt specific rows, columns, ya individual cells access karne\n",
    "#   ke liye essential hai\n",
    "# - Do indexing notations:\n",
    "#   * arr[row][col] - chained indexing (kam efficient, do operations)\n",
    "#   * arr[row, col] - tuple indexing (preferred, single operation, faster)\n",
    "# - Indexing rules:\n",
    "#   * Pehla index = row (vertical position)\n",
    "#   * Doosra index = column (horizontal position)\n",
    "#   * Dono 0-based indexing use karte hain\n",
    "#   * Negative indices dono dimensions ke liye kaam karte hain\n",
    "# - Common use cases:\n",
    "#   * Images mein specific pixels access karna: image[y, x]\n",
    "#   * Specific feature values lena: dataset[sample_idx, feature_idx]\n",
    "#   * Calculations ke liye matrix elements extract karna\n",
    "#   * Neural network weight matrices mein weights access karna\n",
    "#   * Tabular data se specific data points select karna\n",
    "# - Example:\n",
    "#   matrix = np.array([[10, 20, 30],\n",
    "#                      [40, 50, 60],\n",
    "#                      [70, 80, 90]])\n",
    "#   matrix[0, 0]    # 10 - top-left corner\n",
    "#   matrix[0, 2]    # 30 - pehli row, teesra column\n",
    "#   matrix[2, 1]    # 80 - teesri row, doosra column\n",
    "#   matrix[-1, -1]  # 90 - bottom-right corner\n",
    "#   matrix[1]       # [40, 50, 60] - poori doosri row\n",
    "#   matrix[:, 0]    # [10, 40, 70] - poora pehla column\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "index_arr = np.array([[1, 2, 3, 4],    # Row 0: index 0\n",
    "                      [5, 6, 7, 8]])   # Row 1: index 1\n",
    "\n",
    "print(index_arr[1][2])\n",
    "# Output: 7\n",
    "# Accessing row 1, column 2\n",
    "# Better way: index_arr[1, 2] (single operation, more efficient)\n",
    "# Visual representation:\n",
    "# [[1, 2, 3, 4],     Row 0\n",
    "#  [5, 6, 7, 8]]     Row 1 ← We want this row\n",
    "#     ↑\n",
    "#  Column 2 - Element is 7\n",
    "\n",
    "# =============================================================================\n",
    "# FANCY INDEXING (ADVANCED INDEXING WITH ARRAYS)\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Uses an array or list of indices to select multiple elements\n",
    "#   at once from another array. Returns a new array with selected elements.\n",
    "# - Why use it: Extremely powerful for selecting non-contiguous or specific\n",
    "#   elements based on conditions, indices, or patterns. Essential for data\n",
    "#   sampling, feature selection, and batch processing in ML.\n",
    "# - Syntax: array[index_array] where index_array is list/array of indices\n",
    "# - Key features:\n",
    "#   * Can select elements in any order\n",
    "#   * Can repeat indices to duplicate elements\n",
    "#   * Can use with multi-dimensional arrays\n",
    "#   * Always returns a COPY (not a view)\n",
    "# - Common use cases:\n",
    "#   * Selecting specific samples from dataset: X_train[selected_indices]\n",
    "#   * Random sampling: X[np.random.choice(len(X), size=100)]\n",
    "#   * Feature selection: features[:, important_feature_indices]\n",
    "#   * Creating mini-batches for training\n",
    "#   * Reordering data based on custom sequence\n",
    "#   * Extracting elements at specific positions\n",
    "# - Example:\n",
    "#   data = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "#   \n",
    "#   # Select elements at indices 0, 3, 7\n",
    "#   indices = [0, 3, 7]\n",
    "#   selected = data[indices]  # [10, 40, 80]\n",
    "#   \n",
    "#   # Select in different order\n",
    "#   indices = [7, 0, 3]\n",
    "#   reordered = data[indices]  # [80, 10, 40]\n",
    "#   \n",
    "#   # Duplicate elements\n",
    "#   indices = [0, 0, 1, 1]\n",
    "#   duplicated = data[indices]  # [10, 10, 20, 20]\n",
    "#   \n",
    "#   # Random sampling (mini-batch creation)\n",
    "#   batch_size = 32\n",
    "#   random_indices = np.random.choice(len(data), size=batch_size, replace=False)\n",
    "#   mini_batch = data[random_indices]\n",
    "#   \n",
    "#   # 2D fancy indexing\n",
    "#   matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "#   row_indices = [0, 2]    # Select rows 0 and 2\n",
    "#   selected_rows = matrix[row_indices]  # [[1, 2, 3], [7, 8, 9]]\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Indices ki array ya list use karke doosre array se\n",
    "#   ek saath multiple elements select karta hai. Selected elements ka naya\n",
    "#   array return karta hai.\n",
    "# - Kab use karein: Conditions, indices, ya patterns ke basis par\n",
    "#   non-contiguous ya specific elements select karne ke liye bahut powerful\n",
    "#   hai. ML mein data sampling, feature selection, aur batch processing ke\n",
    "#   liye essential hai.\n",
    "# - Syntax: array[index_array] jahan index_array indices ki list/array hai\n",
    "# - Key features:\n",
    "#   * Kisi bhi order mein elements select kar sakte hain\n",
    "#   * Elements duplicate karne ke liye indices repeat kar sakte hain\n",
    "#   * Multi-dimensional arrays ke saath use kar sakte hain\n",
    "#   * Hamesha COPY return karta hai (view nahi)\n",
    "# - Common use cases:\n",
    "#   * Dataset se specific samples select karna: X_train[selected_indices]\n",
    "#   * Random sampling: X[np.random.choice(len(X), size=100)]\n",
    "#   * Feature selection: features[:, important_feature_indices]\n",
    "#   * Training ke liye mini-batches banana\n",
    "#   * Custom sequence ke basis par data reorder karna\n",
    "#   * Specific positions par elements extract karna\n",
    "# - Example:\n",
    "#   data = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "#   \n",
    "#   # Indices 0, 3, 7 par elements select karo\n",
    "#   indices = [0, 3, 7]\n",
    "#   selected = data[indices]  # [10, 40, 80]\n",
    "#   \n",
    "#   # Alag order mein select karo\n",
    "#   indices = [7, 0, 3]\n",
    "#   reordered = data[indices]  # [80, 10, 40]\n",
    "#   \n",
    "#   # Elements duplicate karo\n",
    "#   indices = [0, 0, 1, 1]\n",
    "#   duplicated = data[indices]  # [10, 10, 20, 20]\n",
    "#   \n",
    "#   # Random sampling (mini-batch creation)\n",
    "#   batch_size = 32\n",
    "#   random_indices = np.random.choice(len(data), size=batch_size, replace=False)\n",
    "#   mini_batch = data[random_indices]\n",
    "#   \n",
    "#   # 2D fancy indexing\n",
    "#   matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "#   row_indices = [0, 2]    # Rows 0 aur 2 select karo\n",
    "#   selected_rows = matrix[row_indices]  # [[1, 2, 3], [7, 8, 9]]\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "idx = [0, 2, 4]  # Indices to select: first, third, and fifth elements\n",
    "\n",
    "print(arr[idx])\n",
    "# Output: [1 3 5]\n",
    "# Selects elements at positions 0, 2, 4 from [1, 2, 3, 4, 5]\n",
    "# arr[0]=1, arr[2]=3, arr[4]=5\n",
    "\n",
    "# =============================================================================\n",
    "# BOOLEAN INDEXING (CONDITIONAL SELECTION)\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Uses boolean conditions to filter and select elements from\n",
    "#   an array. Creates a boolean mask (True/False array) and returns elements\n",
    "#   where mask is True.\n",
    "# - Why use it: Most powerful feature for data filtering in ML. Essential for\n",
    "#   data cleaning, outlier removal, feature engineering, and conditional\n",
    "#   operations. Works like SQL WHERE clause or pandas query.\n",
    "# - How it works:\n",
    "#   1. Condition creates boolean array: arr > 2 → [False, False, True, True, True]\n",
    "#   2. Boolean array acts as mask: True = include, False = exclude\n",
    "#   3. Returns new array with only True elements\n",
    "# - Key features:\n",
    "#   * Can use any comparison: >, <, >=, <=, ==, !=\n",
    "#   * Can combine conditions: & (and), | (or), ~ (not)\n",
    "#   * Always returns a COPY (not a view)\n",
    "#   * Result length depends on condition (can be 0 to original length)\n",
    "# - Common use cases:\n",
    "#   * Data filtering: Remove outliers, select specific ranges\n",
    "#   * Feature engineering: Create binary features based on conditions\n",
    "#   * Data cleaning: Remove invalid values (NaN, negative values, etc.)\n",
    "#   * Conditional statistics: Mean of positive values, count of outliers\n",
    "#   * Threshold-based selection: Select predictions above confidence threshold\n",
    "#   * Class separation: Select samples of specific class\n",
    "# - Example:\n",
    "#   ages = np.array([18, 25, 32, 45, 67, 23, 51])\n",
    "#   \n",
    "#   # Filter adults over 30\n",
    "#   adults = ages[ages > 30]  # [32, 45, 67, 51]\n",
    "#   \n",
    "#   # Filter specific range (20-40 years old)\n",
    "#   young_adults = ages[(ages >= 20) & (ages <= 40)]  # [25, 32, 23]\n",
    "#   \n",
    "#   # Exclude seniors (not over 65)\n",
    "#   non_seniors = ages[~(ages > 65)]  # [18, 25, 32, 45, 23, 51]\n",
    "#   \n",
    "#   # Multiple conditions with OR\n",
    "#   extremes = ages[(ages < 25) | (ages > 60)]  # [18, 67, 23]\n",
    "#   \n",
    "#   # ML example: Filter predictions\n",
    "#   predictions = np.array([0.2, 0.8, 0.6, 0.3, 0.9])\n",
    "#   high_confidence = predictions[predictions > 0.5]  # [0.8, 0.6, 0.9]\n",
    "#   \n",
    "#   # Data cleaning: Remove negative values\n",
    "#   data_with_errors = np.array([5, -1, 10, -3, 15, 20])\n",
    "#   clean_data = data_with_errors[data_with_errors >= 0]  # [5, 10, 15, 20]\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Boolean conditions use karke array se elements filter\n",
    "#   aur select karta hai. Boolean mask (True/False array) banata hai aur\n",
    "#   elements return karta hai jahan mask True hai.\n",
    "# - Kab use karein: ML mein data filtering ke liye sabse powerful feature hai.\n",
    "#   Data cleaning, outlier removal, feature engineering, aur conditional\n",
    "#   operations ke liye essential hai. SQL WHERE clause ya pandas query jaisa\n",
    "#   kaam karta hai.\n",
    "# - Kaise kaam karta hai:\n",
    "#   1. Condition boolean array banata hai: arr > 2 → [False, False, True, True, True]\n",
    "#   2. Boolean array mask ki tarah kaam karta hai: True = include, False = exclude\n",
    "#   3. Sirf True elements ka naya array return karta hai\n",
    "# - Key features:\n",
    "#   * Koi bhi comparison use kar sakte hain: >, <, >=, <=, ==, !=\n",
    "#   * Conditions combine kar sakte hain: & (and), | (or), ~ (not)\n",
    "#   * Hamesha COPY return karta hai (view nahi)\n",
    "#   * Result ki length condition par depend karti hai (0 se original length tak)\n",
    "# - Common use cases:\n",
    "#   * Data filtering: Outliers remove karna, specific ranges select karna\n",
    "#   * Feature engineering: Conditions ke basis par binary features banana\n",
    "#   * Data cleaning: Invalid values remove karna (NaN, negative values, etc.)\n",
    "#   * Conditional statistics: Positive values ka mean, outliers ki count\n",
    "#   * Threshold-based selection: Confidence threshold se upar predictions select karna\n",
    "#   * Class separation: Specific class ke samples select karna\n",
    "# - Example:\n",
    "#   ages = np.array([18, 25, 32, 45, 67, 23, 51])\n",
    "#   \n",
    "#   # 30 se zyada umar wale adults filter karo\n",
    "#   adults = ages[ages > 30]  # [32, 45, 67, 51]\n",
    "#   \n",
    "#   # Specific range filter karo (20-40 saal ke log)\n",
    "#   young_adults = ages[(ages >= 20) & (ages <= 40)]  # [25, 32, 23]\n",
    "#   \n",
    "#   # Seniors exclude karo (65 se zyada nahi)\n",
    "#   non_seniors = ages[~(ages > 65)]  # [18, 25, 32, 45, 23, 51]\n",
    "#   \n",
    "#   # OR ke saath multiple conditions\n",
    "#   extremes = ages[(ages < 25) | (ages > 60)]  # [18, 67, 23]\n",
    "#   \n",
    "#   # ML example: Predictions filter karo\n",
    "#   predictions = np.array([0.2, 0.8, 0.6, 0.3, 0.9])\n",
    "#   high_confidence = predictions[predictions > 0.5]  # [0.8, 0.6, 0.9]\n",
    "#   \n",
    "#   # Data cleaning: Negative values remove karo\n",
    "#   data_with_errors = np.array([5, -1, 10, -3, 15, 20])\n",
    "#   clean_data = data_with_errors[data_with_errors >= 0]  # [5, 10, 15, 20]\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Boolean Indexing Example 1: Greater than condition\n",
    "print(arr[arr > 2])\n",
    "# Output: [3 4 5]\n",
    "# Process:\n",
    "# 1. arr > 2 creates: [False, False, True, True, True]\n",
    "# 2. Selects elements where True: [3, 4, 5]\n",
    "\n",
    "# Boolean Indexing Example 2: Even numbers (modulo operation)\n",
    "print(arr[arr % 2 == 0])\n",
    "# Output: [2 4]\n",
    "# Process:\n",
    "# 1. arr % 2 gives remainders: [1, 0, 1, 0, 1]\n",
    "# 2. arr % 2 == 0 creates: [False, True, False, True, False]\n",
    "# 3. Selects even numbers: [2, 4]\n",
    "\n",
    "# Boolean Indexing Example 3: Odd numbers (inequality)\n",
    "print(arr[arr % 2 != 0])\n",
    "# Output: [1 3 5]\n",
    "# Process:\n",
    "# 1. arr % 2 gives remainders: [1, 0, 1, 0, 1]\n",
    "# 2. arr % 2 != 0 creates: [True, False, True, False, True]\n",
    "# 3. Selects odd numbers: [1, 3, 5]\n",
    "\n",
    "# =============================================================================\n",
    "# ADVANCED BOOLEAN INDEXING PATTERNS\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Combining multiple conditions (use parentheses!):\n",
    "# \n",
    "# # AND condition: Both must be True\n",
    "# result = arr[(arr > 2) & (arr < 5)]  # [3, 4]\n",
    "# \n",
    "# # OR condition: At least one must be True\n",
    "# result = arr[(arr < 2) | (arr > 4)]  # [1, 5]\n",
    "# \n",
    "# # NOT condition: Inverse of condition\n",
    "# result = arr[~(arr > 3)]  # [1, 2, 3] - elements NOT greater than 3\n",
    "# \n",
    "# # Complex nested conditions\n",
    "# result = arr[((arr > 1) & (arr < 4)) | (arr == 5)]  # [2, 3, 5]\n",
    "# \n",
    "# # Using np.where for conditional replacement\n",
    "# modified = np.where(arr > 3, 100, arr)  # Replace >3 with 100, keep rest\n",
    "# # Result: [1, 2, 3, 100, 100]\n",
    "# \n",
    "# HINGLISH:\n",
    "# Multiple conditions combine karna (parentheses use karo!):\n",
    "# \n",
    "# # AND condition: Dono True hone chahiye\n",
    "# result = arr[(arr > 2) & (arr < 5)]  # [3, 4]\n",
    "# \n",
    "# # OR condition: Kam se kam ek True hona chahiye\n",
    "# result = arr[(arr < 2) | (arr > 4)]  # [1, 5]\n",
    "# \n",
    "# # NOT condition: Condition ka inverse\n",
    "# result = arr[~(arr > 3)]  # [1, 2, 3] - elements jo 3 se bade NAHI hain\n",
    "# \n",
    "# # Complex nested conditions\n",
    "# result = arr[((arr > 1) & (arr < 4)) | (arr == 5)]  # [2, 3, 5]\n",
    "# \n",
    "# # Conditional replacement ke liye np.where\n",
    "# modified = np.where(arr > 3, 100, arr)  # >3 ko 100 se replace karo, baaki rakho\n",
    "# # Result: [1, 2, 3, 100, 100]\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# PRACTICAL ML EXAMPLE: DATA FILTERING\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Real-world scenario: Cleaning and filtering a dataset\n",
    "#\n",
    "# # Simulated dataset: [age, income, credit_score]\n",
    "# customers = np.array([[25, 45000, 720],\n",
    "#                       [35, 75000, 680],\n",
    "#                       [45, -5000, 750],  # Error: negative income\n",
    "#                       [52, 90000, 850],\n",
    "#                       [28, 55000, 0]])   # Error: invalid credit score\n",
    "# \n",
    "# # Filter 1: Remove invalid income (negatives)\n",
    "# valid_income = customers[customers[:, 1] >= 0]\n",
    "# \n",
    "# # Filter 2: Select only valid credit scores (300-850)\n",
    "# valid_credit = customers[(customers[:, 2] >= 300) & (customers[:, 2] <= 850)]\n",
    "# \n",
    "# # Filter 3: High-value customers (income > 60k AND credit > 700)\n",
    "# high_value = customers[(customers[:, 1] > 60000) & (customers[:, 2] > 700)]\n",
    "# \n",
    "# # Filter 4: Young professionals (age 25-35 AND income > 50k)\n",
    "# young_prof = customers[(customers[:, 0] >= 25) & \n",
    "#                        (customers[:, 0] <= 35) & \n",
    "#                        (customers[:, 1] > 50000)]\n",
    "#\n",
    "# HINGLISH:\n",
    "# Real-world scenario: Dataset ko clean aur filter karna\n",
    "#\n",
    "# # Simulated dataset: [age, income, credit_score]\n",
    "# customers = np.array([[25, 45000, 720],\n",
    "#                       [35, 75000, 680],\n",
    "#                       [45, -5000, 750],  # Error: negative income\n",
    "#                       [52, 90000, 850],\n",
    "#                       [28, 55000, 0]])   # Error: invalid credit score\n",
    "# \n",
    "# # Filter 1: Invalid income remove karo (negatives)\n",
    "# valid_income = customers[customers[:, 1] >= 0]\n",
    "# \n",
    "# # Filter 2: Sirf valid credit scores select karo (300-850)\n",
    "# valid_credit = customers[(customers[:, 2] >= 300) & (customers[:, 2] <= 850)]\n",
    "# \n",
    "# # Filter 3: High-value customers (income > 60k AUR credit > 700)\n",
    "# high_value = customers[(customers[:, 1] > 60000) & (customers[:, 2] > 700)]\n",
    "# \n",
    "# # Filter 4: Young professionals (age 25-35 AUR income > 50k)\n",
    "# young_prof = customers[(customers[:, 0] >= 25) & \n",
    "#                        (customers[:, 0] <= 35) & \n",
    "#                        (customers[:, 1] > 50000)]\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4670e572-9497-4a00-9acd-c7e0dd94a7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5]\n",
      "[2 3 4 5 6 7 8]\n",
      "[1 2 3 4 5]\n",
      "[1 3 5 7]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# NUMPY ARRAY SLICING - EXTRACTING SUBARRAYS\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# CREATING BASE ARRAY FOR SLICING DEMONSTRATIONS\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Creates a 1D array with 8 sequential elements for\n",
    "#   demonstrating various slicing operations\n",
    "# - Why use it: Slicing is fundamental for extracting portions of data,\n",
    "#   creating training/validation splits, selecting feature ranges, and\n",
    "#   working with subsequences in time series or sequences\n",
    "# - Note: Unlike indexing (which returns single elements), slicing returns\n",
    "#   a VIEW of the original array (not a copy) for efficiency\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: 8 sequential elements ke saath ek 1D array banata hai\n",
    "#   various slicing operations demonstrate karne ke liye\n",
    "# - Kab use karein: Data ke portions extract karne, training/validation\n",
    "#   splits banana, feature ranges select karne, aur time series ya sequences\n",
    "#   mein subsequences ke saath kaam karne ke liye slicing fundamental hai\n",
    "# - Note: Indexing ke unlike (jo single elements return karta hai), slicing\n",
    "#   efficiency ke liye original array ka VIEW return karta hai (copy nahi)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "arr = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "# Array visualization with indices:\n",
    "# Index:    0  1  2  3  4  5  6  7\n",
    "# Value:    1  2  3  4  5  6  7  8\n",
    "# Negative: -8 -7 -6 -5 -4 -3 -2 -1\n",
    "\n",
    "# =============================================================================\n",
    "# SLICING SYNTAX: arr[start:stop:step]\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# General syntax: array[start:stop:step]\n",
    "# - start: Index where slice begins (inclusive) - default is 0\n",
    "# - stop: Index where slice ends (exclusive, not included) - default is len(array)\n",
    "# - step: Increment between indices (default is 1)\n",
    "# \n",
    "# Key rules:\n",
    "# 1. start is INCLUSIVE - element at start index IS included\n",
    "# 2. stop is EXCLUSIVE - element at stop index is NOT included\n",
    "# 3. Omitting values uses defaults: [:] means entire array\n",
    "# 4. Negative indices count from end: -1 is last element\n",
    "# 5. Slicing returns a VIEW (not copy) - modifying slice affects original\n",
    "# 6. Out-of-bounds indices don't raise errors (unlike indexing)\n",
    "#\n",
    "# HINGLISH:\n",
    "# General syntax: array[start:stop:step]\n",
    "# - start: Index jahan slice shuru hota hai (inclusive) - default 0 hai\n",
    "# - stop: Index jahan slice khatam hota hai (exclusive, include nahi hota) - default len(array) hai\n",
    "# - step: Indices ke beech increment (default 1 hai)\n",
    "# \n",
    "# Key rules:\n",
    "# 1. start INCLUSIVE hai - start index par element include hota hai\n",
    "# 2. stop EXCLUSIVE hai - stop index par element include NAHI hota\n",
    "# 3. Values omit karne par defaults use hote hain: [:] matlab poora array\n",
    "# 4. Negative indices end se count karte hain: -1 last element hai\n",
    "# 5. Slicing VIEW return karta hai (copy nahi) - slice modify karne se original affect hota hai\n",
    "# 6. Out-of-bounds indices error nahi raise karte (indexing ke unlike)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# =============================================================================\n",
    "# SLICE 1: arr[start:stop] - Range Selection\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Extracts elements from start index (inclusive) to stop\n",
    "#   index (exclusive). The stop element itself is NOT included.\n",
    "# - Syntax: arr[start:stop]\n",
    "# - Why use it: Most common slicing pattern for selecting a range of elements.\n",
    "#   Essential for creating subsets, batches, or extracting specific ranges.\n",
    "# - Formula: Returns elements at indices [start, start+1, ..., stop-1]\n",
    "# - Length of result: stop - start elements\n",
    "# - Common use cases:\n",
    "#   * Creating train/test splits: X_train = X[:800], X_test = X[800:]\n",
    "#   * Extracting time windows: recent_data = timeseries[-100:]\n",
    "#   * Selecting feature ranges: features = data[:, 10:50]\n",
    "#   * Creating mini-batches: batch = data[i:i+batch_size]\n",
    "#   * Removing first/last elements: middle = arr[1:-1]\n",
    "# - Example:\n",
    "#   data = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "#   data[2:6]    # [30, 40, 50, 60] - indices 2,3,4,5 (6 excluded)\n",
    "#   data[0:3]    # [10, 20, 30] - first 3 elements\n",
    "#   data[5:9]    # [60, 70, 80, 90] - last 4 elements\n",
    "#   data[-4:-1]  # [60, 70, 80] - 4th-last to 2nd-last (last excluded)\n",
    "#   \n",
    "#   # ML example: Train-test split (80-20)\n",
    "#   dataset = np.arange(1000)\n",
    "#   split_idx = int(0.8 * len(dataset))  # 800\n",
    "#   train = dataset[:split_idx]    # First 800 samples\n",
    "#   test = dataset[split_idx:]     # Last 200 samples\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Start index (inclusive) se stop index (exclusive) tak\n",
    "#   elements extract karta hai. Stop element khud include NAHI hota.\n",
    "# - Syntax: arr[start:stop]\n",
    "# - Kab use karein: Elements ki range select karne ka sabse common slicing\n",
    "#   pattern. Subsets, batches banana, ya specific ranges extract karne ke\n",
    "#   liye essential hai.\n",
    "# - Formula: Indices [start, start+1, ..., stop-1] par elements return karta hai\n",
    "# - Result ki length: stop - start elements\n",
    "# - Common use cases:\n",
    "#   * Train/test splits banana: X_train = X[:800], X_test = X[800:]\n",
    "#   * Time windows extract karna: recent_data = timeseries[-100:]\n",
    "#   * Feature ranges select karna: features = data[:, 10:50]\n",
    "#   * Mini-batches banana: batch = data[i:i+batch_size]\n",
    "#   * Pehle/aakhri elements remove karna: middle = arr[1:-1]\n",
    "# - Example:\n",
    "#   data = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "#   data[2:6]    # [30, 40, 50, 60] - indices 2,3,4,5 (6 excluded)\n",
    "#   data[0:3]    # [10, 20, 30] - pehle 3 elements\n",
    "#   data[5:9]    # [60, 70, 80, 90] - aakhri 4 elements\n",
    "#   data[-4:-1]  # [60, 70, 80] - 4th-last se 2nd-last (last excluded)\n",
    "#   \n",
    "#   # ML example: Train-test split (80-20)\n",
    "#   dataset = np.arange(1000)\n",
    "#   split_idx = int(0.8 * len(dataset))  # 800\n",
    "#   train = dataset[:split_idx]    # Pehle 800 samples\n",
    "#   test = dataset[split_idx:]     # Aakhri 200 samples\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(arr[0:5])\n",
    "# Output: [1 2 3 4 5]\n",
    "# Extracts elements from index 0 to 4 (5 is excluded)\n",
    "# Visual: [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "#          ↑           ↑\n",
    "#       start=0    stop=5 (not included)\n",
    "# Returns 5 elements: indices 0,1,2,3,4\n",
    "\n",
    "# =============================================================================\n",
    "# SLICE 2: arr[start:] - From Start to End\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Extracts all elements from start index to the end of array.\n",
    "#   Omitting the stop value means \"go until the end\".\n",
    "# - Syntax: arr[start:]\n",
    "# - Why use it: Convenient shorthand for getting all remaining elements after\n",
    "#   a certain point. Very common in data splitting and sequence processing.\n",
    "# - Equivalent to: arr[start:len(arr)]\n",
    "# - Common use cases:\n",
    "#   * Getting test set after train split: X_test = X[train_size:]\n",
    "#   * Removing first n elements: without_header = data[5:]\n",
    "#   * Processing from middle onwards: second_half = arr[len(arr)//2:]\n",
    "#   * Skipping warmup period: actual_data = timeseries[warmup_steps:]\n",
    "#   * Getting tail of sequence: recent = data[-100:]\n",
    "# - Example:\n",
    "#   prices = np.array([100, 102, 98, 105, 110, 108, 115])\n",
    "#   prices[3:]    # [105, 110, 108, 115] - from index 3 to end\n",
    "#   prices[-3:]   # [108, 115] - last 3 elements (from 3rd-last to end)\n",
    "#   prices[0:]    # [100, 102, 98, 105, 110, 108, 115] - entire array\n",
    "#   \n",
    "#   # ML example: Create validation set\n",
    "#   X_train_full = np.arange(1000)\n",
    "#   val_size = 200\n",
    "#   X_train = X_train_full[:-val_size]  # First 800\n",
    "#   X_val = X_train_full[-val_size:]    # Last 200 (from -200 to end)\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Start index se array ke end tak saare elements extract\n",
    "#   karta hai. Stop value omit karne ka matlab hai \"end tak jao\".\n",
    "# - Syntax: arr[start:]\n",
    "# - Kab use karein: Kisi point ke baad ke saare remaining elements lene ka\n",
    "#   convenient shorthand. Data splitting aur sequence processing mein bahut\n",
    "#   common hai.\n",
    "# - Equivalent to: arr[start:len(arr)]\n",
    "# - Common use cases:\n",
    "#   * Train split ke baad test set lena: X_test = X[train_size:]\n",
    "#   * Pehle n elements remove karna: without_header = data[5:]\n",
    "#   * Middle se aage process karna: second_half = arr[len(arr)//2:]\n",
    "#   * Warmup period skip karna: actual_data = timeseries[warmup_steps:]\n",
    "#   * Sequence ki tail lena: recent = data[-100:]\n",
    "# - Example:\n",
    "#   prices = np.array([100, 102, 98, 105, 110, 108, 115])\n",
    "#   prices[3:]    # [105, 110, 108, 115] - index 3 se end tak\n",
    "#   prices[-3:]   # [108, 115] - aakhri 3 elements (3rd-last se end tak)\n",
    "#   prices[0:]    # [100, 102, 98, 105, 110, 108, 115] - poora array\n",
    "#   \n",
    "#   # ML example: Validation set banana\n",
    "#   X_train_full = np.arange(1000)\n",
    "#   val_size = 200\n",
    "#   X_train = X_train_full[:-val_size]  # Pehle 800\n",
    "#   X_val = X_train_full[-val_size:]    # Aakhri 200 (-200 se end tak)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(arr[1:])\n",
    "# Output: [2 3 4 5 6 7 8]\n",
    "# Extracts elements from index 1 to end\n",
    "# Visual: [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "#             ↑                 ↑\n",
    "#          start=1            end\n",
    "# Returns 7 elements: indices 1,2,3,4,5,6,7\n",
    "\n",
    "# =============================================================================\n",
    "# SLICE 3: arr[:stop] - From Beginning to Stop\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Extracts elements from the beginning (index 0) up to but\n",
    "#   not including the stop index. Omitting start means \"start from beginning\".\n",
    "# - Syntax: arr[:stop]\n",
    "# - Why use it: Convenient for getting first n elements. Very common for\n",
    "#   creating training sets, limiting data size, or getting prefixes.\n",
    "# - Equivalent to: arr[0:stop]\n",
    "# - Common use cases:\n",
    "#   * Getting first n samples: X_train = X[:train_size]\n",
    "#   * Limiting dataset size: limited_data = large_data[:10000]\n",
    "#   * Getting top-k results: top_10 = sorted_results[:10]\n",
    "#   * Creating training set: train = data[:int(0.8*len(data))]\n",
    "#   * Removing last elements: without_tail = arr[:-5]\n",
    "# - Example:\n",
    "#   scores = np.array([95, 87, 92, 78, 88, 94, 85, 90])\n",
    "#   scores[:3]    # [95, 87, 92] - first 3 elements\n",
    "#   scores[:5]    # [95, 87, 92, 78, 88] - first 5 elements\n",
    "#   scores[:-2]   # [95, 87, 92, 78, 88, 94] - all except last 2\n",
    "#   \n",
    "#   # ML example: Quick prototyping with small subset\n",
    "#   full_dataset = np.arange(100000)\n",
    "#   quick_test = full_dataset[:1000]  # Just first 1000 for testing\n",
    "#   \n",
    "#   # Getting top predictions\n",
    "#   probs = np.array([0.1, 0.9, 0.3, 0.8, 0.6])\n",
    "#   sorted_indices = np.argsort(probs)[::-1]  # Sort descending\n",
    "#   top_3_indices = sorted_indices[:3]  # Get top 3\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Beginning (index 0) se lekar stop index tak (excluding)\n",
    "#   elements extract karta hai. Start omit karne ka matlab \"beginning se shuru\".\n",
    "# - Syntax: arr[:stop]\n",
    "#- Kab use karein: Pehle n elements lene ka convenient tarika. Training sets\n",
    "#   banane, data size limit karne, ya prefixes lene ke liye bahut common.\n",
    "# - Equivalent to: arr[0:stop]\n",
    "# - Common use cases:\n",
    "#   * Pehle n samples lena: X_train = X[:train_size]\n",
    "#   * Dataset size limit karna: limited_data = large_data[:10000]\n",
    "#   * Top-k results lena: top_10 = sorted_results[:10]\n",
    "#   * Training set banana: train = data[:int(0.8*len(data))]\n",
    "#   * Aakhri elements remove karna: without_tail = arr[:-5]\n",
    "# - Example:\n",
    "#   scores = np.array([95, 87, 92, 78, 88, 94, 85, 90])\n",
    "#   scores[:3]    # [95, 87, 92] - pehle 3 elements\n",
    "#   scores[:5]    # [95, 87, 92, 78, 88] - pehle 5 elements\n",
    "#   scores[:-2]   # [95, 87, 92, 78, 88, 94] - aakhri 2 ko chhod ke sab\n",
    "#   \n",
    "#   # ML example: Small subset ke saath quick prototyping\n",
    "#   full_dataset = np.arange(100000)\n",
    "#   quick_test = full_dataset[:1000]  # Testing ke liye sirf pehle 1000\n",
    "#   \n",
    "#   # Top predictions lena\n",
    "#   probs = np.array([0.1, 0.9, 0.3, 0.8, 0.6])\n",
    "#   sorted_indices = np.argsort(probs)[::-1]  # Descending sort\n",
    "#   top_3_indices = sorted_indices[:3]  # Top 3 lo\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(arr[:5])\n",
    "# Output: [1 2 3 4 5]\n",
    "# Extracts elements from beginning to index 4 (5 is excluded)\n",
    "# Visual: [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "#          ↑           ↑\n",
    "#       start=0    stop=5 (not included)\n",
    "# Returns 5 elements: indices 0,1,2,3,4\n",
    "# Same output as arr[0:5]\n",
    "\n",
    "# =============================================================================\n",
    "# SLICE 4: arr[::step] - Every Nth Element\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Extracts elements at regular intervals (every step-th element)\n",
    "#   from the entire array. Omitting start and stop means \"entire array\".\n",
    "# - Syntax: arr[::step] or arr[start:stop:step]\n",
    "# - Why use it: Essential for downsampling data, selecting every nth sample,\n",
    "#   creating alternating patterns, or reversing arrays (step=-1).\n",
    "# - Step mechanics:\n",
    "#   * step > 0: Move forward through array\n",
    "#   * step < 0: Move backward through array (reversal)\n",
    "#   * step = 2: Every other element (0, 2, 4, 6, ...)\n",
    "#   * step = 3: Every third element (0, 3, 6, 9, ...)\n",
    "# - Common use cases:\n",
    "#   * Downsampling time series: downsampled = signal[::10]\n",
    "#   * Getting odd/even indices: even = arr[::2], odd = arr[1::2]\n",
    "#   * Reversing arrays: reversed_arr = arr[::-1]\n",
    "#   * Selecting alternating features: selected = features[:, ::2]\n",
    "#   * Creating training batches with stride: batch = data[start::stride]\n",
    "#   * Decimation in signal processing: reduced = data[::decimation_factor]\n",
    "# - Example:\n",
    "#   sequence = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "#   sequence[::2]    # [0, 2, 4, 6, 8] - every 2nd element (even indices)\n",
    "#   sequence[1::2]   # [1, 3, 5, 7, 9] - every 2nd starting from 1 (odd indices)\n",
    "#   sequence[::3]    # [0, 3, 6, 9] - every 3rd element\n",
    "#   sequence[::-1]   # [9, 8, 7, 6, 5, 4, 3, 2, 1, 0] - reversed\n",
    "#   sequence[::-2]   # [9, 7, 5, 3, 1] - every 2nd element, reversed\n",
    "#   \n",
    "#   # ML example: Downsampling high-frequency sensor data\n",
    "#   sensor_data = np.sin(np.linspace(0, 10, 1000))  # 1000 samples\n",
    "#   downsampled = sensor_data[::10]  # Keep every 10th sample (100 samples)\n",
    "#   \n",
    "#   # Creating train/val splits with alternating samples\n",
    "#   indices = np.arange(1000)\n",
    "#   train_indices = indices[::2]  # Even indices for training\n",
    "#   val_indices = indices[1::2]   # Odd indices for validation\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Poore array se regular intervals par (har step-th\n",
    "#   element) elements extract karta hai. Start aur stop omit karne ka matlab\n",
    "#   \"poora array\".\n",
    "# - Syntax: arr[::step] ya arr[start:stop:step]\n",
    "# - Kab use karein: Data downsample karne, har nth sample select karne,\n",
    "#   alternating patterns banana, ya arrays reverse karne (step=-1) ke liye\n",
    "#   essential hai.\n",
    "# - Step mechanics:\n",
    "#   * step > 0: Array mein aage move karo\n",
    "#   * step < 0: Array mein peeche move karo (reversal)\n",
    "#   * step = 2: Har doosra element (0, 2, 4, 6, ...)\n",
    "#   * step = 3: Har teesra element (0, 3, 6, 9, ...)\n",
    "# - Common use cases:\n",
    "#   * Time series downsample karna: downsampled = signal[::10]\n",
    "#   * Odd/even indices lena: even = arr[::2], odd = arr[1::2]\n",
    "#   * Arrays reverse karna: reversed_arr = arr[::-1]\n",
    "#   * Alternating features select karna: selected = features[:, ::2]\n",
    "#   * Stride ke saath training batches banana: batch = data[start::stride]\n",
    "#   * Signal processing mein decimation: reduced = data[::decimation_factor]\n",
    "# - Example:\n",
    "#   sequence = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "#   sequence[::2]    # [0, 2, 4, 6, 8] - har 2nd element (even indices)\n",
    "#   sequence[1::2]   # [1, 3, 5, 7, 9] - 1 se shuru karke har 2nd (odd indices)\n",
    "#   sequence[::3]    # [0, 3, 6, 9] - har 3rd element\n",
    "#   sequence[::-1]   # [9, 8, 7, 6, 5, 4, 3, 2, 1, 0] - reversed\n",
    "#   sequence[::-2]   # [9, 7, 5, 3, 1] - har 2nd element, reversed\n",
    "#   \n",
    "#   # ML example: High-frequency sensor data downsample karna\n",
    "#   sensor_data = np.sin(np.linspace(0, 10, 1000))  # 1000 samples\n",
    "#   downsampled = sensor_data[::10]  # Har 10th sample rakho (100 samples)\n",
    "#   \n",
    "#   # Alternating samples ke saath train/val splits banana\n",
    "#   indices = np.arange(1000)\n",
    "#   train_indices = indices[::2]  # Training ke liye even indices\n",
    "#   val_indices = indices[1::2]   # Validation ke liye odd indices\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(arr[::2])\n",
    "# Output: [1 3 5 7]\n",
    "# Extracts every 2nd element (step=2) from entire array\n",
    "# Visual: [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "#          ↑     ↑     ↑     ↑\n",
    "#       idx 0   2     4     6\n",
    "# Returns 4 elements: indices 0,2,4,6\n",
    "# Starts at index 0, then jumps by 2 each time\n",
    "\n",
    "# =============================================================================\n",
    "# ADVANCED SLICING PATTERNS\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Combining start, stop, and step:\n",
    "# \n",
    "# # Get every 2nd element between indices 1 and 7\n",
    "# arr[1:7:2]  # [2, 4, 6] - indices 1,3,5\n",
    "# \n",
    "# # Get every 3rd element from start to index 8\n",
    "# arr[:8:3]  # [1, 4, 7] - indices 0,3,6\n",
    "# \n",
    "# # Get every 2nd element from index 2 to end\n",
    "# arr[2::2]  # [3, 5, 7] - indices 2,4,6\n",
    "# \n",
    "# # Reverse entire array\n",
    "# arr[::-1]  # [8, 7, 6, 5, 4, 3, 2, 1]\n",
    "# \n",
    "# # Reverse portion of array (indices 2 to 6)\n",
    "# arr[6:2:-1]  # [7, 6, 5, 4] - reverse from index 6 to 3\n",
    "# \n",
    "# # Get last 4 elements in reverse\n",
    "# arr[-1:-5:-1]  # [8, 7, 6, 5]\n",
    "# \n",
    "# # Copy entire array (creates actual copy, not view)\n",
    "# arr_copy = arr[:]  # Full copy\n",
    "# \n",
    "# HINGLISH:\n",
    "# Start, stop, aur step combine karna:\n",
    "# \n",
    "# # Indices 1 aur 7 ke beech har 2nd element lo\n",
    "# arr[1:7:2]  # [2, 4, 6] - indices 1,3,5\n",
    "# \n",
    "# # Start se index 8 tak har 3rd element lo\n",
    "# arr[:8:3]  # [1, 4, 7] - indices 0,3,6\n",
    "# \n",
    "# # Index 2 se end tak har 2nd element lo\n",
    "# arr[2::2]  # [3, 5, 7] - indices 2,4,6\n",
    "# \n",
    "# # Poora array reverse karo\n",
    "# arr[::-1]  # [8, 7, 6, 5, 4, 3, 2, 1]\n",
    "# \n",
    "# # Array ke portion ko reverse karo (indices 2 se 6)\n",
    "# arr[6:2:-1]  # [7, 6, 5, 4] - index 6 se 3 tak reverse\n",
    "# \n",
    "# # Aakhri 4 elements reverse mein lo\n",
    "# arr[-1:-5:-1]  # [8, 7, 6, 5]\n",
    "# \n",
    "# # Poore array ki copy banao (actual copy, view nahi)\n",
    "# arr_copy = arr[:]  # Full copy\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# 2D ARRAY SLICING\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Slicing works on each dimension independently:\n",
    "# \n",
    "# matrix = np.array([[1, 2, 3, 4],\n",
    "#                    [5, 6, 7, 8],\n",
    "#                    [9, 10, 11, 12]])\n",
    "# \n",
    "# # Get first 2 rows, all columns\n",
    "# matrix[:2, :]  # [[1,2,3,4], [5,6,7,8]]\n",
    "# \n",
    "# # Get all rows, first 3 columns\n",
    "# matrix[:, :3]  # [[1,2,3], [5,6,7], [9,10,11]]\n",
    "# \n",
    "# # Get rows 1-2, columns 1-3\n",
    "# matrix[1:3, 1:3]  # [[6,7], [10,11]]\n",
    "# \n",
    "# # Get every other row, every other column\n",
    "# matrix[::2, ::2]  # [[1,3], [9,11]]\n",
    "# \n",
    "# # Reverse rows, keep columns\n",
    "# matrix[::-1, :]  # [[9,10,11,12], [5,6,7,8], [1,2,3,4]]\n",
    "# \n",
    "# HINGLISH:\n",
    "# Har dimension par independently slicing kaam karti hai:\n",
    "# \n",
    "# matrix = np.array([[1, 2, 3, 4],\n",
    "#                    [5, 6, 7, 8],\n",
    "#                    [9, 10, 11, 12]])\n",
    "# \n",
    "# # Pehli 2 rows lo, saare columns\n",
    "# matrix[:2, :]  # [[1,2,3,4], [5,6,7,8]]\n",
    "# \n",
    "# # Saari rows lo, pehle 3 columns\n",
    "# matrix[:, :3]  # [[1,2,3], [5,6,7], [9,10,11]]\n",
    "# \n",
    "# # Rows 1-2 lo, columns 1-3\n",
    "# matrix[1:3, 1:3]  # [[6,7], [10,11]]\n",
    "# \n",
    "# # Har doosri row, har doosra column\n",
    "# matrix[::2, ::2]  # [[1,3], [9,11]]\n",
    "# \n",
    "# # Rows reverse karo, columns same rakho\n",
    "# matrix[::-1, :]  # [[9,10,11,12], [5,6,7,8], [1,2,3,4]]\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# VIEW vs COPY - CRITICAL CONCEPT\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Slicing creates VIEWS (not copies) - modifying slice affects original!\n",
    "# \n",
    "# original = np.array([1, 2, 3, 4, 5])\n",
    "# slice_view = original[1:4]  # [2, 3, 4] - this is a VIEW\n",
    "# slice_view[0] = 999  # Modify the slice\n",
    "# print(original)  # [1, 999, 3, 4, 5] - ORIGINAL CHANGED!\n",
    "# \n",
    "# # To create independent copy, use .copy()\n",
    "# original = np.array([1, 2, 3, 4, 5])\n",
    "# slice_copy = original[1:4].copy()  # [2, 3, 4] - this is a COPY\n",
    "# slice_copy[0] = 999  # Modify the copy\n",
    "# print(original)  # [1, 2, 3, 4, 5] - original UNCHANGED\n",
    "# \n",
    "# When to use copy():\n",
    "# - When you need to modify slice without affecting original\n",
    "# - When storing slices for later use\n",
    "# - When passing slices to functions that might modify them\n",
    "# \n",
    "# HINGLISH:\n",
    "# Slicing VIEWS banata hai (copies nahi) - slice modify karne se original affect hota hai!\n",
    "# \n",
    "# original = np.array([1, 2, 3, 4, 5])\n",
    "# slice_view = original[1:4]  # [2, 3, 4] - yeh ek VIEW hai\n",
    "# slice_view[0] = 999  # Slice ko modify karo\n",
    "# print(original)  # [1, 999, 3, 4, 5] - ORIGINAL BADAL GAYA!\n",
    "# \n",
    "# # Independent copy banane ke liye .copy() use karo\n",
    "# original = np.array([1, 2, 3, 4, 5])\n",
    "# slice_copy = original[1:4].copy()  # [2, 3, 4] - yeh ek COPY hai\n",
    "# slice_copy[0] = 999  # Copy ko modify karo\n",
    "# print(original)  # [1, 2, 3, 4, 5] - original UNCHANGED raha\n",
    "# \n",
    "# copy() kab use karein:\n",
    "# - Jab aapko slice modify karna ho bina original ko affect kiye\n",
    "# - Jab slices ko baad mein use karne ke liye store karna ho\n",
    "# - Jab functions ko slices pass karna ho jo unhe modify kar sakte hain\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# PRACTICAL ML EXAMPLE: TRAIN-VAL-TEST SPLIT\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Real-world scenario: Splitting dataset for ML (60% train, 20% val, 20% test)\n",
    "#\n",
    "# dataset = np.arange(1000)  # Simulated dataset with 1000 samples\n",
    "# \n",
    "# # Calculate split indices\n",
    "# train_end = int(0.6 * len(dataset))  # 600\n",
    "# val_end = int(0.8 * len(dataset))    # 800\n",
    "# \n",
    "# # Create splits using slicing\n",
    "# train_data = dataset[:train_end]      # [0:600] - first 600 samples\n",
    "# val_data = dataset[train_end:val_end] # [600:800] - next 200 samples\n",
    "# test_data = dataset[val_end:]         # [800:] - last 200 samples\n",
    "# \n",
    "# print(f\"Train: {len(train_data)} samples\")  # 600\n",
    "# print(f\"Val: {len(val_data)} samples\")      # 200\n",
    "# print(f\"Test: {len(test_data)} samples\")    # 200\n",
    "#\n",
    "# HINGLISH:\n",
    "# Real-world scenario: ML ke liye dataset split karna (60% train, 20% val, 20% test)\n",
    "#\n",
    "# dataset = np.arange(1000)  # 1000 samples ke saath simulated dataset\n",
    "# \n",
    "# # Split indices calculate karo\n",
    "# train_end = int(0.6 * len(dataset))  # 600\n",
    "# val_end = int(0.8 * len(dataset))    # 800\n",
    "# \n",
    "# # Slicing use karke splits banao\n",
    "# train_data = dataset[:train_end]      # [0:600] - pehle 600 samples\n",
    "# val_data = dataset[train_end:val_end] # [600:800] - agle 200 samples\n",
    "# test_data = dataset[val_end:]         # [800:] - aakhri 200 samples\n",
    "# \n",
    "# print(f\"Train: {len(train_data)} samples\")  # 600\n",
    "# print(f\"Val: {len(val_data)} samples\")      # 200\n",
    "# print(f\"Test: {len(test_data)} samples\")    # 200\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70cc2cd6-f313-430f-977e-132fd0b5e51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1 22 33 44]\n",
      " [22 33 44 55]\n",
      " [66 77 88 99]]\n",
      "584\n",
      "[ 89 132 165 198]\n",
      "[100 154 330]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MULTI-DIMENSIONAL ARRAYS AND AXIS-BASED OPERATIONS\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# CREATING A 2D ARRAY (MATRIX) FOR AGGREGATION OPERATIONS\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Creates a 2D array (matrix) with 3 rows and 4 columns to\n",
    "#   demonstrate axis-based operations and aggregations\n",
    "# - Why use it: Understanding axis operations is CRUCIAL in ML for computing\n",
    "#   statistics across different dimensions (e.g., mean per feature, sum per\n",
    "#   sample, batch normalization, etc.)\n",
    "# - Structure: 3×4 matrix = 3 rows (samples/observations), 4 columns (features)\n",
    "# - Common use cases:\n",
    "#   * Representing datasets: rows=samples, columns=features\n",
    "#   * Batch operations: computing statistics across batch dimension\n",
    "#   * Feature engineering: aggregating values across different axes\n",
    "#   * Image processing: operations across height, width, or channels\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Aggregation operations demonstrate karne ke liye 3 rows\n",
    "#   aur 4 columns ke saath ek 2D array (matrix) banata hai\n",
    "# - Kab use karein: ML mein axis operations samajhna BAHUT ZAROORI hai kyunki\n",
    "#   different dimensions across statistics compute karna padta hai (jaise har\n",
    "#   feature ka mean, har sample ka sum, batch normalization, etc.)\n",
    "# - Structure: 3×4 matrix = 3 rows (samples/observations), 4 columns (features)\n",
    "# - Common use cases:\n",
    "#   * Datasets represent karna: rows=samples, columns=features\n",
    "#   * Batch operations: batch dimension across statistics compute karna\n",
    "#   * Feature engineering: alag axes across values aggregate karna\n",
    "#   * Image processing: height, width, ya channels across operations\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "arr2D = np.array([[1, 22, 33, 44],     # Row 0 (Sample 0)\n",
    "                  [22, 33, 44, 55],    # Row 1 (Sample 1)\n",
    "                  [66, 77, 88, 99]])   # Row 2 (Sample 2)\n",
    "#                  ↑   ↑   ↑   ↑\n",
    "#               Col0 Col1 Col2 Col3\n",
    "#             (Feature 0-3)\n",
    "\n",
    "print(arr2D)\n",
    "# Output:\n",
    "# [[ 1 22 33 44]\n",
    "#  [22 33 44 55]\n",
    "#  [66 77 88 99]]\n",
    "# \n",
    "# Matrix visualization:\n",
    "#        Column 0  Column 1  Column 2  Column 3\n",
    "# Row 0:    1        22        33        44\n",
    "# Row 1:   22        33        44        55\n",
    "# Row 2:   66        77        88        99\n",
    "\n",
    "# =============================================================================\n",
    "# OPERATION 1: np.sum() - TOTAL SUM (NO AXIS SPECIFIED)\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Computes the sum of ALL elements in the array, collapsing\n",
    "#   all dimensions into a single scalar value\n",
    "# - Syntax: np.sum(array) or array.sum()\n",
    "# - Why use it: Useful for getting total of all values, computing loss\n",
    "#   functions, or checking data magnitude\n",
    "# - When axis is NOT specified:\n",
    "#   * Treats entire array as flat/1D\n",
    "#   * Returns single number (scalar)\n",
    "#   * Sums every single element regardless of position\n",
    "# - Formula: Returns a + b + c + ... for all elements\n",
    "# - Common use cases:\n",
    "#   * Computing total loss across all samples and features\n",
    "#   * Getting overall dataset statistics\n",
    "#   * Calculating total pixel intensity in images\n",
    "#   * Verifying data preprocessing (e.g., ensuring probabilities sum to N)\n",
    "#   * Computing total weights in neural networks\n",
    "# - Example:\n",
    "#   matrix = np.array([[1, 2, 3],\n",
    "#                      [4, 5, 6]])\n",
    "#   total = np.sum(matrix)  # 1+2+3+4+5+6 = 21\n",
    "#   \n",
    "#   # ML example: Total loss\n",
    "#   batch_losses = np.array([[0.5, 0.3],\n",
    "#                            [0.4, 0.2],\n",
    "#                            [0.6, 0.1]])\n",
    "#   total_loss = np.sum(batch_losses)  # Sum of all individual losses\n",
    "#   \n",
    "#   # Image example: Total pixel intensity\n",
    "#   image = np.random.randint(0, 255, (100, 100))  # 100x100 grayscale\n",
    "#   total_intensity = np.sum(image)  # Sum of all pixel values\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Array ke SAARE elements ka sum compute karta hai, saare\n",
    "#   dimensions ko ek single scalar value mein collapse karke\n",
    "# - Syntax: np.sum(array) ya array.sum()\n",
    "# - Kab use karein: Saari values ka total lene, loss functions compute karne,\n",
    "#   ya data magnitude check karne ke liye useful hai\n",
    "# - Jab axis specify NAHI kiya:\n",
    "#   * Poore array ko flat/1D ki tarah treat karta hai\n",
    "#   * Single number (scalar) return karta hai\n",
    "#   * Position se koi fark nahi, har element ko sum karta hai\n",
    "# - Formula: Saare elements ke liye a + b + c + ... return karta hai\n",
    "# - Common use cases:\n",
    "#   * Saare samples aur features across total loss compute karna\n",
    "#   * Overall dataset statistics lena\n",
    "#   * Images mein total pixel intensity calculate karna\n",
    "#   * Data preprocessing verify karna (jaise probabilities ka sum N hai)\n",
    "#   * Neural networks mein total weights compute karna\n",
    "# - Example:\n",
    "#   matrix = np.array([[1, 2, 3],\n",
    "#                      [4, 5, 6]])\n",
    "#   total = np.sum(matrix)  # 1+2+3+4+5+6 = 21\n",
    "#   \n",
    "#   # ML example: Total loss\n",
    "#   batch_losses = np.array([[0.5, 0.3],\n",
    "#                            [0.4, 0.2],\n",
    "#                            [0.6, 0.1]])\n",
    "#   total_loss = np.sum(batch_losses)  # Saare individual losses ka sum\n",
    "#   \n",
    "#   # Image example: Total pixel intensity\n",
    "#   image = np.random.randint(0, 255, (100, 100))  # 100x100 grayscale\n",
    "#   total_intensity = np.sum(image)  # Saare pixel values ka sum\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(np.sum(arr2D))\n",
    "# Output: 589\n",
    "# Calculation: 1+22+33+44+22+33+44+55+66+77+88+99 = 589\n",
    "# Sums every single element in the matrix, regardless of row or column\n",
    "\n",
    "# =============================================================================\n",
    "# UNDERSTANDING AXES IN NUMPY - CRITICAL CONCEPT\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# AXIS CONCEPT - THE MOST IMPORTANT CONCEPT IN NUMPY:\n",
    "# \n",
    "# In a 2D array:\n",
    "# - axis=0 means \"along rows\" (vertically, top to bottom)\n",
    "# - axis=1 means \"along columns\" (horizontally, left to right)\n",
    "# \n",
    "# THINK OF IT AS: \"Which dimension gets COLLAPSED?\"\n",
    "# - axis=0: Rows collapse → Result has same number of columns\n",
    "# - axis=1: Columns collapse → Result has same number of rows\n",
    "# \n",
    "# Visual representation for arr2D with shape (3, 4):\n",
    "# \n",
    "#        axis=1 →\n",
    "#        (along columns, horizontal)\n",
    "#        Column 0  Column 1  Column 2  Column 3\n",
    "#    ↓     [1        22        33        44]      Row 0\n",
    "# axis=0   [22       33        44        55]      Row 1\n",
    "# (along   [66       77        88        99]      Row 2\n",
    "#  rows,\n",
    "# vertical)\n",
    "# \n",
    "# When you do np.sum(arr2D, axis=0):\n",
    "# - You're summing DOWN the rows (collapsing row dimension)\n",
    "# - For each column, add all row values\n",
    "# - Result shape: (4,) - one sum per column\n",
    "# - Column 0: 1+22+66=89, Column 1: 22+33+77=132, etc.\n",
    "# \n",
    "# When you do np.sum(arr2D, axis=1):\n",
    "# - You're summing ACROSS the columns (collapsing column dimension)\n",
    "# - For each row, add all column values\n",
    "# - Result shape: (3,) - one sum per row\n",
    "# - Row 0: 1+22+33+44=100, Row 1: 22+33+44+55=154, etc.\n",
    "# \n",
    "# MEMORY TRICK:\n",
    "# axis=0 → \"0 is vertical like standing | \" → operates on ROWS (vertically)\n",
    "# axis=1 → \"1 is horizontal like lying —\" → operates on COLUMNS (horizontally)\n",
    "# \n",
    "# For 3D arrays (like RGB images):\n",
    "# - axis=0: Along depth/batch (first dimension)\n",
    "# - axis=1: Along height/rows (second dimension)  \n",
    "# - axis=2: Along width/columns (third dimension)\n",
    "#\n",
    "# HINGLISH:\n",
    "# AXIS CONCEPT - NUMPY MEIN SABSE IMPORTANT CONCEPT:\n",
    "# \n",
    "# 2D array mein:\n",
    "# - axis=0 matlab \"rows ke along\" (vertically, upar se neeche)\n",
    "# - axis=1 matlab \"columns ke along\" (horizontally, left se right)\n",
    "# \n",
    "# AISE SOCHO: \"Kaun sa dimension COLLAPSE hota hai?\"\n",
    "# - axis=0: Rows collapse → Result mein same number of columns\n",
    "# - axis=1: Columns collapse → Result mein same number of rows\n",
    "# \n",
    "# arr2D ke liye visual representation with shape (3, 4):\n",
    "# \n",
    "#        axis=1 →\n",
    "#        (columns ke along, horizontal)\n",
    "#        Column 0  Column 1  Column 2  Column 3\n",
    "#    ↓     [1        22        33        44]      Row 0\n",
    "# axis=0   [22       33        44        55]      Row 1\n",
    "# (rows    [66       77        88        99]      Row 2\n",
    "#  ke along,\n",
    "# vertical)\n",
    "# \n",
    "# Jab aap np.sum(arr2D, axis=0) karte hain:\n",
    "# - Aap rows ko NEECHE sum kar rahe hain (row dimension collapse)\n",
    "# - Har column ke liye, saari row values add karo\n",
    "# - Result shape: (4,) - har column ka ek sum\n",
    "# - Column 0: 1+22+66=89, Column 1: 22+33+77=132, etc.\n",
    "# \n",
    "# Jab aap np.sum(arr2D, axis=1) karte hain:\n",
    "# - Aap columns ko ACROSS sum kar rahe hain (column dimension collapse)\n",
    "# - Har row ke liye, saari column values add karo\n",
    "# - Result shape: (3,) - har row ka ek sum\n",
    "# - Row 0: 1+22+33+44=100, Row 1: 22+33+44+55=154, etc.\n",
    "# \n",
    "# YAAD RAKHNE KA TARIKA:\n",
    "# axis=0 → \"0 khada hai jaise | \" → ROWS par operate (vertically)\n",
    "# axis=1 → \"1 leta hua hai jaise —\" → COLUMNS par operate (horizontally)\n",
    "# \n",
    "# 3D arrays ke liye (jaise RGB images):\n",
    "# - axis=0: Depth/batch ke along (pehla dimension)\n",
    "# - axis=1: Height/rows ke along (doosra dimension)  \n",
    "# - axis=2: Width/columns ke along (teesra dimension)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# =============================================================================\n",
    "# OPERATION 2: np.sum() WITH axis=0 - SUM ALONG ROWS (COLUMN-WISE SUM)\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Sums values DOWN each column, collapsing the row dimension.\n",
    "#   Results in one sum per column.\n",
    "# - Syntax: np.sum(array, axis=0) or array.sum(axis=0)\n",
    "# - Why use it: Essential for computing per-feature statistics in ML datasets\n",
    "#   where rows are samples and columns are features\n",
    "# - axis=0 behavior:\n",
    "#   * Operates along axis 0 (rows) - goes DOWN vertically\n",
    "#   * Collapses row dimension: (3, 4) → (4,)\n",
    "#   * Returns array with one value per column\n",
    "#   * Each result is sum of all rows for that column\n",
    "# - Common use cases:\n",
    "#   * Computing feature sums across all samples\n",
    "#   * Batch statistics: sum across batch dimension\n",
    "#   * Image processing: sum across height (all rows)\n",
    "#   * Column-wise aggregation in datasets\n",
    "#   * Computing totals per feature for normalization\n",
    "# - Example:\n",
    "#   data = np.array([[10, 20, 30],\n",
    "#                    [40, 50, 60],\n",
    "#                    [70, 80, 90]])\n",
    "#   column_sums = np.sum(data, axis=0)\n",
    "#   # [120, 150, 180] = [10+40+70, 20+50+80, 30+60+90]\n",
    "#   \n",
    "#   # ML example: Feature sums across samples\n",
    "#   X = np.array([[1.5, 2.3, 3.1],   # Sample 1\n",
    "#                 [2.1, 1.9, 2.8],   # Sample 2\n",
    "#                 [1.8, 2.5, 3.2]])  # Sample 3\n",
    "#   feature_sums = np.sum(X, axis=0)\n",
    "#   # [5.4, 6.7, 9.1] - sum for each of 3 features\n",
    "#   feature_means = feature_sums / X.shape[0]  # Mean per feature\n",
    "#   \n",
    "#   # Batch processing: Sum across batch\n",
    "#   batch_predictions = np.array([[0.1, 0.9],\n",
    "#                                 [0.3, 0.7],\n",
    "#                                 [0.2, 0.8]])  # 3 samples, 2 classes\n",
    "#   class_totals = np.sum(batch_predictions, axis=0)\n",
    "#   # [0.6, 2.4] - total probability for each class across batch\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Har column ko NEECHE sum karta hai, row dimension\n",
    "#   collapse karke. Har column ke liye ek sum milta hai.\n",
    "# - Syntax: np.sum(array, axis=0) ya array.sum(axis=0)\n",
    "# - Kab use karein: ML datasets mein per-feature statistics compute karne ke\n",
    "#   liye essential hai jahan rows samples hain aur columns features hain\n",
    "# - axis=0 behavior:\n",
    "#   * Axis 0 (rows) ke along operate karta hai - vertically NEECHE jaata hai\n",
    "#   * Row dimension collapse: (3, 4) → (4,)\n",
    "#   * Har column ke liye ek value ka array return karta hai\n",
    "#   * Har result us column ke saare rows ka sum hai\n",
    "# - Common use cases:\n",
    "#   * Saare samples across feature sums compute karna\n",
    "#   * Batch statistics: batch dimension across sum\n",
    "#   * Image processing: height across sum (saari rows)\n",
    "#   * Datasets mein column-wise aggregation\n",
    "#   * Normalization ke liye har feature ka total compute karna\n",
    "# - Example:\n",
    "#   data = np.array([[10, 20, 30],\n",
    "#                    [40, 50, 60],\n",
    "#                    [70, 80, 90]])\n",
    "#   column_sums = np.sum(data, axis=0)\n",
    "#   # [120, 150, 180] = [10+40+70, 20+50+80, 30+60+90]\n",
    "#   \n",
    "#   # ML example: Samples across feature sums\n",
    "#   X = np.array([[1.5, 2.3, 3.1],   # Sample 1\n",
    "#                 [2.1, 1.9, 2.8],   # Sample 2\n",
    "#                 [1.8, 2.5, 3.2]])  # Sample 3\n",
    "#   feature_sums = np.sum(X, axis=0)\n",
    "#   # [5.4, 6.7, 9.1] - 3 features mein se har ek ka sum\n",
    "#   feature_means = feature_sums / X.shape[0]  # Har feature ka mean\n",
    "#   \n",
    "#   # Batch processing: Batch across sum\n",
    "#   batch_predictions = np.array([[0.1, 0.9],\n",
    "#                                 [0.3, 0.7],\n",
    "#                                 [0.2, 0.8]])  # 3 samples, 2 classes\n",
    "#   class_totals = np.sum(batch_predictions, axis=0)\n",
    "#   # [0.6, 2.4] - batch across har class ka total probability\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "sum_of_columns = np.sum(arr2D, axis=0)\n",
    "print(sum_of_columns)\n",
    "# Output: [ 89 132 165 198]\n",
    "# \n",
    "# Calculation (summing DOWN each column):\n",
    "# Column 0: 1 + 22 + 66 = 89\n",
    "# Column 1: 22 + 33 + 77 = 132\n",
    "# Column 2: 33 + 44 + 88 = 165\n",
    "# Column 3: 44 + 55 + 99 = 198\n",
    "# \n",
    "# Visual:\n",
    "#    Col0  Col1  Col2  Col3\n",
    "#     ↓     ↓     ↓     ↓\n",
    "#   [ 1    22    33    44]\n",
    "#   [22    33    44    55]\n",
    "#   [66    77    88    99]\n",
    "#    ↓     ↓     ↓     ↓\n",
    "#   [89   132   165   198]\n",
    "# \n",
    "# Result shape: (4,) - one sum for each of 4 columns\n",
    "# Original shape: (3, 4) → axis=0 collapsed → (4,)\n",
    "\n",
    "# =============================================================================\n",
    "# OPERATION 3: np.sum() WITH axis=1 - SUM ALONG COLUMNS (ROW-WISE SUM)\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Sums values ACROSS each row, collapsing the column dimension.\n",
    "#   Results in one sum per row.\n",
    "# - Syntax: np.sum(array, axis=1) or array.sum(axis=1)\n",
    "# - Why use it: Essential for computing per-sample statistics in ML datasets\n",
    "#   where rows are samples and columns are features\n",
    "# - axis=1 behavior:\n",
    "#   * Operates along axis 1 (columns) - goes ACROSS horizontally\n",
    "#   * Collapses column dimension: (3, 4) → (3,)\n",
    "#   * Returns array with one value per row\n",
    "#   * Each result is sum of all columns for that row\n",
    "# - Common use cases:\n",
    "#   * Computing total/sum per sample across all features\n",
    "#   * Row-wise aggregation in datasets\n",
    "#   * Calculating total score per student (rows=students, cols=subjects)\n",
    "#   * Sum of predictions per sample across classes\n",
    "#   * Image processing: sum across width (all columns)\n",
    "# - Example:\n",
    "#   data = np.array([[10, 20, 30],\n",
    "#                    [40, 50, 60],\n",
    "#                    [70, 80, 90]])\n",
    "#   row_sums = np.sum(data, axis=1)\n",
    "#   # [60, 150, 240] = [10+20+30, 40+50+60, 70+80+90]\n",
    "#   \n",
    "#   # ML example: Total feature value per sample\n",
    "#   X = np.array([[1.5, 2.3, 3.1],   # Sample 1\n",
    "#                 [2.1, 1.9, 2.8],   # Sample 2\n",
    "#                 [1.8, 2.5, 3.2]])  # Sample 3\n",
    "#   sample_totals = np.sum(X, axis=1)\n",
    "#   # [6.9, 6.8, 7.5] - total for each sample across all features\n",
    "#   \n",
    "#   # Probability normalization\n",
    "#   logits = np.array([[2.0, 1.0, 0.5],\n",
    "#                      [1.5, 2.5, 1.0]])  # 2 samples, 3 classes\n",
    "#   exp_logits = np.exp(logits)\n",
    "#   row_sums = np.sum(exp_logits, axis=1, keepdims=True)\n",
    "#   probabilities = exp_logits / row_sums  # Softmax normalization\n",
    "#   \n",
    "#   # Student scores: Total per student\n",
    "#   scores = np.array([[85, 90, 78],   # Student 1: Math, Science, English\n",
    "#                      [92, 88, 95],   # Student 2\n",
    "#                      [78, 85, 82]])  # Student 3\n",
    "#   total_scores = np.sum(scores, axis=1)\n",
    "#   # [253, 275, 245] - each student's total score\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Har row ko ACROSS sum karta hai, column dimension\n",
    "#   collapse karke. Har row ke liye ek sum milta hai.\n",
    "# - Syntax: np.sum(array, axis=1) ya array.sum(axis=1)\n",
    "# - Kab use karein: ML datasets mein per-sample statistics compute karne ke\n",
    "#   liye essential hai jahan rows samples hain aur columns features hain\n",
    "# - axis=1 behavior:\n",
    "#   * Axis 1 (columns) ke along operate karta hai - horizontally ACROSS jaata hai\n",
    "#   * Column dimension collapse: (3, 4) → (3,)\n",
    "#   * Har row ke liye ek value ka array return karta hai\n",
    "#   * Har result us row ke saare columns ka sum hai\n",
    "# - Common use cases:\n",
    "#   * Saare features across har sample ka total/sum compute karna\n",
    "#   * Datasets mein row-wise aggregation\n",
    "#   * Har student ka total score calculate karna (rows=students, cols=subjects)\n",
    "#   * Classes across har sample ke predictions ka sum\n",
    "#   * Image processing: width across sum (saare columns)\n",
    "# - Example:\n",
    "#   data = np.array([[10, 20, 30],\n",
    "#                    [40, 50, 60],\n",
    "#                    [70, 80, 90]])\n",
    "#   row_sums = np.sum(data, axis=1)\n",
    "#   # [60, 150, 240] = [10+20+30, 40+50+60, 70+80+90]\n",
    "#   \n",
    "#   # ML example: Har sample ka total feature value\n",
    "#   X = np.array([[1.5, 2.3, 3.1],   # Sample 1\n",
    "#                 [2.1, 1.9, 2.8],   # Sample 2\n",
    "#                 [1.8, 2.5, 3.2]])  # Sample 3\n",
    "#   sample_totals = np.sum(X, axis=1)\n",
    "#   # [6.9, 6.8, 7.5] - saare features across har sample ka total\n",
    "#   \n",
    "#   # Probability normalization\n",
    "#   logits = np.array([[2.0, 1.0, 0.5],\n",
    "#                      [1.5, 2.5, 1.0]])  # 2 samples, 3 classes\n",
    "#   exp_logits = np.exp(logits)\n",
    "#   row_sums = np.sum(exp_logits, axis=1, keepdims=True)\n",
    "#   probabilities = exp_logits / row_sums  # Softmax normalization\n",
    "#   \n",
    "#   # Student scores: Har student ka total\n",
    "#   scores = np.array([[85, 90, 78],   # Student 1: Math, Science, English\n",
    "#                      [92, 88, 95],   # Student 2\n",
    "#                      [78, 85, 82]])  # Student 3\n",
    "#   total_scores = np.sum(scores, axis=1)\n",
    "#   # [253, 275, 245] - har student ka total score\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "sum_of_rows = np.sum(arr2D, axis=1)\n",
    "print(sum_of_rows)\n",
    "# Output: [100 154 330]\n",
    "# \n",
    "# Calculation (summing ACROSS each row):\n",
    "# Row 0: 1 + 22 + 33 + 44 = 100\n",
    "# Row 1: 22 + 33 + 44 + 55 = 154\n",
    "# Row 2: 66 + 77 + 88 + 99 = 330\n",
    "# \n",
    "# Visual:\n",
    "#   Row 0: [ 1  22  33  44] → 100\n",
    "#           ←───────────→\n",
    "#   Row 1: [22  33  44  55] → 154\n",
    "#           ←───────────→\n",
    "#   Row 2: [66  77  88  99] → 330\n",
    "#           ←───────────→\n",
    "# \n",
    "# Result shape: (3,) - one sum for each of 3 rows\n",
    "# Original shape: (3, 4) → axis=1 collapsed → (3,)\n",
    "\n",
    "# =============================================================================\n",
    "# OTHER COMMON AGGREGATION FUNCTIONS WITH AXIS\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# All these functions work the same way with axis parameter:\n",
    "# \n",
    "# 1. np.mean(array, axis=None/0/1) - Average\n",
    "#    mean_cols = np.mean(arr2D, axis=0)  # Mean of each column\n",
    "#    mean_rows = np.mean(arr2D, axis=1)  # Mean of each row\n",
    "# \n",
    "# 2. np.max(array, axis=None/0/1) - Maximum value\n",
    "#    max_cols = np.max(arr2D, axis=0)  # Max in each column\n",
    "#    max_rows = np.max(arr2D, axis=1)  # Max in each row\n",
    "# \n",
    "# 3. np.min(array, axis=None/0/1) - Minimum value\n",
    "#    min_cols = np.min(arr2D, axis=0)  # Min in each column\n",
    "#    min_rows = np.min(arr2D, axis=1)  # Min in each row\n",
    "# \n",
    "# 4. np.std(array, axis=None/0/1) - Standard deviation\n",
    "#    std_cols = np.std(arr2D, axis=0)  # Std dev of each column (feature)\n",
    "#    std_rows = np.std(arr2D, axis=1)  # Std dev of each row (sample)\n",
    "# \n",
    "# 5. np.var(array, axis=None/0/1) - Variance\n",
    "#    var_cols = np.var(arr2D, axis=0)  # Variance of each column\n",
    "# \n",
    "# 6. np.median(array, axis=None/0/1) - Median value\n",
    "#    median_cols = np.median(arr2D, axis=0)  # Median of each column\n",
    "# \n",
    "# 7. np.argmax(array, axis=None/0/1) - Index of maximum\n",
    "#    argmax_cols = np.argmax(arr2D, axis=0)  # Row index of max in each col\n",
    "#    argmax_rows = np.argmax(arr2D, axis=1)  # Col index of max in each row\n",
    "# \n",
    "# 8. np.argmin(array, axis=None/0/1) - Index of minimum\n",
    "#    argmin_rows = np.argmin(arr2D, axis=1)  # Col index of min in each row\n",
    "# \n",
    "# 9. np.prod(array, axis=None/0/1) - Product of elements\n",
    "#    prod_rows = np.prod(arr2D, axis=1)  # Product across each row\n",
    "# \n",
    "# 10. np.cumsum(array, axis=None/0/1) - Cumulative sum\n",
    "#     cumsum_cols = np.cumsum(arr2D, axis=0)  # Running sum down columns\n",
    "#\n",
    "# HINGLISH:\n",
    "# Yeh saare functions axis parameter ke saath same tarah kaam karte hain:\n",
    "# \n",
    "# 1. np.mean(array, axis=None/0/1) - Average\n",
    "#    mean_cols = np.mean(arr2D, axis=0)  # Har column ka mean\n",
    "#    mean_rows = np.mean(arr2D, axis=1)  # Har row ka mean\n",
    "# \n",
    "# 2. np.max(array, axis=None/0/1) - Maximum value\n",
    "#    max_cols = np.max(arr2D, axis=0)  # Har column mein max\n",
    "#    max_rows = np.max(arr2D, axis=1)  # Har row mein max\n",
    "# \n",
    "# 3. np.min(array, axis=None/0/1) - Minimum value\n",
    "#    min_cols = np.min(arr2D, axis=0)  # Har column mein min\n",
    "#    min_rows = np.min(arr2D, axis=1)  # Har row mein min\n",
    "# \n",
    "# 4. np.std(array, axis=None/0/1) - Standard deviation\n",
    "#    std_cols = np.std(arr2D, axis=0)  # Har column (feature) ka std dev\n",
    "#    std_rows = np.std(arr2D, axis=1)  # Har row (sample) ka std dev\n",
    "# \n",
    "# 5. np.var(array, axis=None/0/1) - Variance\n",
    "#    var_cols = np.var(arr2D, axis=0)  # Har column ka variance\n",
    "# \n",
    "# 6. np.median(array, axis=None/0/1) - Median value\n",
    "#    median_cols = np.median(arr2D, axis=0)  # Har column ka median\n",
    "# \n",
    "# 7. np.argmax(array, axis=None/0/1) - Maximum ka index\n",
    "#    argmax_cols = np.argmax(arr2D, axis=0)  # Har col mein max ka row index\n",
    "#    argmax_rows = np.argmax(arr2D, axis=1)  # Har row mein max ka col index\n",
    "# \n",
    "# 8. np.argmin(array, axis=None/0/1) - Minimum ka index\n",
    "#    argmin_rows = np.argmin(arr2D, axis=1)  # Har row mein min ka col index\n",
    "# \n",
    "# 9. np.prod(array, axis=None/0/1) - Elements ka product\n",
    "#    prod_rows = np.prod(arr2D, axis=1)  # Har row across product\n",
    "# \n",
    "# 10. np.cumsum(array, axis=None/0/1) - Cumulative sum\n",
    "#     cumsum_cols = np.cumsum(arr2D, axis=0)  # Columns neeche running sum\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# PRACTICAL ML EXAMPLE: FEATURE NORMALIZATION\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Real-world scenario: Normalizing features (Z-score normalization)\n",
    "#\n",
    "# # Dataset: 4 samples, 3 features\n",
    "# X = np.array([[1.0, 2.0, 3.0],\n",
    "#               [4.0, 5.0, 6.0],\n",
    "#               [7.0, 8.0, 9.0],\n",
    "#               [10.0, 11.0, 12.0]])\n",
    "# \n",
    "# # Compute mean and std for each feature (axis=0)\n",
    "# feature_means = np.mean(X, axis=0)  # [5.5, 6.5, 7.5]\n",
    "# feature_stds = np.std(X, axis=0)    # [3.416, 3.416, 3.416]\n",
    "# \n",
    "# # Normalize: (X - mean) / std for each feature\n",
    "# X_normalized = (X - feature_means) / feature_stds\n",
    "# \n",
    "# # Verify normalization worked\n",
    "# print(np.mean(X_normalized, axis=0))  # ~[0, 0, 0] - mean centered\n",
    "# print(np.std(X_normalized, axis=0))   # ~[1, 1, 1] - unit variance\n",
    "#\n",
    "# HINGLISH:\n",
    "# Real-world scenario: Features normalize karna (Z-score normalization)\n",
    "#\n",
    "# # Dataset: 4 samples, 3 features\n",
    "# X = np.array([[1.0, 2.0, 3.0],\n",
    "#               [4.0, 5.0, 6.0],\n",
    "#               [7.0, 8.0, 9.0],\n",
    "#               [10.0, 11.0, 12.0]])\n",
    "# \n",
    "# # Har feature ke liye mean aur std compute karo (axis=0)\n",
    "# feature_means = np.mean(X, axis=0)  # [5.5, 6.5, 7.5]\n",
    "# feature_stds = np.std(X, axis=0)    # [3.416, 3.416, 3.416]\n",
    "# \n",
    "# # Normalize karo: har feature ke liye (X - mean) / std\n",
    "# X_normalized = (X - feature_means) / feature_stds\n",
    "# \n",
    "# # Verify karo ki normalization kaam kiya\n",
    "# print(np.mean(X_normalized, axis=0))  # ~[0, 0, 0] - mean centered\n",
    "# print(np.std(X_normalized, axis=0))   # ~[1, 1, 1] - unit variance\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "030d13d6-24a7-40d3-9576-c9d07f947eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1  2  3]\n",
      "  [ 4  5  6]\n",
      "  [ 7  8  9]]\n",
      "\n",
      " [[11 22 33]\n",
      "  [44 55 66]\n",
      "  [77 88 99]]] (2, 3, 3)\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3D ARRAYS (TENSORS) - MULTI-DIMENSIONAL DATA STRUCTURES\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# CREATING A 3D ARRAY (TENSOR)\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Creates a 3-dimensional array (tensor) with shape (2, 3, 3).\n",
    "#   This is essentially a stack of 2D matrices.\n",
    "# - Why use it: 3D arrays are FUNDAMENTAL in deep learning for representing:\n",
    "#   * RGB images: (height, width, channels) - e.g., (224, 224, 3)\n",
    "#   * Video frames: (frames, height, width) or (frames, height, width, channels)\n",
    "#   * Batches of data: (batch_size, samples, features)\n",
    "#   * Convolutional layer outputs: (batch, height, width, filters)\n",
    "#   * Time series batches: (batch, timesteps, features)\n",
    "# - Structure breakdown:\n",
    "#   * First dimension (axis 0): \"depth\" or \"layers\" - 2 matrices stacked\n",
    "#   * Second dimension (axis 1): rows within each matrix - 3 rows\n",
    "#   * Third dimension (axis 2): columns within each matrix - 3 columns\n",
    "# - Shape (2, 3, 3) means:\n",
    "#   * 2 \"layers\" or \"matrices\" or \"channels\"\n",
    "#   * Each matrix has 3 rows\n",
    "#   * Each matrix has 3 columns\n",
    "#   * Total elements: 2 × 3 × 3 = 18 elements\n",
    "# - Think of it as:\n",
    "#   * A box containing 2 sheets of paper (layers/matrices)\n",
    "#   * Each sheet has a 3×3 grid of numbers\n",
    "#   * You need 3 indices to locate any number: [which_layer, which_row, which_column]\n",
    "# - Common use cases:\n",
    "#   * Color images: arr[height, width, RGB_channel]\n",
    "#   * Batch of grayscale images: arr[image_num, height, width]\n",
    "#   * Video data: arr[frame, height, width]\n",
    "#   * CNN feature maps: arr[batch, feature_map_h, feature_map_w]\n",
    "#   * Sequence batches: arr[batch, sequence_length, feature_dim]\n",
    "# - Example:\n",
    "#   # RGB image (100×100 pixels, 3 color channels)\n",
    "#   image = np.zeros((100, 100, 3))  # shape: (100, 100, 3)\n",
    "#   # Accessing pixel at row 50, col 30, red channel\n",
    "#   red_value = image[50, 30, 0]\n",
    "#   \n",
    "#   # Batch of 32 grayscale images (28×28 each)\n",
    "#   batch = np.zeros((32, 28, 28))  # shape: (32, 28, 28)\n",
    "#   # Accessing 5th image in batch\n",
    "#   fifth_image = batch[4, :, :]  # shape: (28, 28)\n",
    "#   \n",
    "#   # Time series: 64 samples, each with 100 timesteps, 10 features\n",
    "#   timeseries = np.zeros((64, 100, 10))  # shape: (64, 100, 10)\n",
    "#   # Accessing first sample's data at timestep 50\n",
    "#   features_at_t50 = timeseries[0, 50, :]  # shape: (10,)\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Shape (2, 3, 3) ke saath ek 3-dimensional array (tensor)\n",
    "#   banata hai. Yeh basically 2D matrices ka stack hai.\n",
    "# - Kab use karein: 3D arrays deep learning mein represent karne ke liye\n",
    "#   FUNDAMENTAL hain:\n",
    "#   * RGB images: (height, width, channels) - jaise (224, 224, 3)\n",
    "#   * Video frames: (frames, height, width) ya (frames, height, width, channels)\n",
    "#   * Data ke batches: (batch_size, samples, features)\n",
    "#   * Convolutional layer outputs: (batch, height, width, filters)\n",
    "#   * Time series batches: (batch, timesteps, features)\n",
    "# - Structure breakdown:\n",
    "#   * Pehla dimension (axis 0): \"depth\" ya \"layers\" - 2 matrices stacked\n",
    "#   * Doosra dimension (axis 1): har matrix ke andar rows - 3 rows\n",
    "#   * Teesra dimension (axis 2): har matrix ke andar columns - 3 columns\n",
    "# - Shape (2, 3, 3) ka matlab:\n",
    "#   * 2 \"layers\" ya \"matrices\" ya \"channels\"\n",
    "#   * Har matrix mein 3 rows hain\n",
    "#   * Har matrix mein 3 columns hain\n",
    "#   * Total elements: 2 × 3 × 3 = 18 elements\n",
    "# - Aise socho:\n",
    "#   * Ek box jismein 2 sheets of paper hain (layers/matrices)\n",
    "#   * Har sheet par 3×3 grid of numbers hai\n",
    "#   * Kisi bhi number ko locate karne ke liye 3 indices chahiye: [kaun_si_layer, kaun_si_row, kaun_sa_column]\n",
    "# - Common use cases:\n",
    "#   * Color images: arr[height, width, RGB_channel]\n",
    "#   * Grayscale images ka batch: arr[image_num, height, width]\n",
    "#   * Video data: arr[frame, height, width]\n",
    "#   * CNN feature maps: arr[batch, feature_map_h, feature_map_w]\n",
    "#   * Sequence batches: arr[batch, sequence_length, feature_dim]\n",
    "# - Example:\n",
    "#   # RGB image (100×100 pixels, 3 color channels)\n",
    "#   image = np.zeros((100, 100, 3))  # shape: (100, 100, 3)\n",
    "#   # Row 50, col 30, red channel par pixel access karna\n",
    "#   red_value = image[50, 30, 0]\n",
    "#   \n",
    "#   # 32 grayscale images ka batch (har ek 28×28)\n",
    "#   batch = np.zeros((32, 28, 28))  # shape: (32, 28, 28)\n",
    "#   # Batch mein 5th image access karna\n",
    "#   fifth_image = batch[4, :, :]  # shape: (28, 28)\n",
    "#   \n",
    "#   # Time series: 64 samples, har ek mein 100 timesteps, 10 features\n",
    "#   timeseries = np.zeros((64, 100, 10))  # shape: (64, 100, 10)\n",
    "#   # Pehle sample ka data timestep 50 par access karna\n",
    "#   features_at_t50 = timeseries[0, 50, :]  # shape: (10,)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "arr3D = np.array([\n",
    "    # Matrix 0 (First layer/depth) - axis 0, index 0\n",
    "    [[1, 2, 3],      # Row 0 of Matrix 0\n",
    "     [4, 5, 6],      # Row 1 of Matrix 0\n",
    "     [7, 8, 9]],     # Row 2 of Matrix 0\n",
    "    \n",
    "    # Matrix 1 (Second layer/depth) - axis 0, index 1\n",
    "    [[11, 22, 33],   # Row 0 of Matrix 1\n",
    "     [44, 55, 66],   # Row 1 of Matrix 1\n",
    "     [77, 88, 99]]   # Row 2 of Matrix 1\n",
    "])\n",
    "\n",
    "# =============================================================================\n",
    "# VISUALIZING 3D ARRAY STRUCTURE\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Visual representation of arr3D with shape (2, 3, 3):\n",
    "#\n",
    "# Think of it as 2 layers stacked on top of each other:\n",
    "#\n",
    "# Layer 0 (arr3D[0]):          Layer 1 (arr3D[1]):\n",
    "# ┌─────────────┐              ┌─────────────┐\n",
    "# │ 1   2   3  │              │ 11  22  33 │\n",
    "# │ 4   5   6  │              │ 44  55  66 │\n",
    "# │ 7   8   9  │              │ 77  88  99 │\n",
    "# └─────────────┘              └─────────────┘\n",
    "#\n",
    "# 3D coordinates system:\n",
    "# - arr3D[layer, row, column]\n",
    "# - arr3D[axis_0, axis_1, axis_2]\n",
    "# - arr3D[depth, height, width]\n",
    "#\n",
    "# Examples of accessing elements:\n",
    "# arr3D[0, 0, 0] = 1   (Layer 0, Row 0, Col 0)\n",
    "# arr3D[0, 1, 1] = 5   (Layer 0, Row 1, Col 1)\n",
    "# arr3D[1, 0, 0] = 11  (Layer 1, Row 0, Col 0)\n",
    "# arr3D[1, 1, 1] = 55  (Layer 1, Row 1, Col 1)\n",
    "# arr3D[1, 2, 2] = 99  (Layer 1, Row 2, Col 2)\n",
    "#\n",
    "# HINGLISH:\n",
    "# Shape (2, 3, 3) ke saath arr3D ka visual representation:\n",
    "#\n",
    "# Ise 2 layers ki tarah socho jo ek doosre ke upar stacked hain:\n",
    "#\n",
    "# Layer 0 (arr3D[0]):          Layer 1 (arr3D[1]):\n",
    "# ┌─────────────┐              ┌─────────────┐\n",
    "# │ 1   2   3  │              │ 11  22  33 │\n",
    "# │ 4   5   6  │              │ 44  55  66 │\n",
    "# │ 7   8   9  │              │ 77  88  99 │\n",
    "# └─────────────┘              └─────────────┘\n",
    "#\n",
    "# 3D coordinates system:\n",
    "# - arr3D[layer, row, column]\n",
    "# - arr3D[axis_0, axis_1, axis_2]\n",
    "# - arr3D[depth, height, width]\n",
    "#\n",
    "# Elements access karne ke examples:\n",
    "# arr3D[0, 0, 0] = 1   (Layer 0, Row 0, Col 0)\n",
    "# arr3D[0, 1, 1] = 5   (Layer 0, Row 1, Col 1)\n",
    "# arr3D[1, 0, 0] = 11  (Layer 1, Row 0, Col 0)\n",
    "# arr3D[1, 1, 1] = 55  (Layer 1, Row 1, Col 1)\n",
    "# arr3D[1, 2, 2] = 99  (Layer 1, Row 2, Col 2)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(arr3D, arr3D.shape)\n",
    "# Output:\n",
    "# [[[ 1  2  3]\n",
    "#   [ 4  5  6]\n",
    "#   [ 7  8  9]]\n",
    "#\n",
    "#  [[11 22 33]\n",
    "#   [44 55 66]\n",
    "#   [77 88 99]]] (2, 3, 3)\n",
    "#\n",
    "# Shape (2, 3, 3) breakdown:\n",
    "# - First number (2): Number of 3×3 matrices (depth/layers)\n",
    "# - Second number (3): Number of rows in each matrix\n",
    "# - Third number (3): Number of columns in each matrix\n",
    "# - Total elements: 2 × 3 × 3 = 18\n",
    "\n",
    "# =============================================================================\n",
    "# 3D ARRAY INDEXING - ACCESSING INDIVIDUAL ELEMENTS\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Accesses a single element in a 3D array using three indices:\n",
    "#   [layer_index, row_index, column_index]\n",
    "# - Syntax: arr3D[i, j, k] where:\n",
    "#   * i = which layer/matrix (axis 0) - ranges from 0 to 1 in this case\n",
    "#   * j = which row within that matrix (axis 1) - ranges from 0 to 2\n",
    "#   * k = which column within that row (axis 2) - ranges from 0 to 2\n",
    "# - Why use it: Essential for accessing specific values in multi-dimensional\n",
    "#   data like individual pixel values in images, specific features in batches,\n",
    "#   or particular timesteps in sequences\n",
    "# - Indexing order matters:\n",
    "#   * Always: [slowest-changing, ..., fastest-changing]\n",
    "#   * For images: [batch, height, width, channel] or [height, width, channel]\n",
    "#   * For video: [frame, height, width] or [frame, height, width, channel]\n",
    "#   * For batches: [sample, feature_1, feature_2, ...]\n",
    "# - Common use cases:\n",
    "#   * Getting pixel value: image[y, x, channel]\n",
    "#   * Accessing specific frame: video[frame_num, y, x]\n",
    "#   * Getting element from batch: batch[sample_idx, row, col]\n",
    "#   * Extracting feature value: data[sample, timestep, feature]\n",
    "# - Example:\n",
    "#   # RGB image (256×256, 3 channels)\n",
    "#   img = np.random.rand(256, 256, 3)\n",
    "#   red_pixel = img[100, 150, 0]    # Red value at pixel (100, 150)\n",
    "#   green_pixel = img[100, 150, 1]  # Green value at same pixel\n",
    "#   blue_pixel = img[100, 150, 2]   # Blue value at same pixel\n",
    "#   \n",
    "#   # Batch of images (32 images, 28×28 each)\n",
    "#   batch = np.random.rand(32, 28, 28)\n",
    "#   pixel_value = batch[5, 10, 15]  # Pixel at (10,15) in 6th image\n",
    "#   \n",
    "#   # Time series (10 samples, 100 timesteps, 5 features)\n",
    "#   ts_data = np.random.rand(10, 100, 5)\n",
    "#   feature_val = ts_data[0, 50, 2]  # 3rd feature at timestep 50, sample 0\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Teen indices use karke 3D array mein ek single element\n",
    "#   access karta hai: [layer_index, row_index, column_index]\n",
    "# - Syntax: arr3D[i, j, k] jahan:\n",
    "#   * i = kaun si layer/matrix (axis 0) - is case mein 0 se 1 tak\n",
    "#   * j = us matrix mein kaun si row (axis 1) - 0 se 2 tak\n",
    "#   * k = us row mein kaun sa column (axis 2) - 0 se 2 tak\n",
    "# - Kab use karein: Multi-dimensional data mein specific values access karne\n",
    "#   ke liye essential hai jaise images mein individual pixel values, batches\n",
    "#   mein specific features, ya sequences mein particular timesteps\n",
    "# - Indexing order important hai:\n",
    "#   * Hamesha: [slowest-changing, ..., fastest-changing]\n",
    "#   * Images ke liye: [batch, height, width, channel] ya [height, width, channel]\n",
    "#   * Video ke liye: [frame, height, width] ya [frame, height, width, channel]\n",
    "#   * Batches ke liye: [sample, feature_1, feature_2, ...]\n",
    "# - Common use cases:\n",
    "#   * Pixel value lena: image[y, x, channel]\n",
    "#   * Specific frame access karna: video[frame_num, y, x]\n",
    "#   * Batch se element lena: batch[sample_idx, row, col]\n",
    "#   * Feature value extract karna: data[sample, timestep, feature]\n",
    "# - Example:\n",
    "#   # RGB image (256×256, 3 channels)\n",
    "#   img = np.random.rand(256, 256, 3)\n",
    "#   red_pixel = img[100, 150, 0]    # Pixel (100, 150) par red value\n",
    "#   green_pixel = img[100, 150, 1]  # Same pixel par green value\n",
    "#   blue_pixel = img[100, 150, 2]   # Same pixel par blue value\n",
    "#   \n",
    "#   # Images ka batch (32 images, har ek 28×28)\n",
    "#   batch = np.random.rand(32, 28, 28)\n",
    "#   pixel_value = batch[5, 10, 15]  # 6th image mein (10,15) par pixel\n",
    "#   \n",
    "#   # Time series (10 samples, 100 timesteps, 5 features)\n",
    "#   ts_data = np.random.rand(10, 100, 5)\n",
    "#   feature_val = ts_data[0, 50, 2]  # Sample 0 mein timestep 50 par 3rd feature\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(arr3D[1, 1, 1])\n",
    "# Output: 55\n",
    "#\n",
    "# Breakdown of indexing arr3D[1, 1, 1]:\n",
    "# - Index 0 (layer/matrix): 1 → Select the SECOND matrix (Matrix 1)\n",
    "# - Index 1 (row): 1 → Select the SECOND row within Matrix 1\n",
    "# - Index 2 (column): 1 → Select the SECOND column within that row\n",
    "#\n",
    "# Visual trace:\n",
    "# Step 1: arr3D[1] → Select Matrix 1\n",
    "#   [[11, 22, 33],\n",
    "#    [44, 55, 66],  ← We want this row\n",
    "#    [77, 88, 99]]\n",
    "#\n",
    "# Step 2: arr3D[1, 1] → Select Row 1 of Matrix 1\n",
    "#   [44, 55, 66]\n",
    "#        ↑\n",
    "#     We want this column\n",
    "#\n",
    "# Step 3: arr3D[1, 1, 1] → Select Column 1\n",
    "#   55 ← Final result\n",
    "\n",
    "# =============================================================================\n",
    "# MORE 3D INDEXING EXAMPLES\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Accessing different elements to understand indexing:\n",
    "#\n",
    "# # Get first element (top-left of first matrix)\n",
    "# print(arr3D[0, 0, 0])  # Output: 1\n",
    "#\n",
    "# # Get last element (bottom-right of second matrix)\n",
    "# print(arr3D[1, 2, 2])  # Output: 99\n",
    "#\n",
    "# # Get entire first matrix (all rows and columns of layer 0)\n",
    "# print(arr3D[0])        # Output: [[1,2,3], [4,5,6], [7,8,9]]\n",
    "# # Or equivalently:\n",
    "# print(arr3D[0, :, :])  # Same output\n",
    "#\n",
    "# # Get entire second matrix\n",
    "# print(arr3D[1])        # Output: [[11,22,33], [44,55,66], [77,88,99]]\n",
    "#\n",
    "# # Get first row from first matrix\n",
    "# print(arr3D[0, 0])     # Output: [1, 2, 3]\n",
    "# # Or equivalently:\n",
    "# print(arr3D[0, 0, :])  # Same output\n",
    "#\n",
    "# # Get middle row from second matrix\n",
    "# print(arr3D[1, 1])     # Output: [44, 55, 66]\n",
    "#\n",
    "# # Get first column from all matrices (all layers, all rows, column 0)\n",
    "# print(arr3D[:, :, 0])  # Output: [[1,4,7], [11,44,77]]\n",
    "#\n",
    "# # Get middle element from all matrices (all layers, row 1, col 1)\n",
    "# print(arr3D[:, 1, 1])  # Output: [5, 55]\n",
    "#\n",
    "# # Using negative indices\n",
    "# print(arr3D[-1, -1, -1])  # Output: 99 (last layer, last row, last column)\n",
    "#\n",
    "# HINGLISH:\n",
    "# Indexing samajhne ke liye alag elements access karna:\n",
    "#\n",
    "# # Pehla element lo (pehli matrix ka top-left)\n",
    "# print(arr3D[0, 0, 0])  # Output: 1\n",
    "#\n",
    "# # Aakhri element lo (doosri matrix ka bottom-right)\n",
    "# print(arr3D[1, 2, 2])  # Output: 99\n",
    "#\n",
    "# # Poori pehli matrix lo (layer 0 ki saari rows aur columns)\n",
    "# print(arr3D[0])        # Output: [[1,2,3], [4,5,6], [7,8,9]]\n",
    "# # Ya equivalently:\n",
    "# print(arr3D[0, :, :])  # Same output\n",
    "#\n",
    "# # Poori doosri matrix lo\n",
    "# print(arr3D[1])        # Output: [[11,22,33], [44,55,66], [77,88,99]]\n",
    "#\n",
    "# # Pehli matrix se pehli row lo\n",
    "# print(arr3D[0, 0])     # Output: [1, 2, 3]\n",
    "# # Ya equivalently:\n",
    "# print(arr3D[0, 0, :])  # Same output\n",
    "#\n",
    "# # Doosri matrix se middle row lo\n",
    "# print(arr3D[1, 1])     # Output: [44, 55, 66]\n",
    "#\n",
    "# # Saari matrices se pehla column lo (saari layers, saari rows, column 0)\n",
    "# print(arr3D[:, :, 0])  # Output: [[1,4,7], [11,44,77]]\n",
    "#\n",
    "# # Saari matrices se middle element lo (saari layers, row 1, col 1)\n",
    "# print(arr3D[:, 1, 1])  # Output: [5, 55]\n",
    "#\n",
    "# # Negative indices use karna\n",
    "# print(arr3D[-1, -1, -1])  # Output: 99 (last layer, last row, last column)\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# 3D ARRAY AXIS OPERATIONS\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Understanding axis operations in 3D arrays (CRITICAL for ML):\n",
    "#\n",
    "# For arr3D with shape (2, 3, 3):\n",
    "# - axis=0: Operates along depth/layers (collapses layers)\n",
    "# - axis=1: Operates along rows (collapses rows)\n",
    "# - axis=2: Operates along columns (collapses columns)\n",
    "#\n",
    "# Examples:\n",
    "#\n",
    "# # Sum along axis 0 (collapse layers) - shape (2,3,3) → (3,3)\n",
    "# sum_axis0 = np.sum(arr3D, axis=0)\n",
    "# # Adds corresponding elements from both matrices\n",
    "# # [[1+11, 2+22, 3+33],\n",
    "# #  [4+44, 5+55, 6+66],\n",
    "# #  [7+77, 8+88, 9+99]]\n",
    "# # = [[12, 24, 36], [48, 60, 72], [84, 96, 108]]\n",
    "#\n",
    "# # Sum along axis 1 (collapse rows) - shape (2,3,3) → (2,3)\n",
    "# sum_axis1 = np.sum(arr3D, axis=1)\n",
    "# # For each matrix, sums down the rows (column-wise sums)\n",
    "# # Matrix 0: [1+4+7, 2+5+8, 3+6+9] = [12, 15, 18]\n",
    "# # Matrix 1: [11+44+77, 22+55+88, 33+66+99] = [132, 165, 198]\n",
    "# # Result: [[12, 15, 18], [132, 165, 198]]\n",
    "#\n",
    "# # Sum along axis 2 (collapse columns) - shape (2,3,3) → (2,3)\n",
    "# sum_axis2 = np.sum(arr3D, axis=2)\n",
    "# # For each matrix, sums across the columns (row-wise sums)\n",
    "# # Matrix 0: [1+2+3, 4+5+6, 7+8+9] = [6, 15, 24]\n",
    "# # Matrix 1: [11+22+33, 44+55+66, 77+88+99] = [66, 165, 264]\n",
    "# # Result: [[6, 15, 24], [66, 165, 264]]\n",
    "#\n",
    "# # Sum along multiple axes\n",
    "# sum_axis_01 = np.sum(arr3D, axis=(0, 1))  # Collapse layers and rows → (3,)\n",
    "# sum_all = np.sum(arr3D)  # Collapse all axes → scalar\n",
    "#\n",
    "# HINGLISH:\n",
    "# 3D arrays mein axis operations samajhna (ML ke liye CRITICAL):\n",
    "#\n",
    "# Shape (2, 3, 3) wale arr3D ke liye:\n",
    "# - axis=0: Depth/layers ke along operate (layers collapse)\n",
    "# - axis=1: Rows ke along operate (rows collapse)\n",
    "# - axis=2: Columns ke along operate (columns collapse)\n",
    "#\n",
    "# Examples:\n",
    "#\n",
    "# # axis 0 ke along sum (layers collapse) - shape (2,3,3) → (3,3)\n",
    "# sum_axis0 = np.sum(arr3D, axis=0)\n",
    "# # Dono matrices se corresponding elements add karta hai\n",
    "# # [[1+11, 2+22, 3+33],\n",
    "# #  [4+44, 5+55, 6+66],\n",
    "# #  [7+77, 8+88, 9+99]]\n",
    "# # = [[12, 24, 36], [48, 60, 72], [84, 96, 108]]\n",
    "#\n",
    "# # axis 1 ke along sum (rows collapse) - shape (2,3,3) → (2,3)\n",
    "# sum_axis1 = np.sum(arr3D, axis=1)\n",
    "# # Har matrix ke liye, rows ko neeche sum karta hai (column-wise sums)\n",
    "# # Matrix 0: [1+4+7, 2+5+8, 3+6+9] = [12, 15, 18]\n",
    "# # Matrix 1: [11+44+77, 22+55+88, 33+66+99] = [132, 165, 198]\n",
    "# # Result: [[12, 15, 18], [132, 165, 198]]\n",
    "#\n",
    "# # axis 2 ke along sum (columns collapse) - shape (2,3,3) → (2,3)\n",
    "# sum_axis2 = np.sum(arr3D, axis=2)\n",
    "# # Har matrix ke liye, columns ko across sum karta hai (row-wise sums)\n",
    "# # Matrix 0: [1+2+3, 4+5+6, 7+8+9] = [6, 15, 24]\n",
    "# # Matrix 1: [11+22+33, 44+55+66, 77+88+99] = [66, 165, 264]\n",
    "# # Result: [[6, 15, 24], [66, 165, 264]]\n",
    "#\n",
    "# # Multiple axes ke along sum\n",
    "# sum_axis_01 = np.sum(arr3D, axis=(0, 1))  # Layers aur rows collapse → (3,)\n",
    "# sum_all = np.sum(arr3D)  # Saare axes collapse → scalar\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# PRACTICAL ML EXAMPLE: RGB IMAGE OPERATIONS\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Real-world scenario: Processing an RGB image\n",
    "#\n",
    "# # Simulate RGB image: 4×4 pixels, 3 color channels (R, G, B)\n",
    "# rgb_image = np.array([\n",
    "#     # Pixel row 0\n",
    "#     [[255, 0, 0],    [255, 0, 0],    [0, 255, 0],    [0, 255, 0]],\n",
    "#     # Pixel row 1\n",
    "#     [[255, 0, 0],    [255, 0, 0],    [0, 255, 0],    [0, 255, 0]],\n",
    "#     # Pixel row 2\n",
    "#     [[0, 0, 255],    [0, 0, 255],    [255, 255, 0],  [255, 255, 0]],\n",
    "#     # Pixel row 3\n",
    "#     [[0, 0, 255],    [0, 0, 255],    [255, 255, 0],  [255, 255, 0]]\n",
    "# ])  # Shape: (4, 4, 3) - height=4, width=4, channels=3\n",
    "#\n",
    "# # Get red channel only (extract plane)\n",
    "# red_channel = rgb_image[:, :, 0]  # Shape: (4, 4)\n",
    "#\n",
    "# # Get green channel\n",
    "# green_channel = rgb_image[:, :, 1]  # Shape: (4, 4)\n",
    "#\n",
    "# # Get blue channel\n",
    "# blue_channel = rgb_image[:, :, 2]  # Shape: (4, 4)\n",
    "#\n",
    "# # Convert to grayscale (average of R, G, B)\n",
    "# grayscale = np.mean(rgb_image, axis=2)  # Shape: (4, 4)\n",
    "#\n",
    "# # Get pixel at position (2, 3) - all channels\n",
    "# pixel_rgb = rgb_image[2, 3, :]  # Shape: (3,) - [R, G, B] values\n",
    "#\n",
    "# # Get average intensity per channel across entire image\n",
    "# avg_red = np.mean(rgb_image[:, :, 0])\n",
    "# avg_green = np.mean(rgb_image[:, :, 1])\n",
    "# avg_blue = np.mean(rgb_image[:, :, 2])\n",
    "# # Or more efficiently:\n",
    "# channel_averages = np.mean(rgb_image, axis=(0, 1))  # Shape: (3,)\n",
    "#\n",
    "# HINGLISH:\n",
    "# Real-world scenario: RGB image process karna\n",
    "#\n",
    "# # RGB image simulate karo: 4×4 pixels, 3 color channels (R, G, B)\n",
    "# rgb_image = np.array([\n",
    "#     # Pixel row 0\n",
    "#     [[255, 0, 0],    [255, 0, 0],    [0, 255, 0],    [0, 255, 0]],\n",
    "#     # Pixel row 1\n",
    "#     [[255, 0, 0],    [255, 0, 0],    [0, 255, 0],    [0, 255, 0]],\n",
    "#     # Pixel row 2\n",
    "#     [[0, 0, 255],    [0, 0, 255],    [255, 255, 0],  [255, 255, 0]],\n",
    "#     # Pixel row 3\n",
    "#     [[0, 0, 255],    [0, 0, 255],    [255, 255, 0],  [255, 255, 0]]\n",
    "# ])  # Shape: (4, 4, 3) - height=4, width=4, channels=3\n",
    "#\n",
    "# # Sirf red channel lo (plane extract karo)\n",
    "# red_channel = rgb_image[:, :, 0]  # Shape: (4, 4)\n",
    "#\n",
    "# # Green channel lo\n",
    "# green_channel = rgb_image[:, :, 1]  # Shape: (4, 4)\n",
    "#\n",
    "# # Blue channel lo\n",
    "# blue_channel = rgb_image[:, :, 2]  # Shape: (4, 4)\n",
    "#\n",
    "# # Grayscale mein convert karo (R, G, B ka average)\n",
    "# grayscale = np.mean(rgb_image, axis=2)  # Shape: (4, 4)\n",
    "#\n",
    "# # Position (2, 3) par pixel lo - saare channels\n",
    "# pixel_rgb = rgb_image[2, 3, :]  # Shape: (3,) - [R, G, B] values\n",
    "#\n",
    "# # Poori image mein har channel ka average intensity lo\n",
    "# avg_red = np.mean(rgb_image[:, :, 0])\n",
    "# avg_green = np.mean(rgb_image[:, :, 1])\n",
    "# avg_blue = np.mean(rgb_image[:, :, 2])\n",
    "# # Ya zyada efficiently:\n",
    "# channel_averages = np.mean(rgb_image, axis=(0, 1))  # Shape: (3,)\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# 3D SLICING EXAMPLES\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Slicing works independently on each axis:\n",
    "#\n",
    "# # Get both matrices, first 2 rows, all columns\n",
    "# slice1 = arr3D[:, :2, :]\n",
    "# # Shape: (2, 2, 3)\n",
    "#\n",
    "# # Get first matrix, all rows, last 2 columns\n",
    "# slice2 = arr3D[0, :, -2:]\n",
    "# # Shape: (3, 2)\n",
    "#\n",
    "# # Get second matrix, middle row only\n",
    "# slice3 = arr3D[1, 1, :]\n",
    "# # Shape: (3,) - [44, 55, 66]\n",
    "#\n",
    "# # Get diagonal elements from both matrices\n",
    "# diag0 = arr3D[0, [0,1,2], [0,1,2]]  # [1, 5, 9]\n",
    "# diag1 = arr3D[1, [0,1,2], [0,1,2]]  # [11, 55, 99]\n",
    "#\n",
    "# HINGLISH:\n",
    "# Har axis par independently slicing kaam karti hai:\n",
    "#\n",
    "# # Dono matrices lo, pehli 2 rows, saare columns\n",
    "# slice1 = arr3D[:, :2, :]\n",
    "# # Shape: (2, 2, 3)\n",
    "#\n",
    "# # Pehli matrix lo, saari rows, aakhri 2 columns\n",
    "# slice2 = arr3D[0, :, -2:]\n",
    "# # Shape: (3, 2)\n",
    "#\n",
    "# # Doosri matrix lo, sirf middle row\n",
    "# slice3 = arr3D[1, 1, :]\n",
    "# # Shape: (3,) - [44, 55, 66]\n",
    "#\n",
    "# # Dono matrices se diagonal elements lo\n",
    "# diag0 = arr3D[0, [0,1,2], [0,1,2]]  # [1, 5, 9]\n",
    "# diag1 = arr3D[1, [0,1,2], [0,1,2]]  # [11, 55, 99]\n",
    "# =============================================================================\n",
    "\n",
    "# **Quick Reference: 3D Array Structure**\n",
    "\n",
    "# | Dimension | Name | Common Interpretations | Index Range (this example) |\n",
    "# |-----------|------|------------------------|---------------------------|\n",
    "# | axis=0 | Depth/Layer/Batch | Matrices, Images, Samples | 0-1 (2 layers) |\n",
    "# | axis=1 | Height/Row | Vertical position | 0-2 (3 rows) |\n",
    "# | axis=2 | Width/Column | Horizontal position | 0-2 (3 columns) |\n",
    "\n",
    "# **Common 3D Shapes in ML:**\n",
    "# - **RGB Image**: `(height, width, 3)` - e.g., `(224, 224, 3)`\n",
    "# - **Batch of Grayscale**: `(batch, height, width)` - e.g., `(32, 28, 28)`\n",
    "# - **Batch of RGB**: `(batch, height, width, 3)` - e.g., `(32, 224, 224, 3)`\n",
    "# - **Time Series Batch**: `(batch, timesteps, features)` - e.g., `(64, 100, 10)`\n",
    "\n",
    "# **Indexing Pattern:**\n",
    "# ```\n",
    "# arr3D[layer, row, column]\n",
    "#       ↑      ↑     ↑\n",
    "#    axis=0  axis=1 axis=2\n",
    "#    depth   height  width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf2e367a-a46f-4fdc-8a08-c233ec7eaef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  4  9 16 25]\n",
      "[3 4 5 6 7]\n",
      "[ 6  8 10 12 14]\n",
      "[[ 2  4  6  8 10]\n",
      " [ 3  5  7  9 11]]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# NUMPY VECTORIZATION - EFFICIENT ELEMENT-WISE OPERATIONS\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# WHAT IS VECTORIZATION?\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Vectorization is NumPy's ability to perform operations on entire arrays\n",
    "# WITHOUT explicit Python loops. This is:\n",
    "# - MUCH faster (10-100x) than Python loops because operations run in C\n",
    "# - More readable and concise\n",
    "# - Automatically parallelized on modern CPUs\n",
    "# - The CORE reason NumPy exists and why it's essential for ML\n",
    "#\n",
    "# Instead of:\n",
    "#   result = []\n",
    "#   for i in range(len(arr)):\n",
    "#       result.append(arr[i] ** 2)  # SLOW - Python loop\n",
    "#\n",
    "# You write:\n",
    "#   result = arr ** 2  # FAST - vectorized operation\n",
    "#\n",
    "# HINGLISH:\n",
    "# Vectorization NumPy ki ability hai poore arrays par operations perform karne\n",
    "# ki BINA explicit Python loops ke. Yeh:\n",
    "# - Python loops se BAHUT zyada fast hai (10-100x) kyunki operations C mein\n",
    "#   run hote hain\n",
    "# - Zyada readable aur concise hai\n",
    "# - Modern CPUs par automatically parallelized hai\n",
    "# - NumPy exist karne ki aur ML ke liye essential hone ki CORE reason hai\n",
    "#\n",
    "# Iske bajaye:\n",
    "#   result = []\n",
    "#   for i in range(len(arr)):\n",
    "#       result.append(arr[i] ** 2)  # SLOW - Python loop\n",
    "#\n",
    "# Aap likhte hain:\n",
    "#   result = arr ** 2  # FAST - vectorized operation\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# =============================================================================\n",
    "# CREATING ARRAYS FOR VECTORIZATION DEMONSTRATIONS\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Creates sample arrays to demonstrate vectorized operations\n",
    "# - arr: 1D array with 5 elements\n",
    "# - arr2: Another 1D array with 5 elements (same shape as arr)\n",
    "# - arr3: 2D array with shape (2, 5) - demonstrates broadcasting\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Vectorized operations demonstrate karne ke liye sample\n",
    "#   arrays banata hai\n",
    "# - arr: 5 elements wala 1D array\n",
    "# - arr2: Doosra 1D array with 5 elements (arr jaisa hi shape)\n",
    "# - arr3: Shape (2, 5) wala 2D array - broadcasting demonstrate karta hai\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "arr = np.array([1, 2, 3, 4, 5])\n",
    "arr2 = np.array([5, 6, 7, 8, 9])\n",
    "arr3 = np.array([[1, 2, 3, 4, 5],   # Row 0\n",
    "                 [2, 3, 4, 5, 6]])  # Row 1\n",
    "\n",
    "# =============================================================================\n",
    "# OPERATION 1: ELEMENT-WISE EXPONENTIATION (arr ** 2)\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Squares each element in the array independently. The **\n",
    "#   operator performs element-wise exponentiation.\n",
    "# - Syntax: array ** power\n",
    "# - Why use it: Common in ML for:\n",
    "#   * Computing squared errors: (predictions - actual) ** 2\n",
    "#   * Calculating variance and standard deviation\n",
    "#   * Feature engineering: creating polynomial features\n",
    "#   * L2 regularization: sum of squared weights\n",
    "#   * Euclidean distance: sqrt(sum((x - y) ** 2))\n",
    "# - How it works:\n",
    "#   * Takes each element independently\n",
    "#   * Raises it to the specified power\n",
    "#   * Returns new array with same shape\n",
    "#   * MUCH faster than Python loop\n",
    "# - Performance comparison:\n",
    "#   # Slow Python way (avoid this):\n",
    "#   result = [x ** 2 for x in arr]  # List comprehension - slower\n",
    "#   \n",
    "#   # Fast NumPy way (use this):\n",
    "#   result = arr ** 2  # Vectorized - 10-100x faster\n",
    "# - Common use cases:\n",
    "#   * Mean Squared Error (MSE): np.mean((y_pred - y_true) ** 2)\n",
    "#   * L2 norm (magnitude): np.sqrt(np.sum(vector ** 2))\n",
    "#   * Variance: np.mean((data - mean) ** 2)\n",
    "#   * Polynomial features: X_squared = X ** 2\n",
    "#   * Distance calculations in clustering/KNN\n",
    "# - Example:\n",
    "#   # Mean Squared Error calculation\n",
    "#   predictions = np.array([2.5, 3.0, 4.2, 5.1])\n",
    "#   actual = np.array([2.0, 3.5, 4.0, 5.0])\n",
    "#   squared_errors = (predictions - actual) ** 2\n",
    "#   # [0.25, 0.25, 0.04, 0.01]\n",
    "#   mse = np.mean(squared_errors)  # 0.1375\n",
    "#   \n",
    "#   # L2 norm (Euclidean length of vector)\n",
    "#   vector = np.array([3, 4])\n",
    "#   magnitude = np.sqrt(np.sum(vector ** 2))  # 5.0\n",
    "#   \n",
    "#   # Creating polynomial features\n",
    "#   X = np.array([1, 2, 3, 4, 5])\n",
    "#   X_poly = np.column_stack([X, X**2, X**3])\n",
    "#   # [[1, 1, 1], [2, 4, 8], [3, 9, 27], [4, 16, 64], [5, 25, 125]]\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Array ke har element ko independently square karta hai.\n",
    "#   ** operator element-wise exponentiation perform karta hai.\n",
    "# - Syntax: array ** power\n",
    "# - Kab use karein: ML mein common hai:\n",
    "#   * Squared errors compute karna: (predictions - actual) ** 2\n",
    "#   * Variance aur standard deviation calculate karna\n",
    "#   * Feature engineering: polynomial features banana\n",
    "#   * L2 regularization: squared weights ka sum\n",
    "#   * Euclidean distance: sqrt(sum((x - y) ** 2))\n",
    "# - Kaise kaam karta hai:\n",
    "#   * Har element ko independently leta hai\n",
    "#   * Use specified power tak raise karta hai\n",
    "#   * Same shape wala naya array return karta hai\n",
    "#   * Python loop se BAHUT zyada fast hai\n",
    "# - Performance comparison:\n",
    "#   # Slow Python tarika (isse avoid karo):\n",
    "#   result = [x ** 2 for x in arr]  # List comprehension - slower\n",
    "#   \n",
    "#   # Fast NumPy tarika (isse use karo):\n",
    "#   result = arr ** 2  # Vectorized - 10-100x faster\n",
    "# - Common use cases:\n",
    "#   * Mean Squared Error (MSE): np.mean((y_pred - y_true) ** 2)\n",
    "#   * L2 norm (magnitude): np.sqrt(np.sum(vector ** 2))\n",
    "#   * Variance: np.mean((data - mean) ** 2)\n",
    "#   * Polynomial features: X_squared = X ** 2\n",
    "#   * Clustering/KNN mein distance calculations\n",
    "# - Example:\n",
    "#   # Mean Squared Error calculation\n",
    "#   predictions = np.array([2.5, 3.0, 4.2, 5.1])\n",
    "#   actual = np.array([2.0, 3.5, 4.0, 5.0])\n",
    "#   squared_errors = (predictions - actual) ** 2\n",
    "#   # [0.25, 0.25, 0.04, 0.01]\n",
    "#   mse = np.mean(squared_errors)  # 0.1375\n",
    "#   \n",
    "#   # L2 norm (vector ki Euclidean length)\n",
    "#   vector = np.array([3, 4])\n",
    "#   magnitude = np.sqrt(np.sum(vector ** 2))  # 5.0\n",
    "#   \n",
    "#   # Polynomial features banana\n",
    "#   X = np.array([1, 2, 3, 4, 5])\n",
    "#   X_poly = np.column_stack([X, X**2, X**3])\n",
    "#   # [[1, 1, 1], [2, 4, 8], [3, 9, 27], [4, 16, 64], [5, 25, 125]]\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(arr ** 2)\n",
    "# Output: [ 1  4  9 16 25]\n",
    "#\n",
    "# Element-wise calculation:\n",
    "# [1, 2, 3, 4, 5] ** 2\n",
    "#  ↓  ↓  ↓  ↓  ↓\n",
    "# [1² 2² 3² 4² 5²]\n",
    "#  ↓  ↓  ↓  ↓  ↓\n",
    "# [1, 4, 9, 16, 25]\n",
    "#\n",
    "# Each element is squared independently\n",
    "# No loops needed - operation is vectorized\n",
    "\n",
    "# =============================================================================\n",
    "# OPERATION 2: SCALAR ADDITION (arr + scalar)\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Adds a scalar (single number) to every element in the array.\n",
    "#   This is called \"broadcasting\" - the scalar is automatically extended to\n",
    "#   match the array shape.\n",
    "# - Syntax: array + scalar (or scalar + array - commutative)\n",
    "# - Why use it: Essential for:\n",
    "#   * Data normalization: shifting data by a constant\n",
    "#   * Bias addition in neural networks: output = weights @ input + bias\n",
    "#   * Feature scaling: adjusting ranges\n",
    "#   * Temperature conversion: celsius + 273.15 = kelvin\n",
    "#   * Offset corrections in data preprocessing\n",
    "# - Broadcasting behavior:\n",
    "#   * Scalar is treated as array of same shape with repeated value\n",
    "#   * [1, 2, 3] + 2 becomes [1, 2, 3] + [2, 2, 2]\n",
    "#   * Then element-wise addition is performed\n",
    "# - All arithmetic operators work this way:\n",
    "#   * Addition: arr + 5\n",
    "#   * Subtraction: arr - 3\n",
    "#   * Multiplication: arr * 2\n",
    "#   * Division: arr / 4\n",
    "#   * Power: arr ** 0.5\n",
    "#   * Modulo: arr % 2\n",
    "# - Common use cases:\n",
    "#   * Data centering: data - mean\n",
    "#   * Adding bias in neural networks\n",
    "#   * Temperature conversions\n",
    "#   * Shifting probability distributions\n",
    "#   * Adjusting pixel intensities in images\n",
    "# - Example:\n",
    "#   # Centering data (zero-mean normalization)\n",
    "#   data = np.array([10, 20, 30, 40, 50])\n",
    "#   mean = np.mean(data)  # 30\n",
    "#   centered = data - mean  # [-20, -10, 0, 10, 20]\n",
    "#   \n",
    "#   # Temperature conversion\n",
    "#   celsius = np.array([0, 10, 20, 30, 100])\n",
    "#   fahrenheit = celsius * 9/5 + 32  # [32, 50, 68, 86, 212]\n",
    "#   \n",
    "#   # Neural network bias addition\n",
    "#   weighted_sum = np.array([0.5, -0.3, 0.8])\n",
    "#   bias = 0.1\n",
    "#   output = weighted_sum + bias  # [0.6, -0.2, 0.9]\n",
    "#   \n",
    "#   # Image brightness adjustment\n",
    "#   image = np.random.rand(256, 256) * 255  # Grayscale image\n",
    "#   brighter = image + 50  # Increase brightness by 50\n",
    "#   brighter = np.clip(brighter, 0, 255)  # Keep in valid range\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Array ke har element mein ek scalar (single number)\n",
    "#   add karta hai. Ise \"broadcasting\" kehte hain - scalar automatically array\n",
    "#   shape match karne ke liye extend ho jata hai.\n",
    "# - Syntax: array + scalar (ya scalar + array - commutative hai)\n",
    "# - Kab use karein: Zaroori hai:\n",
    "#   * Data normalization: constant se data shift karna\n",
    "#   * Neural networks mein bias addition: output = weights @ input + bias\n",
    "#   * Feature scaling: ranges adjust karna\n",
    "#   * Temperature conversion: celsius + 273.15 = kelvin\n",
    "#   * Data preprocessing mein offset corrections\n",
    "# - Broadcasting behavior:\n",
    "#   * Scalar ko same shape ke array ki tarah treat kiya jata hai with repeated value\n",
    "#   * [1, 2, 3] + 2 ban jata hai [1, 2, 3] + [2, 2, 2]\n",
    "#   * Phir element-wise addition perform hota hai\n",
    "# - Saare arithmetic operators aise hi kaam karte hain:\n",
    "#   * Addition: arr + 5\n",
    "#   * Subtraction: arr - 3\n",
    "#   * Multiplication: arr * 2\n",
    "#   * Division: arr / 4\n",
    "#   * Power: arr ** 0.5\n",
    "#   * Modulo: arr % 2\n",
    "# - Common use cases:\n",
    "#   * Data centering: data - mean\n",
    "#   * Neural networks mein bias add karna\n",
    "#   * Temperature conversions\n",
    "#   * Probability distributions shift karna\n",
    "#   * Images mein pixel intensities adjust karna\n",
    "# - Example:\n",
    "#   # Data center karna (zero-mean normalization)\n",
    "#   data = np.array([10, 20, 30, 40, 50])\n",
    "#   mean = np.mean(data)  # 30\n",
    "#   centered = data - mean  # [-20, -10, 0, 10, 20]\n",
    "#   \n",
    "#   # Temperature conversion\n",
    "#   celsius = np.array([0, 10, 20, 30, 100])\n",
    "#   fahrenheit = celsius * 9/5 + 32  # [32, 50, 68, 86, 212]\n",
    "#   \n",
    "#   # Neural network mein bias addition\n",
    "#   weighted_sum = np.array([0.5, -0.3, 0.8])\n",
    "#   bias = 0.1\n",
    "#   output = weighted_sum + bias  # [0.6, -0.2, 0.9]\n",
    "#   \n",
    "#   # Image brightness adjustment\n",
    "#   image = np.random.rand(256, 256) * 255  # Grayscale image\n",
    "#   brighter = image + 50  # Brightness 50 se increase karo\n",
    "#   brighter = np.clip(brighter, 0, 255)  # Valid range mein rakho\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(arr + 2)\n",
    "# Output: [3 4 5 6 7]\n",
    "#\n",
    "# Broadcasting visualization:\n",
    "# [1, 2, 3, 4, 5] + 2\n",
    "#  ↓  ↓  ↓  ↓  ↓     ↓ (scalar broadcasts to match array)\n",
    "# [1, 2, 3, 4, 5] + [2, 2, 2, 2, 2]\n",
    "#  ↓  ↓  ↓  ↓  ↓     ↓  ↓  ↓  ↓  ↓\n",
    "# [3, 4, 5, 6, 7]\n",
    "#\n",
    "# The scalar 2 is added to each element\n",
    "\n",
    "# =============================================================================\n",
    "# OPERATION 3: ARRAY + ARRAY (SAME SHAPE)\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Performs element-wise addition of two arrays with the SAME\n",
    "#   shape. Corresponding elements are added together.\n",
    "# - Syntax: array1 + array2 (arrays must have compatible shapes)\n",
    "# - Why use it: Fundamental operation in ML for:\n",
    "#   * Combining predictions from multiple models (ensemble learning)\n",
    "#   * Adding gradients in backpropagation\n",
    "#   * Vector addition in linear algebra\n",
    "#   * Merging feature vectors\n",
    "#   * Computing residuals: actual - prediction\n",
    "# - How it works:\n",
    "#   * Arrays must have same shape or be broadcastable\n",
    "#   * Elements at same positions are added\n",
    "#   * Returns new array with same shape\n",
    "#   * Much faster than Python loops\n",
    "# - All element-wise operators work similarly:\n",
    "#   * Addition: arr1 + arr2\n",
    "#   * Subtraction: arr1 - arr2\n",
    "#   * Multiplication: arr1 * arr2 (NOT matrix multiplication!)\n",
    "#   * Division: arr1 / arr2\n",
    "#   * Power: arr1 ** arr2\n",
    "#   * Comparison: arr1 > arr2, arr1 == arr2\n",
    "# - Common use cases:\n",
    "#   * Ensemble predictions: (model1_pred + model2_pred) / 2\n",
    "#   * Gradient accumulation in training\n",
    "#   * Computing errors: predictions - targets\n",
    "#   * Feature combinations: feature1 + feature2\n",
    "#   * Vector arithmetic in embeddings\n",
    "# - Example:\n",
    "#   # Ensemble learning - averaging two models\n",
    "#   model1_pred = np.array([0.8, 0.2, 0.6, 0.9])\n",
    "#   model2_pred = np.array([0.7, 0.3, 0.5, 0.8])\n",
    "#   ensemble_pred = (model1_pred + model2_pred) / 2\n",
    "#   # [0.75, 0.25, 0.55, 0.85]\n",
    "#   \n",
    "#   # Computing prediction errors\n",
    "#   predictions = np.array([2.5, 3.2, 4.1, 5.0])\n",
    "#   actual = np.array([2.0, 3.5, 4.0, 5.2])\n",
    "#   errors = predictions - actual  # [0.5, -0.3, 0.1, -0.2]\n",
    "#   \n",
    "#   # Gradient accumulation\n",
    "#   grad_batch1 = np.array([0.1, 0.2, 0.3])\n",
    "#   grad_batch2 = np.array([0.15, 0.18, 0.25])\n",
    "#   total_grad = grad_batch1 + grad_batch2  # [0.25, 0.38, 0.55]\n",
    "#   \n",
    "#   # Vector addition\n",
    "#   vec1 = np.array([1, 2, 3])\n",
    "#   vec2 = np.array([4, 5, 6])\n",
    "#   result = vec1 + vec2  # [5, 7, 9]\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: SAME shape wale do arrays ka element-wise addition\n",
    "#   perform karta hai. Corresponding elements ko ek saath add kiya jata hai.\n",
    "# - Syntax: array1 + array2 (arrays ka shape compatible hona chahiye)\n",
    "# - Kab use karein: ML mein fundamental operation hai:\n",
    "#   * Multiple models se predictions combine karna (ensemble learning)\n",
    "#   * Backpropagation mein gradients add karna\n",
    "#   * Linear algebra mein vector addition\n",
    "#   * Feature vectors merge karna\n",
    "#   * Residuals compute karna: actual - prediction\n",
    "# - Kaise kaam karta hai:\n",
    "#   * Arrays ka same shape hona chahiye ya broadcastable hona chahiye\n",
    "#   * Same positions par elements add hote hain\n",
    "#   * Same shape wala naya array return hota hai\n",
    "#   * Python loops se bahut zyada fast hai\n",
    "# - Saare element-wise operators aise hi kaam karte hain:\n",
    "#   * Addition: arr1 + arr2\n",
    "#   * Subtraction: arr1 - arr2\n",
    "#   * Multiplication: arr1 * arr2 (matrix multiplication NAHI!)\n",
    "#   * Division: arr1 / arr2\n",
    "#   * Power: arr1 ** arr2\n",
    "#   * Comparison: arr1 > arr2, arr1 == arr2\n",
    "# - Common use cases:\n",
    "#   * Ensemble predictions: (model1_pred + model2_pred) / 2\n",
    "#   * Training mein gradient accumulation\n",
    "#   * Errors compute karna: predictions - targets\n",
    "#   * Feature combinations: feature1 + feature2\n",
    "#   * Embeddings mein vector arithmetic\n",
    "# - Example:\n",
    "#   # Ensemble learning - do models average karna\n",
    "#   model1_pred = np.array([0.8, 0.2, 0.6, 0.9])\n",
    "#   model2_pred = np.array([0.7, 0.3, 0.5, 0.8])\n",
    "#   ensemble_pred = (model1_pred + model2_pred) / 2\n",
    "#   # [0.75, 0.25, 0.55, 0.85]\n",
    "#   \n",
    "#   # Prediction errors compute karna\n",
    "#   predictions = np.array([2.5, 3.2, 4.1, 5.0])\n",
    "#   actual = np.array([2.0, 3.5, 4.0, 5.2])\n",
    "#   errors = predictions - actual  # [0.5, -0.3, 0.1, -0.2]\n",
    "#   \n",
    "#   # Gradient accumulation\n",
    "#   grad_batch1 = np.array([0.1, 0.2, 0.3])\n",
    "#   grad_batch2 = np.array([0.15, 0.18, 0.25])\n",
    "#   total_grad = grad_batch1 + grad_batch2  # [0.25, 0.38, 0.55]\n",
    "#   \n",
    "#   # Vector addition\n",
    "#   vec1 = np.array([1, 2, 3])\n",
    "#   vec2 = np.array([4, 5, 6])\n",
    "#   result = vec1 + vec2  # [5, 7, 9]\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(arr + arr2)\n",
    "# Output: [ 6  8 10 12 14]\n",
    "#\n",
    "# Element-wise addition:\n",
    "# [1, 2, 3, 4,  5]  (arr)\n",
    "# +\n",
    "# [5, 6, 7, 8,  9]  (arr2)\n",
    "# ↓  ↓  ↓  ↓   ↓\n",
    "# [6, 8, 10, 12, 14]\n",
    "#\n",
    "# Each pair of corresponding elements is added:\n",
    "# 1+5=6, 2+6=8, 3+7=10, 4+8=12, 5+9=14\n",
    "\n",
    "# =============================================================================\n",
    "# OPERATION 4: BROADCASTING - ARRAY + ARRAY (DIFFERENT SHAPES)\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Adds arrays of DIFFERENT but compatible shapes by\n",
    "#   automatically \"broadcasting\" the smaller array to match the larger one.\n",
    "# - Syntax: array1 + array2 (where shapes are compatible for broadcasting)\n",
    "# - Why use it: EXTREMELY powerful feature that makes code concise and fast:\n",
    "#   * Adding bias to each sample in a batch\n",
    "#   * Normalizing each feature independently\n",
    "#   * Image operations across channels\n",
    "#   * Batch processing without explicit loops\n",
    "# - Broadcasting rules (NumPy automatically applies):\n",
    "#   1. If arrays have different number of dimensions, prepend 1s to smaller\n",
    "#   2. Arrays are compatible if for each dimension:\n",
    "#      - They are equal, OR\n",
    "#      - One of them is 1\n",
    "#   3. The smaller array is \"stretched\" to match larger array's shape\n",
    "# - How it works for arr (shape 5,) + arr3 (shape 2,5):\n",
    "#   Step 1: arr shape (5,) becomes (1, 5) - prepend dimension\n",
    "#   Step 2: arr (1, 5) broadcasts to (2, 5) - repeat along axis 0\n",
    "#   Step 3: Element-wise addition with arr3 (2, 5)\n",
    "# - Broadcasting examples with different shapes:\n",
    "#   (3, 1) + (3, 4) → (3, 4)  ✓ Compatible\n",
    "#   (5,) + (5, 3) → (5, 3)    ✓ Compatible\n",
    "#   (3, 1) + (1, 4) → (3, 4)  ✓ Compatible\n",
    "#   (3, 2) + (3, 4) → Error   ✗ Incompatible\n",
    "# - Common use cases:\n",
    "#   * Adding bias to batch: batch_output + bias_vector\n",
    "#   * Feature normalization: (X - mean) / std (mean & std per feature)\n",
    "#   * Image processing: img + color_adjustment (per channel)\n",
    "#   * Matrix operations with row/column vectors\n",
    "# - Example:\n",
    "#   # Adding bias to each sample in batch\n",
    "#   batch = np.array([[1, 2, 3],\n",
    "#                     [4, 5, 6],\n",
    "#                     [7, 8, 9]])  # shape: (3, 3) - 3 samples, 3 features\n",
    "#   bias = np.array([0.1, 0.2, 0.3])  # shape: (3,) - one bias per feature\n",
    "#   output = batch + bias  # Broadcasting: (3,3) + (3,) → (3,3)\n",
    "#   # [[1.1, 2.2, 3.3],\n",
    "#   #  [4.1, 5.2, 6.3],\n",
    "#   #  [7.1, 8.2, 9.3]]\n",
    "#   \n",
    "#   # Feature normalization (mean & std per feature)\n",
    "#   X = np.array([[1, 10, 100],\n",
    "#                 [2, 20, 200],\n",
    "#                 [3, 30, 300]])  # shape: (3, 3)\n",
    "#   mean = np.array([2, 20, 200])  # shape: (3,) - mean per feature\n",
    "#   std = np.array([1, 10, 100])   # shape: (3,) - std per feature\n",
    "#   X_normalized = (X - mean) / std  # Broadcasting both operations\n",
    "#   \n",
    "#   # Image RGB adjustment\n",
    "#   img = np.random.rand(256, 256, 3)  # shape: (256, 256, 3)\n",
    "#   rgb_adjust = np.array([1.1, 1.0, 0.9])  # shape: (3,) - R,G,B adjustments\n",
    "#   adjusted = img * rgb_adjust  # Broadcasting: (256,256,3) * (3,)\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: DIFFERENT lekin compatible shapes wale arrays ko\n",
    "#   automatically chhote array ko bade array ke shape mein \"broadcast\" karke\n",
    "#   add karta hai.\n",
    "# - Syntax: array1 + array2 (jahan shapes broadcasting ke liye compatible hain)\n",
    "# - Kab use karein: BAHUT powerful feature jo code ko concise aur fast banata hai:\n",
    "#   * Batch mein har sample mein bias add karna\n",
    "#   * Har feature ko independently normalize karna\n",
    "#   * Channels across image operations\n",
    "#   * Explicit loops ke bina batch processing\n",
    "# - Broadcasting rules (NumPy automatically apply karta hai):\n",
    "#   1. Agar arrays ke dimensions alag hain, to chhote mein 1s prepend karo\n",
    "#   2. Arrays compatible hain agar har dimension ke liye:\n",
    "#      - Wo equal hain, YA\n",
    "#      - Unme se ek 1 hai\n",
    "#   3. Chhota array bade array ke shape match karne ke liye \"stretched\" hota hai\n",
    "# - arr (shape 5,) + arr3 (shape 2,5) ke liye kaise kaam karta hai:\n",
    "#   Step 1: arr ka shape (5,) ban jata hai (1, 5) - dimension prepend\n",
    "#   Step 2: arr (1, 5) broadcast hota hai (2, 5) mein - axis 0 ke along repeat\n",
    "#   Step 3: arr3 (2, 5) ke saath element-wise addition\n",
    "# - Alag shapes ke saath broadcasting examples:\n",
    "#   (3, 1) + (3, 4) → (3, 4)  ✓ Compatible\n",
    "#   (5,) + (5, 3) → (5, 3)    ✓ Compatible\n",
    "#   (3, 1) + (1, 4) → (3, 4)  ✓ Compatible\n",
    "#   (3, 2) + (3, 4) → Error   ✗ Incompatible\n",
    "# - Common use cases:\n",
    "#   * Batch mein bias add karna: batch_output + bias_vector\n",
    "#   * Feature normalization: (X - mean) / std (har feature ke liye mean & std)\n",
    "#   * Image processing: img + color_adjustment (har channel ke liye)\n",
    "#   * Row/column vectors ke saath matrix operations\n",
    "# - Example:\n",
    "#   # Batch mein har sample mein bias add karna\n",
    "#   batch = np.array([[1, 2, 3],\n",
    "#                     [4, 5, 6],\n",
    "#                     [7, 8, 9]])  # shape: (3, 3) - 3 samples, 3 features\n",
    "#   bias = np.array([0.1, 0.2, 0.3])  # shape: (3,) - har feature ke liye ek bias\n",
    "#   output = batch + bias  # Broadcasting: (3,3) + (3,) → (3,3)\n",
    "#   # [[1.1, 2.2, 3.3],\n",
    "#   #  [4.1, 5.2, 6.3],\n",
    "#   #  [7.1, 8.2, 9.3]]\n",
    "#   \n",
    "#   # Feature normalization (har feature ke liye mean & std)\n",
    "#   X = np.array([[1, 10, 100],\n",
    "#                 [2, 20, 200],\n",
    "#                 [3, 30, 300]])  # shape: (3, 3)\n",
    "#   mean = np.array([2, 20, 200])  # shape: (3,) - har feature ka mean\n",
    "#   std = np.array([1, 10, 100])   # shape: (3,) - har feature ka std\n",
    "#   X_normalized = (X - mean) / std  # Dono operations broadcast hote hain\n",
    "#   \n",
    "#   # Image RGB adjustment\n",
    "#   img = np.random.rand(256, 256, 3)  # shape: (256, 256, 3)\n",
    "#   rgb_adjust = np.array([1.1, 1.0, 0.9])  # shape: (3,) - R,G,B adjustments\n",
    "#   adjusted = img * rgb_adjust  # Broadcasting: (256,256,3) * (3,)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(arr + arr3)\n",
    "# Output:\n",
    "# [[2 4 6 8 10]\n",
    "#  [3 5 7 9 11]]\n",
    "#\n",
    "# Broadcasting visualization:\n",
    "# arr shape:  (5,)   → treated as (1, 5) → broadcast to (2, 5)\n",
    "# arr3 shape: (2, 5) → stays as (2, 5)\n",
    "#\n",
    "# Step-by-step:\n",
    "# Original arr:  [1, 2, 3, 4, 5]  (shape: 5,)\n",
    "# \n",
    "# After broadcasting arr to (2, 5):\n",
    "# [[1, 2, 3, 4, 5],   ← arr broadcasted (repeated)\n",
    "#  [1, 2, 3, 4, 5]]   ← same row repeated\n",
    "# \n",
    "# arr3:\n",
    "# [[1, 2, 3, 4, 5],   ← Row 0\n",
    "#  [2, 3, 4, 5, 6]]   ← Row 1\n",
    "# \n",
    "# Element-wise addition:\n",
    "# [[1+1, 2+2, 3+3, 4+4, 5+5],\n",
    "#  [1+2, 2+3, 3+4, 4+5, 5+6]]\n",
    "# \n",
    "# Result:\n",
    "# [[2, 4, 6, 8, 10],\n",
    "#  [3, 5, 7, 9, 11]]\n",
    "\n",
    "# =============================================================================\n",
    "# ALL VECTORIZED ARITHMETIC OPERATIONS\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# NumPy supports all arithmetic operations in vectorized form:\n",
    "#\n",
    "# # Addition\n",
    "# arr + 5          # Add scalar to all elements\n",
    "# arr1 + arr2      # Element-wise addition\n",
    "#\n",
    "# # Subtraction\n",
    "# arr - 3          # Subtract scalar from all elements\n",
    "# arr1 - arr2      # Element-wise subtraction\n",
    "#\n",
    "# # Multiplication (element-wise, NOT matrix multiplication)\n",
    "# arr * 2          # Multiply all elements by scalar\n",
    "# arr1 * arr2      # Element-wise multiplication\n",
    "#\n",
    "# # Division\n",
    "# arr / 4          # Divide all elements by scalar\n",
    "# arr1 / arr2      # Element-wise division\n",
    "#\n",
    "# # Floor division\n",
    "# arr // 3         # Integer division\n",
    "#\n",
    "# # Modulo\n",
    "# arr % 2          # Remainder after division\n",
    "#\n",
    "# # Exponentiation\n",
    "# arr ** 2         # Square all elements\n",
    "# arr ** 0.5       # Square root (same as np.sqrt(arr))\n",
    "# arr1 ** arr2     # Element-wise power\n",
    "#\n",
    "# # Comparison operations (return boolean arrays)\n",
    "# arr > 3          # [False, False, False, True, True]\n",
    "# arr == 3         # [False, False, True, False, False]\n",
    "# arr1 < arr2      # Element-wise comparison\n",
    "#\n",
    "# # Logical operations\n",
    "# (arr > 2) & (arr < 5)   # AND operation\n",
    "# (arr < 2) | (arr > 4)   # OR operation\n",
    "# ~(arr > 3)              # NOT operation\n",
    "#\n",
    "# HINGLISH:\n",
    "# NumPy saare arithmetic operations ko vectorized form mein support karta hai:\n",
    "#\n",
    "# # Addition\n",
    "# arr + 5          # Saare elements mein scalar add karo\n",
    "# arr1 + arr2      # Element-wise addition\n",
    "#\n",
    "# # Subtraction\n",
    "# arr - 3          # Saare elements se scalar subtract karo\n",
    "# arr1 - arr2      # Element-wise subtraction\n",
    "#\n",
    "# # Multiplication (element-wise, matrix multiplication NAHI)\n",
    "# arr * 2          # Saare elements ko scalar se multiply karo\n",
    "# arr1 * arr2      # Element-wise multiplication\n",
    "#\n",
    "# # Division\n",
    "# arr / 4          # Saare elements ko scalar se divide karo\n",
    "# arr1 / arr2      # Element-wise division\n",
    "#\n",
    "# # Floor division\n",
    "# arr // 3         # Integer division\n",
    "#\n",
    "# # Modulo\n",
    "# arr % 2          # Division ke baad remainder\n",
    "#\n",
    "# # Exponentiation\n",
    "# arr ** 2         # Saare elements ko square karo\n",
    "# arr ** 0.5       # Square root (np.sqrt(arr) jaisa hi)\n",
    "# arr1 ** arr2     # Element-wise power\n",
    "#\n",
    "# # Comparison operations (boolean arrays return karte hain)\n",
    "# arr > 3          # [False, False, False, True, True]\n",
    "# arr == 3         # [False, False, True, False, False]\n",
    "# arr1 < arr2      # Element-wise comparison\n",
    "#\n",
    "# # Logical operations\n",
    "# (arr > 2) & (arr < 5)   # AND operation\n",
    "# (arr < 2) | (arr > 4)   # OR operation\n",
    "# ~(arr > 3)              # NOT operation\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# PERFORMANCE COMPARISON: LOOP vs VECTORIZATION\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Demonstrating why vectorization is crucial for ML:\n",
    "#\n",
    "# import time\n",
    "# \n",
    "# # Create large array\n",
    "# large_arr = np.arange(1000000)\n",
    "# \n",
    "# # Method 1: Python loop (SLOW)\n",
    "# start = time.time()\n",
    "# result_loop = []\n",
    "# for x in large_arr:\n",
    "#     result_loop.append(x ** 2)\n",
    "# loop_time = time.time() - start\n",
    "# print(f\"Loop time: {loop_time:.4f} seconds\")\n",
    "# \n",
    "# # Method 2: List comprehension (STILL SLOW)\n",
    "# start = time.time()\n",
    "# result_comp = [x ** 2 for x in large_arr]\n",
    "# comp_time = time.time() - start\n",
    "# print(f\"Comprehension time: {comp_time:.4f} seconds\")\n",
    "# \n",
    "# # Method 3: NumPy vectorization (FAST!)\n",
    "# start = time.time()\n",
    "# result_vec = large_arr ** 2\n",
    "# vec_time = time.time() - start\n",
    "# print(f\"Vectorization time: {vec_time:.4f} seconds\")\n",
    "# \n",
    "# # Typical results:\n",
    "# # Loop time: 0.2500 seconds\n",
    "# # Comprehension time: 0.1800 seconds\n",
    "# # Vectorization time: 0.0020 seconds  ← 100x faster!\n",
    "#\n",
    "# HINGLISH:\n",
    "# Demonstrate kar rahe hain ki vectorization ML ke liye kyun crucial hai:\n",
    "#\n",
    "# import time\n",
    "# \n",
    "# # Bada array banao\n",
    "# large_arr = np.arange(1000000)\n",
    "# \n",
    "# # Method 1: Python loop (SLOW)\n",
    "# start = time.time()\n",
    "# result_loop = []\n",
    "# for x in large_arr:\n",
    "#     result_loop.append(x ** 2)\n",
    "# loop_time = time.time() - start\n",
    "# print(f\"Loop time: {loop_time:.4f} seconds\")\n",
    "# \n",
    "# # Method 2: List comprehension (ABHI BHI SLOW)\n",
    "# start = time.time()\n",
    "# result_comp = [x ** 2 for x in large_arr]\n",
    "# comp_time = time.time() - start\n",
    "# print(f\"Comprehension time: {comp_time:.4f} seconds\")\n",
    "# \n",
    "# # Method 3: NumPy vectorization (FAST!)\n",
    "# start = time.time()\n",
    "# result_vec = large_arr ** 2\n",
    "# vec_time = time.time() - start\n",
    "# print(f\"Vectorization time: {vec_time:.4f} seconds\")\n",
    "# \n",
    "# # Typical results:\n",
    "# # Loop time: 0.2500 seconds\n",
    "# # Comprehension time: 0.1800 seconds\n",
    "# # Vectorization time: 0.0020 seconds  ← 100x faster!\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# PRACTICAL ML EXAMPLE: GRADIENT DESCENT STEP\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Real-world scenario: One step of gradient descent (vectorized)\n",
    "#\n",
    "# # Hyperparameters\n",
    "# learning_rate = 0.01\n",
    "# \n",
    "# # Model parameters (weights)\n",
    "# weights = np.array([0.5, -0.3, 0.8, 0.2])\n",
    "# \n",
    "# # Computed gradients from backpropagation\n",
    "# gradients = np.array([0.1, -0.05, 0.15, 0.02])\n",
    "# \n",
    "# # Gradient descent update (fully vectorized - no loops!)\n",
    "# weights = weights - learning_rate * gradients\n",
    "# # [0.499, -0.2995, 0.7985, 0.1998]\n",
    "# \n",
    "# # Without vectorization, you'd need:\n",
    "# # for i in range(len(weights)):\n",
    "# #     weights[i] = weights[i] - learning_rate * gradients[i]\n",
    "# # Much slower and less readable!\n",
    "#\n",
    "# HINGLISH:\n",
    "# Real-world scenario: Gradient descent ka ek step (vectorized)\n",
    "#\n",
    "# # Hyperparameters\n",
    "# learning_rate = 0.01\n",
    "# \n",
    "# # Model parameters (weights)\n",
    "# weights = np.array([0.5, -0.3, 0.8, 0.2])\n",
    "# \n",
    "# # Backpropagation se computed gradients\n",
    "# gradients = np.array([0.1, -0.05, 0.15, 0.02])\n",
    "# \n",
    "# # Gradient descent update (fully vectorized - loops nahi!)\n",
    "# weights = weights - learning_rate * gradients\n",
    "# # [0.499, -0.2995, 0.7985, 0.1998]\n",
    "# \n",
    "# # Vectorization ke bina, aapko yeh karna padega:\n",
    "# # for i in range(len(weights)):\n",
    "# #     weights[i] = weights[i] - learning_rate * gradients[i]\n",
    "# # Bahut slower aur kam readable!\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "701cf553-33e4-4c62-b222-d1d826493a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5\n",
      "1.118033988749895\n",
      "[[-1.34164079 -0.4472136 ]\n",
      " [ 0.4472136   1.34164079]]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DATA NORMALIZATION (STANDARDIZATION/Z-SCORE NORMALIZATION)\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# WHAT IS NORMALIZATION?\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Normalization (specifically Z-score normalization/standardization) transforms\n",
    "# data to have:\n",
    "# - Mean = 0 (centered around zero)\n",
    "# - Standard deviation = 1 (unit variance)\n",
    "#\n",
    "# This is one of the MOST IMPORTANT preprocessing steps in ML because:\n",
    "# - Features with different scales can dominate learning (e.g., age vs income)\n",
    "# - Gradient descent converges faster with normalized data\n",
    "# - Many ML algorithms (SVM, Neural Networks, KNN) perform better\n",
    "# - Prevents numerical instability in computations\n",
    "#\n",
    "# Formula: z = (x - μ) / σ\n",
    "# where:\n",
    "# - x = original value\n",
    "# - μ (mu) = mean of the data\n",
    "# - σ (sigma) = standard deviation of the data\n",
    "# - z = normalized value (z-score)\n",
    "#\n",
    "# HINGLISH:\n",
    "# Normalization (specifically Z-score normalization/standardization) data ko\n",
    "# transform karta hai taaki:\n",
    "# - Mean = 0 ho (zero ke around centered)\n",
    "# - Standard deviation = 1 ho (unit variance)\n",
    "#\n",
    "# Yeh ML mein sabse IMPORTANT preprocessing steps mein se ek hai kyunki:\n",
    "# - Alag scales wale features learning ko dominate kar sakte hain (jaise age vs income)\n",
    "# - Normalized data ke saath gradient descent zyada fast converge hota hai\n",
    "# - Bahut saare ML algorithms (SVM, Neural Networks, KNN) better perform karte hain\n",
    "# - Computations mein numerical instability se bachata hai\n",
    "#\n",
    "# Formula: z = (x - μ) / σ\n",
    "# jahan:\n",
    "# - x = original value\n",
    "# - μ (mu) = data ka mean\n",
    "# - σ (sigma) = data ka standard deviation\n",
    "# - z = normalized value (z-score)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# =============================================================================\n",
    "# CREATING SAMPLE DATA FOR NORMALIZATION\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Creates a simple 2×2 matrix for demonstrating normalization\n",
    "# - Values: [1, 2, 3, 4] - small range for easy calculation verification\n",
    "# - Why this example: Simple numbers make it easy to understand the math\n",
    "#   behind normalization\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Normalization demonstrate karne ke liye ek simple\n",
    "#   2×2 matrix banata hai\n",
    "# - Values: [1, 2, 3, 4] - easy calculation verification ke liye small range\n",
    "# - Yeh example kyun: Simple numbers normalization ke peeche ka math samajhne\n",
    "#   mein aasaan banate hain\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "arr = np.array([[1, 2],    # Row 0\n",
    "                [3, 4]])   # Row 1\n",
    "# Shape: (2, 2)\n",
    "# Total elements: 4\n",
    "# Values: 1, 2, 3, 4\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: COMPUTING THE MEAN (AVERAGE)\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Calculates the arithmetic mean (average) of ALL elements\n",
    "#   in the array\n",
    "# - Syntax: np.mean(array, axis=None) - axis=None means compute over all elements\n",
    "# - Formula: mean = (sum of all values) / (count of values)\n",
    "# - Why compute mean: It represents the \"center\" of the data. We subtract\n",
    "#   this to center the data around zero.\n",
    "# - How it works:\n",
    "#   * Sums all elements: 1 + 2 + 3 + 4 = 10\n",
    "#   * Divides by count: 10 / 4 = 2.5\n",
    "#   * Returns single scalar value\n",
    "# - Common use cases:\n",
    "#   * Data centering for normalization\n",
    "#   * Computing average performance metrics\n",
    "#   * Finding typical values in datasets\n",
    "#   * Baseline for variance calculations\n",
    "#   * Feature scaling in preprocessing\n",
    "# - Mean computation options:\n",
    "#   * np.mean(arr) - mean of all elements (what we use here)\n",
    "#   * np.mean(arr, axis=0) - mean per column (feature-wise)\n",
    "#   * np.mean(arr, axis=1) - mean per row (sample-wise)\n",
    "# - Example:\n",
    "#   # Global mean (all elements)\n",
    "#   data = np.array([[10, 20], [30, 40]])\n",
    "#   global_mean = np.mean(data)  # 25.0\n",
    "#   \n",
    "#   # Per-feature mean (column-wise)\n",
    "#   X = np.array([[1, 10, 100],\n",
    "#                 [2, 20, 200],\n",
    "#                 [3, 30, 300]])\n",
    "#   feature_means = np.mean(X, axis=0)  # [2., 20., 200.]\n",
    "#   \n",
    "#   # Per-sample mean (row-wise)\n",
    "#   sample_means = np.mean(X, axis=1)  # [37., 74., 111.]\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Array ke SAARE elements ka arithmetic mean (average)\n",
    "#   calculate karta hai\n",
    "# - Syntax: np.mean(array, axis=None) - axis=None matlab saare elements par compute\n",
    "# - Formula: mean = (saari values ka sum) / (values ki count)\n",
    "# - Mean kyun compute kare: Yeh data ke \"center\" ko represent karta hai. Hum\n",
    "#   ise subtract karte hain taaki data zero ke around center ho jaye.\n",
    "# - Kaise kaam karta hai:\n",
    "#   * Saare elements ko sum karta hai: 1 + 2 + 3 + 4 = 10\n",
    "#   * Count se divide karta hai: 10 / 4 = 2.5\n",
    "#   * Single scalar value return karta hai\n",
    "# - Common use cases:\n",
    "#   * Normalization ke liye data centering\n",
    "#   * Average performance metrics compute karna\n",
    "#   * Datasets mein typical values dhoondhna\n",
    "#   * Variance calculations ke liye baseline\n",
    "#   * Preprocessing mein feature scaling\n",
    "# - Mean computation options:\n",
    "#   * np.mean(arr) - saare elements ka mean (yahan hum yeh use kar rahe hain)\n",
    "#   * np.mean(arr, axis=0) - har column ka mean (feature-wise)\n",
    "#   * np.mean(arr, axis=1) - har row ka mean (sample-wise)\n",
    "# - Example:\n",
    "#   # Global mean (saare elements)\n",
    "#   data = np.array([[10, 20], [30, 40]])\n",
    "#   global_mean = np.mean(data)  # 25.0\n",
    "#   \n",
    "#   # Per-feature mean (column-wise)\n",
    "#   X = np.array([[1, 10, 100],\n",
    "#                 [2, 20, 200],\n",
    "#                 [3, 30, 300]])\n",
    "#   feature_means = np.mean(X, axis=0)  # [2., 20., 200.]\n",
    "#   \n",
    "#   # Per-sample mean (row-wise)\n",
    "#   sample_means = np.mean(X, axis=1)  # [37., 74., 111.]\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "mean = np.mean(arr)\n",
    "# Calculation: (1 + 2 + 3 + 4) / 4 = 10 / 4 = 2.5\n",
    "\n",
    "print(mean)\n",
    "# Output: 2.5\n",
    "# This is the center point of the data\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: COMPUTING STANDARD DEVIATION\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Calculates how spread out the data is from the mean.\n",
    "#   Measures the average distance of each point from the mean.\n",
    "# - Syntax: np.std(array, axis=None, ddof=0)\n",
    "# - Formula: σ = sqrt(mean((x - μ)²))\n",
    "#   Step-by-step:\n",
    "#   1. Subtract mean from each value: (x - μ)\n",
    "#   2. Square each difference: (x - μ)²\n",
    "#   3. Take mean of squared differences: mean((x - μ)²) = variance\n",
    "#   4. Take square root: sqrt(variance) = standard deviation\n",
    "# - Why compute std: It measures the \"scale\" of the data. We divide by this\n",
    "#   to scale the data to unit variance (std = 1).\n",
    "# - ddof parameter (Degrees of Freedom):\n",
    "#   * ddof=0 (default): Population std (divide by N) - use for entire dataset\n",
    "#   * ddof=1: Sample std (divide by N-1) - use for sample statistics\n",
    "#   * For ML normalization, typically use ddof=0\n",
    "# - How it works for our data:\n",
    "#   Step 1: Differences from mean: [1-2.5, 2-2.5, 3-2.5, 4-2.5]\n",
    "#                                 = [-1.5, -0.5, 0.5, 1.5]\n",
    "#   Step 2: Square differences: [2.25, 0.25, 0.25, 2.25]\n",
    "#   Step 3: Mean of squares: (2.25 + 0.25 + 0.25 + 2.25) / 4 = 5.0 / 4 = 1.25\n",
    "#   Step 4: Square root: sqrt(1.25) ≈ 1.118\n",
    "# - Interpretation:\n",
    "#   * Small std (close to 0): Data points are close to mean (little variation)\n",
    "#   * Large std: Data points are spread out (high variation)\n",
    "#   * std = 0: All values are identical\n",
    "# - Common use cases:\n",
    "#   * Measuring data variability/spread\n",
    "#   * Normalizing data to unit variance\n",
    "#   * Detecting outliers (values > 3 std from mean)\n",
    "#   * Comparing variability across features\n",
    "#   * Risk assessment in finance\n",
    "# - Example:\n",
    "#   # Low variability\n",
    "#   low_var = np.array([10, 10.5, 9.5, 10])\n",
    "#   np.std(low_var)  # ≈ 0.35 - values close to mean\n",
    "#   \n",
    "#   # High variability\n",
    "#   high_var = np.array([1, 50, 100, 200])\n",
    "#   np.std(high_var)  # ≈ 74.4 - values spread out\n",
    "#   \n",
    "#   # Per-feature std (important for ML)\n",
    "#   X = np.array([[1, 100],\n",
    "#                 [2, 200],\n",
    "#                 [3, 300]])\n",
    "#   feature_stds = np.std(X, axis=0)  # [0.816, 81.65]\n",
    "#   # Feature 2 has much higher variability\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Calculate karta hai ki data mean se kitna spread out\n",
    "#   hai. Har point ki mean se average distance measure karta hai.\n",
    "# - Syntax: np.std(array, axis=None, ddof=0)\n",
    "# - Formula: σ = sqrt(mean((x - μ)²))\n",
    "#   Step-by-step:\n",
    "#   1. Har value se mean subtract karo: (x - μ)\n",
    "#   2. Har difference ko square karo: (x - μ)²\n",
    "#   3. Squared differences ka mean lo: mean((x - μ)²) = variance\n",
    "#   4. Square root lo: sqrt(variance) = standard deviation\n",
    "# - Std kyun compute kare: Yeh data ke \"scale\" ko measure karta hai. Hum isse\n",
    "#   divide karte hain taaki data unit variance (std = 1) tak scale ho jaye.\n",
    "# - ddof parameter (Degrees of Freedom):\n",
    "#   * ddof=0 (default): Population std (N se divide) - poore dataset ke liye use\n",
    "#   * ddof=1: Sample std (N-1 se divide) - sample statistics ke liye use\n",
    "#   * ML normalization ke liye, typically ddof=0 use karte hain\n",
    "# - Hamare data ke liye kaise kaam karta hai:\n",
    "#   Step 1: Mean se differences: [1-2.5, 2-2.5, 3-2.5, 4-2.5]\n",
    "#                               = [-1.5, -0.5, 0.5, 1.5]\n",
    "#   Step 2: Differences square karo: [2.25, 0.25, 0.25, 2.25]\n",
    "#   Step 3: Squares ka mean: (2.25 + 0.25 + 0.25 + 2.25) / 4 = 5.0 / 4 = 1.25\n",
    "#   Step 4: Square root: sqrt(1.25) ≈ 1.118\n",
    "# - Interpretation:\n",
    "#   * Chhota std (0 ke paas): Data points mean ke paas hain (kam variation)\n",
    "#   * Bada std: Data points spread out hain (zyada variation)\n",
    "#   * std = 0: Saari values same hain\n",
    "# - Common use cases:\n",
    "#   * Data variability/spread measure karna\n",
    "#   * Data ko unit variance tak normalize karna\n",
    "#   * Outliers detect karna (values > mean se 3 std)\n",
    "#   * Features across variability compare karna\n",
    "#   * Finance mein risk assessment\n",
    "# - Example:\n",
    "#   # Kam variability\n",
    "#   low_var = np.array([10, 10.5, 9.5, 10])\n",
    "#   np.std(low_var)  # ≈ 0.35 - values mean ke paas\n",
    "#   \n",
    "#   # Zyada variability\n",
    "#   high_var = np.array([1, 50, 100, 200])\n",
    "#   np.std(high_var)  # ≈ 74.4 - values spread out\n",
    "#   \n",
    "#   # Per-feature std (ML ke liye important)\n",
    "#   X = np.array([[1, 100],\n",
    "#                 [2, 200],\n",
    "#                 [3, 300]])\n",
    "#   feature_stds = np.std(X, axis=0)  # [0.816, 81.65]\n",
    "#   # Feature 2 mein bahut zyada variability hai\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "std_dev = np.std(arr)\n",
    "# Calculation:\n",
    "# Step 1: [1-2.5, 2-2.5, 3-2.5, 4-2.5] = [-1.5, -0.5, 0.5, 1.5]\n",
    "# Step 2: [2.25, 0.25, 0.25, 2.25]\n",
    "# Step 3: (2.25 + 0.25 + 0.25 + 2.25) / 4 = 1.25\n",
    "# Step 4: sqrt(1.25) ≈ 1.118\n",
    "\n",
    "print(std_dev)\n",
    "# Output: 1.118033988749895\n",
    "# This tells us the average distance from the mean\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: NORMALIZATION (Z-SCORE TRANSFORMATION)\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Transforms the data using the formula: z = (x - μ) / σ\n",
    "#   This creates a new dataset where:\n",
    "#   * Mean = 0 (centered)\n",
    "#   * Standard deviation = 1 (unit variance)\n",
    "# - Why this works: By subtracting mean and dividing by std, we:\n",
    "#   1. Center the data around zero (subtract mean)\n",
    "#   2. Scale to unit variance (divide by std)\n",
    "# - Benefits of normalization:\n",
    "#   * All features on same scale (prevents feature dominance)\n",
    "#   * Faster gradient descent convergence\n",
    "#   * Better performance for distance-based algorithms (KNN, K-Means)\n",
    "#   * Prevents numerical instability\n",
    "#   * Required for many ML algorithms (Neural Networks, SVM, PCA)\n",
    "# - Vectorized operation: Uses broadcasting\n",
    "#   * arr - mean: Subtracts scalar from entire array (broadcasting)\n",
    "#   * (arr - mean) / std_dev: Divides entire array by scalar (broadcasting)\n",
    "# - Interpretation of normalized values (z-scores):\n",
    "#   * z = 0: Value is exactly at the mean\n",
    "#   * z = 1: Value is 1 standard deviation above mean\n",
    "#   * z = -1: Value is 1 standard deviation below mean\n",
    "#   * z = 2: Value is 2 standard deviations above mean\n",
    "#   * |z| > 3: Often considered an outlier\n",
    "# - Common use cases:\n",
    "#   * Preprocessing features for ML models\n",
    "#   * Comparing features with different units (age vs salary)\n",
    "#   * Neural network input normalization\n",
    "#   * PCA (Principal Component Analysis)\n",
    "#   * SVM kernel tricks\n",
    "# - When NOT to use:\n",
    "#   * Tree-based models (Random Forest, XGBoost) - they don't need it\n",
    "#   * When preserving original scale is important\n",
    "#   * When data is already on similar scales\n",
    "# - Example:\n",
    "#   # Feature scaling for ML\n",
    "#   # Without normalization - BAD!\n",
    "#   X = np.array([[25, 50000],      # Age, Salary\n",
    "#                 [30, 60000],\n",
    "#                 [35, 80000]])\n",
    "#   # Salary dominates because of large values\n",
    "#   \n",
    "#   # With normalization - GOOD!\n",
    "#   X_normalized = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "#   # Now both features have similar scale\n",
    "#   \n",
    "#   # Neural network input normalization\n",
    "#   images = np.random.rand(1000, 28, 28)  # MNIST-like\n",
    "#   # Pixel values in range [0, 1]\n",
    "#   mean = np.mean(images)\n",
    "#   std = np.std(images)\n",
    "#   images_normalized = (images - mean) / std\n",
    "#   # Now centered around 0 with std=1\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Formula use karke data transform karta hai: z = (x - μ) / σ\n",
    "#   Yeh naya dataset banata hai jahan:\n",
    "#   * Mean = 0 (centered)\n",
    "#   * Standard deviation = 1 (unit variance)\n",
    "# - Yeh kyun kaam karta hai: Mean subtract karke aur std se divide karke, hum:\n",
    "#   1. Data ko zero ke around center karte hain (mean subtract)\n",
    "#   2. Unit variance tak scale karte hain (std se divide)\n",
    "# - Normalization ke benefits:\n",
    "#   * Saare features same scale par (feature dominance se bachata hai)\n",
    "#   * Gradient descent zyada fast converge hota hai\n",
    "#   * Distance-based algorithms ke liye better performance (KNN, K-Means)\n",
    "#   * Numerical instability se bachata hai\n",
    "#   * Bahut saare ML algorithms ke liye zaroori (Neural Networks, SVM, PCA)\n",
    "# - Vectorized operation: Broadcasting use karta hai\n",
    "#   * arr - mean: Poore array se scalar subtract (broadcasting)\n",
    "#   * (arr - mean) / std_dev: Poore array ko scalar se divide (broadcasting)\n",
    "# - Normalized values (z-scores) ka interpretation:\n",
    "#   * z = 0: Value exactly mean par hai\n",
    "#   * z = 1: Value mean se 1 standard deviation upar hai\n",
    "#   * z = -1: Value mean se 1 standard deviation neeche hai\n",
    "#   * z = 2: Value mean se 2 standard deviations upar hai\n",
    "#   * |z| > 3: Aksar outlier consider kiya jata hai\n",
    "# - Common use cases:\n",
    "#   * ML models ke liye features preprocess karna\n",
    "#   * Alag units wale features compare karna (age vs salary)\n",
    "#   * Neural network input normalization\n",
    "#   * PCA (Principal Component Analysis)\n",
    "#   * SVM kernel tricks\n",
    "# - Kab use NAHI karna:\n",
    "#   * Tree-based models (Random Forest, XGBoost) - unhe zaroorat nahi\n",
    "#   * Jab original scale preserve karna important ho\n",
    "#   * Jab data pehle se similar scales par ho\n",
    "# - Example:\n",
    "#   # ML ke liye feature scaling\n",
    "#   # Normalization ke bina - BURA!\n",
    "#   X = np.array([[25, 50000],      # Age, Salary\n",
    "#                 [30, 60000],\n",
    "#                 [35, 80000]])\n",
    "#   # Salary dominate karti hai badi values ki wajah se\n",
    "#   \n",
    "#   # Normalization ke saath - ACCHA!\n",
    "#   X_normalized = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "#   # Ab dono features similar scale par hain\n",
    "#   \n",
    "#   # Neural network input normalization\n",
    "#   images = np.random.rand(1000, 28, 28)  # MNIST-jaisa\n",
    "#   # Pixel values range [0, 1] mein\n",
    "#   mean = np.mean(images)\n",
    "#   std = np.std(images)\n",
    "#   images_normalized = (images - mean) / std\n",
    "#   # Ab 0 ke around centered with std=1\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "normalized_arr = (arr - mean) / std_dev\n",
    "# Calculation for each element:\n",
    "# Element [0,0]: (1 - 2.5) / 1.118 = -1.5 / 1.118 ≈ -1.342\n",
    "# Element [0,1]: (2 - 2.5) / 1.118 = -0.5 / 1.118 ≈ -0.447\n",
    "# Element [1,0]: (3 - 2.5) / 1.118 = 0.5 / 1.118 ≈ 0.447\n",
    "# Element [1,1]: (4 - 2.5) / 1.118 = 1.5 / 1.118 ≈ 1.342\n",
    "\n",
    "print(normalized_arr)\n",
    "# Output:\n",
    "# [[-1.34164079 -0.4472136 ]\n",
    "#  [ 0.4472136   1.34164079]]\n",
    "#\n",
    "# Interpretation:\n",
    "# - Negative values: Below the original mean\n",
    "# - Positive values: Above the original mean\n",
    "# - Values near 0: Close to the original mean\n",
    "# - Magnitude indicates distance from mean in units of std\n",
    "\n",
    "# =============================================================================\n",
    "# VERIFYING NORMALIZATION WORKED\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# After normalization, we can verify it worked correctly:\n",
    "#\n",
    "# # Check mean of normalized data (should be ≈ 0)\n",
    "# normalized_mean = np.mean(normalized_arr)\n",
    "# print(f\"Mean of normalized data: {normalized_mean}\")\n",
    "# # Output: Mean of normalized data: 0.0 (or very close, like 1e-16)\n",
    "#\n",
    "# # Check std of normalized data (should be ≈ 1)\n",
    "# normalized_std = np.std(normalized_arr)\n",
    "# print(f\"Std of normalized data: {normalized_std}\")\n",
    "# # Output: Std of normalized data: 1.0\n",
    "#\n",
    "# This confirms the normalization was successful!\n",
    "#\n",
    "# HINGLISH:\n",
    "# Normalization ke baad, hum verify kar sakte hain ki yeh sahi kaam kiya:\n",
    "#\n",
    "# # Normalized data ka mean check karo (≈ 0 hona chahiye)\n",
    "# normalized_mean = np.mean(normalized_arr)\n",
    "# print(f\"Mean of normalized data: {normalized_mean}\")\n",
    "# # Output: Mean of normalized data: 0.0 (ya bahut paas, jaise 1e-16)\n",
    "#\n",
    "# # Normalized data ka std check karo (≈ 1 hona chahiye)\n",
    "# normalized_std = np.std(normalized_arr)\n",
    "# print(f\"Std of normalized data: {normalized_std}\")\n",
    "# # Output: Std of normalized data: 1.0\n",
    "#\n",
    "# Yeh confirm karta hai ki normalization successful raha!\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE-WISE VS GLOBAL NORMALIZATION\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# IMPORTANT: In ML, we typically normalize EACH FEATURE independently!\n",
    "#\n",
    "# # BAD: Global normalization (what we did above)\n",
    "# X = np.array([[1, 100],\n",
    "#               [2, 200],\n",
    "#               [3, 300]])\n",
    "# global_mean = np.mean(X)  # Mean across ALL values\n",
    "# global_std = np.std(X)\n",
    "# X_global_norm = (X - global_mean) / global_std\n",
    "# # Problem: Treats all features together, doesn't account for different scales\n",
    "#\n",
    "# # GOOD: Feature-wise normalization (correct for ML)\n",
    "# X = np.array([[1, 100],\n",
    "#               [2, 200],\n",
    "#               [3, 300]])\n",
    "# feature_means = np.mean(X, axis=0)  # Mean per feature: [2, 200]\n",
    "# feature_stds = np.std(X, axis=0)    # Std per feature: [0.816, 81.65]\n",
    "# X_feature_norm = (X - feature_means) / feature_stds\n",
    "# # Each feature normalized independently\n",
    "# # [[(-1/0.816), (-100/81.65)],\n",
    "# #  [(0/0.816), (0/81.65)],\n",
    "# #  [(1/0.816), (100/81.65)]]\n",
    "#\n",
    "# Why feature-wise?\n",
    "# - Each feature has its own distribution\n",
    "# - Preserves relationships within each feature\n",
    "# - Standard practice in ML preprocessing\n",
    "#\n",
    "# HINGLISH:\n",
    "# IMPORTANT: ML mein, hum typically HAR FEATURE ko independently normalize karte hain!\n",
    "#\n",
    "# # BURA: Global normalization (jo humne upar kiya)\n",
    "# X = np.array([[1, 100],\n",
    "#               [2, 200],\n",
    "#               [3, 300]])\n",
    "# global_mean = np.mean(X)  # SAARI values ka mean\n",
    "# global_std = np.std(X)\n",
    "# X_global_norm = (X - global_mean) / global_std\n",
    "# # Problem: Saare features ko ek saath treat karta hai, alag scales account nahi karta\n",
    "#\n",
    "# # ACCHA: Feature-wise normalization (ML ke liye sahi)\n",
    "# X = np.array([[1, 100],\n",
    "#               [2, 200],\n",
    "#               [3, 300]])\n",
    "# feature_means = np.mean(X, axis=0)  # Har feature ka mean: [2, 200]\n",
    "# feature_stds = np.std(X, axis=0)    # Har feature ka std: [0.816, 81.65]\n",
    "# X_feature_norm = (X - feature_means) / feature_stds\n",
    "# # Har feature independently normalized\n",
    "# # [[(-1/0.816), (-100/81.65)],\n",
    "# #  [(0/0.816), (0/81.65)],\n",
    "# #  [(1/0.816), (100/81.65)]]\n",
    "#\n",
    "# Feature-wise kyun?\n",
    "# - Har feature ka apna distribution hota hai\n",
    "# - Har feature ke andar relationships preserve karta hai\n",
    "# - ML preprocessing mein standard practice hai\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# OTHER NORMALIZATION TECHNIQUES\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Besides Z-score normalization, there are other techniques:\n",
    "#\n",
    "# 1. MIN-MAX NORMALIZATION (Scaling to [0, 1])\n",
    "#    Formula: x_scaled = (x - min) / (max - min)\n",
    "#    data = np.array([1, 2, 3, 4, 5])\n",
    "#    min_val = np.min(data)\n",
    "#    max_val = np.max(data)\n",
    "#    minmax_scaled = (data - min_val) / (max_val - min_val)\n",
    "#    # Result: [0, 0.25, 0.5, 0.75, 1.0]\n",
    "#    Use when: Need specific range [0,1] or [-1,1]\n",
    "#\n",
    "# 2. ROBUST SCALING (using median and IQR)\n",
    "#    Formula: x_scaled = (x - median) / IQR\n",
    "#    data = np.array([1, 2, 3, 4, 100])  # Has outlier\n",
    "#    median = np.median(data)\n",
    "#    q75, q25 = np.percentile(data, [75, 25])\n",
    "#    iqr = q75 - q25\n",
    "#    robust_scaled = (data - median) / iqr\n",
    "#    Use when: Data has outliers (robust to outliers)\n",
    "#\n",
    "# 3. L2 NORMALIZATION (Unit norm)\n",
    "#    Formula: x_normalized = x / ||x||₂\n",
    "#    vector = np.array([3, 4])\n",
    "#    l2_norm = np.sqrt(np.sum(vector ** 2))  # 5.0\n",
    "#    unit_vector = vector / l2_norm  # [0.6, 0.8]\n",
    "#    Use when: Need unit length vectors (text embeddings, cosine similarity)\n",
    "#\n",
    "# 4. LOG TRANSFORMATION\n",
    "#    Formula: x_transformed = log(x + 1)\n",
    "#    data = np.array([1, 10, 100, 1000])\n",
    "#    log_transformed = np.log1p(data)  # log(x+1) to handle 0\n",
    "#    Use when: Data is heavily skewed (income, populations)\n",
    "#\n",
    "# HINGLISH:\n",
    "# Z-score normalization ke alawa, aur bhi techniques hain:\n",
    "#\n",
    "# 1. MIN-MAX NORMALIZATION ([0, 1] tak scaling)\n",
    "#    Formula: x_scaled = (x - min) / (max - min)\n",
    "#    data = np.array([1, 2, 3, 4, 5])\n",
    "#    min_val = np.min(data)\n",
    "#    max_val = np.max(data)\n",
    "#    minmax_scaled = (data - min_val) / (max_val - min_val)\n",
    "#    # Result: [0, 0.25, 0.5, 0.75, 1.0]\n",
    "#    Kab use karein: Jab specific range [0,1] ya [-1,1] chahiye\n",
    "#\n",
    "# 2. ROBUST SCALING (median aur IQR use karke)\n",
    "#    Formula: x_scaled = (x - median) / IQR\n",
    "#    data = np.array([1, 2, 3, 4, 100])  # Outlier hai\n",
    "#    median = np.median(data)\n",
    "#    q75, q25 = np.percentile(data, [75, 25])\n",
    "#    iqr = q75 - q25\n",
    "#    robust_scaled = (data - median) / iqr\n",
    "#    Kab use karein: Jab data mein outliers hain (outliers ke against robust)\n",
    "#\n",
    "# 3. L2 NORMALIZATION (Unit norm)\n",
    "#    Formula: x_normalized = x / ||x||₂\n",
    "#    vector = np.array([3, 4])\n",
    "#    l2_norm = np.sqrt(np.sum(vector ** 2))  # 5.0\n",
    "#    unit_vector = vector / l2_norm  # [0.6, 0.8]\n",
    "#    Kab use karein: Jab unit length vectors chahiye (text embeddings, cosine similarity)\n",
    "#\n",
    "# 4. LOG TRANSFORMATION\n",
    "#    Formula: x_transformed = log(x + 1)\n",
    "#    data = np.array([1, 10, 100, 1000])\n",
    "#    log_transformed = np.log1p(data)  # log(x+1) taaki 0 handle ho\n",
    "#    Kab use karein: Jab data heavily skewed ho (income, populations)\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# PRACTICAL ML EXAMPLE: COMPLETE PREPROCESSING PIPELINE\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Real-world scenario: Preprocessing a dataset for neural network\n",
    "#\n",
    "# # Simulate dataset: 100 samples, 3 features\n",
    "# np.random.seed(42)\n",
    "# X_train = np.random.randn(100, 3) * [10, 100, 1000] + [50, 500, 5000]\n",
    "# # Features have very different scales!\n",
    "#\n",
    "# # Step 1: Compute statistics on TRAINING data only\n",
    "# train_means = np.mean(X_train, axis=0)\n",
    "# train_stds = np.std(X_train, axis=0)\n",
    "#\n",
    "# # Step 2: Normalize training data\n",
    "# X_train_normalized = (X_train - train_means) / train_stds\n",
    "#\n",
    "# # Step 3: Normalize test data using TRAINING statistics (CRITICAL!)\n",
    "# X_test = np.random.randn(20, 3) * [10, 100, 1000] + [50, 500, 5000]\n",
    "# X_test_normalized = (X_test - train_means) / train_stds\n",
    "# # Use training mean/std, NOT test mean/std!\n",
    "#\n",
    "# # Step 4: Verify normalization\n",
    "# print(\"Training data - Mean:\", np.mean(X_train_normalized, axis=0))\n",
    "# # Should be close to [0, 0, 0]\n",
    "# print(\"Training data - Std:\", np.std(X_train_normalized, axis=0))\n",
    "# # Should be close to [1, 1, 1]\n",
    "#\n",
    "# WHY use training statistics for test data?\n",
    "# - Prevents data leakage\n",
    "# - Ensures same transformation as training\n",
    "# - Models expect same scale as training\n",
    "#\n",
    "# HINGLISH:\n",
    "# Real-world scenario: Neural network ke liye dataset preprocess karna\n",
    "#\n",
    "# # Dataset simulate karo: 100 samples, 3 features\n",
    "# np.random.seed(42)\n",
    "# X_train = np.random.randn(100, 3) * [10, 100, 1000] + [50, 500, 5000]\n",
    "# # Features ki bahut alag scales hain!\n",
    "#\n",
    "# # Step 1: Sirf TRAINING data par statistics compute karo\n",
    "# train_means = np.mean(X_train, axis=0)\n",
    "# train_stds = np.std(X_train, axis=0)\n",
    "#\n",
    "# # Step 2: Training data normalize karo\n",
    "# X_train_normalized = (X_train - train_means) / train_stds\n",
    "#\n",
    "# # Step 3: Test data ko TRAINING statistics use karke normalize karo (CRITICAL!)\n",
    "# X_test = np.random.randn(20, 3) * [10, 100, 1000] + [50, 500, 5000]\n",
    "# X_test_normalized = (X_test - train_means) / train_stds\n",
    "# # Training mean/std use karo, test mean/std NAHI!\n",
    "#\n",
    "# # Step 4: Normalization verify karo\n",
    "# print(\"Training data - Mean:\", np.mean(X_train_normalized, axis=0))\n",
    "# # [0, 0, 0] ke paas hona chahiye\n",
    "# print(\"Training data - Std:\", np.std(X_train_normalized, axis=0))\n",
    "# # [1, 1, 1] ke paas hona chahiye\n",
    "#\n",
    "# Test data ke liye training statistics kyun use karein?\n",
    "# - Data leakage se bachata hai\n",
    "# - Training jaisa hi transformation ensure karta hai\n",
    "# - Models training jaisi hi scale expect karte hain\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "744ee026-0c19-48cc-897d-2837f6a34e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "720\n",
      "1\n",
      "6\n",
      "3.5\n",
      "0\n",
      "5\n",
      "1.707825127659933\n",
      "3.5\n",
      "2.9166666666666665\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# NUMPY MATHEMATICAL FUNCTIONS - AGGREGATION OPERATIONS\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# WHAT ARE AGGREGATION FUNCTIONS?\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Aggregation functions reduce an array to a single value (or fewer values)\n",
    "# by performing calculations across all elements or along specific axes.\n",
    "# These are ESSENTIAL for:\n",
    "# - Computing statistics on datasets\n",
    "# - Loss function calculations in ML\n",
    "# - Performance metrics evaluation\n",
    "# - Data analysis and exploration\n",
    "# - Feature engineering\n",
    "#\n",
    "# HINGLISH:\n",
    "# Aggregation functions ek array ko single value (ya kam values) mein reduce\n",
    "# karte hain saare elements par ya specific axes ke along calculations\n",
    "# perform karke. Yeh zaroori hain:\n",
    "# - Datasets par statistics compute karne ke liye\n",
    "# - ML mein loss function calculations ke liye\n",
    "# - Performance metrics evaluate karne ke liye\n",
    "# - Data analysis aur exploration ke liye\n",
    "# - Feature engineering ke liye\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# =============================================================================\n",
    "# CREATING SAMPLE ARRAY FOR DEMONSTRATIONS\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Creates a simple 1D array with 6 elements for demonstrating\n",
    "#   various mathematical aggregation functions\n",
    "# - Values: [1, 2, 3, 4, 5, 6] - sequential for easy verification\n",
    "# - Why this example: Small, simple numbers make it easy to understand and\n",
    "#   verify each operation's result\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Various mathematical aggregation functions demonstrate\n",
    "#   karne ke liye 6 elements wala simple 1D array banata hai\n",
    "# - Values: [1, 2, 3, 4, 5, 6] - easy verification ke liye sequential\n",
    "# - Yeh example kyun: Chhote, simple numbers har operation ka result samajhne\n",
    "#   aur verify karne mein aasaan banate hain\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "arr = np.array([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCTION 1: np.sum() - SUMMATION\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Computes the sum (total) of all elements in the array\n",
    "# - Syntax: np.sum(array, axis=None)\n",
    "# - Formula: sum = a₁ + a₂ + a₃ + ... + aₙ\n",
    "# - Why use it: Fundamental operation for:\n",
    "#   * Computing total loss across all samples\n",
    "#   * Calculating total counts/amounts\n",
    "#   * L1 regularization: sum(|weights|)\n",
    "#   * Feature aggregation\n",
    "#   * Batch processing totals\n",
    "# - How it works:\n",
    "#   * Adds all elements together\n",
    "#   * Returns single scalar (if axis=None)\n",
    "#   * Can work along specific axes for multi-dimensional arrays\n",
    "# - Common use cases:\n",
    "#   * Total loss: np.sum((y_pred - y_true) ** 2)\n",
    "#   * Count occurrences: np.sum(arr == value)\n",
    "#   * L1 norm: np.sum(np.abs(weights))\n",
    "#   * Probability verification: np.sum(probabilities) should equal 1\n",
    "#   * Batch totals: np.sum(batch_scores, axis=0)\n",
    "# - Example:\n",
    "#   # Counting elements meeting condition\n",
    "#   data = np.array([1, 5, 3, 8, 2, 9])\n",
    "#   count_above_5 = np.sum(data > 5)  # 2 (values 8 and 9)\n",
    "#   \n",
    "#   # Total loss calculation\n",
    "#   predictions = np.array([0.8, 0.6, 0.9])\n",
    "#   targets = np.array([1.0, 0.5, 1.0])\n",
    "#   total_loss = np.sum((predictions - targets) ** 2)  # 0.14\n",
    "#   \n",
    "#   # Summing along axis\n",
    "#   matrix = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "#   col_sums = np.sum(matrix, axis=0)  # [5, 7, 9] - sum each column\n",
    "#   row_sums = np.sum(matrix, axis=1)  # [6, 15] - sum each row\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Array ke saare elements ka sum (total) compute karta hai\n",
    "# - Syntax: np.sum(array, axis=None)\n",
    "# - Formula: sum = a₁ + a₂ + a₃ + ... + aₙ\n",
    "# - Kab use karein: Fundamental operation hai:\n",
    "#   * Saare samples across total loss compute karne ke liye\n",
    "#   * Total counts/amounts calculate karne ke liye\n",
    "#   * L1 regularization: sum(|weights|)\n",
    "#   * Feature aggregation\n",
    "#   * Batch processing totals\n",
    "# - Kaise kaam karta hai:\n",
    "#   * Saare elements ko ek saath add karta hai\n",
    "#   * Single scalar return karta hai (agar axis=None)\n",
    "#   * Multi-dimensional arrays ke liye specific axes ke along kaam kar sakta hai\n",
    "# - Common use cases:\n",
    "#   * Total loss: np.sum((y_pred - y_true) ** 2)\n",
    "#   * Occurrences count karna: np.sum(arr == value)\n",
    "#   * L1 norm: np.sum(np.abs(weights))\n",
    "#   * Probability verification: np.sum(probabilities) 1 ke barabar hona chahiye\n",
    "#   * Batch totals: np.sum(batch_scores, axis=0)\n",
    "# - Example:\n",
    "#   # Condition meet karne wale elements count karna\n",
    "#   data = np.array([1, 5, 3, 8, 2, 9])\n",
    "#   count_above_5 = np.sum(data > 5)  # 2 (values 8 aur 9)\n",
    "#   \n",
    "#   # Total loss calculation\n",
    "#   predictions = np.array([0.8, 0.6, 0.9])\n",
    "#   targets = np.array([1.0, 0.5, 1.0])\n",
    "#   total_loss = np.sum((predictions - targets) ** 2)  # 0.14\n",
    "#   \n",
    "#   # Axis ke along sum karna\n",
    "#   matrix = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "#   col_sums = np.sum(matrix, axis=0)  # [5, 7, 9] - har column ka sum\n",
    "#   row_sums = np.sum(matrix, axis=1)  # [6, 15] - har row ka sum\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(np.sum(arr))\n",
    "# Output: 21\n",
    "# Calculation: 1 + 2 + 3 + 4 + 5 + 6 = 21\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCTION 2: np.prod() - PRODUCT\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Computes the product (multiplication) of all elements\n",
    "# - Syntax: np.prod(array, axis=None)\n",
    "# - Formula: product = a₁ × a₂ × a₃ × ... × aₙ\n",
    "# - Why use it: Less common than sum, but useful for:\n",
    "#   * Calculating probabilities in naive Bayes: P(A∩B) = P(A) × P(B)\n",
    "#   * Computing geometric mean: (prod(values))^(1/n)\n",
    "#   * Factorial calculations: np.prod(np.arange(1, n+1))\n",
    "#   * Dimensionality calculations: total_size = np.prod(shape)\n",
    "#   * Compound growth: final = initial × np.prod(1 + rates)\n",
    "# - Warning: \n",
    "#   * Can overflow quickly with large numbers\n",
    "#   * Returns 0 if any element is 0\n",
    "#   * May cause underflow with many small decimals\n",
    "# - Common use cases:\n",
    "#   * Independent probability: P(all) = P(1) × P(2) × P(3)\n",
    "#   * Array size calculation: np.prod(arr.shape)\n",
    "#   * Factorial: np.prod(range(1, n+1))\n",
    "#   * Geometric mean: np.prod(arr) ** (1/len(arr))\n",
    "#   * Scaling factors: total_scale = np.prod(scale_factors)\n",
    "# - Example:\n",
    "#   # Calculate factorial\n",
    "#   n = 5\n",
    "#   factorial_5 = np.prod(np.arange(1, n+1))  # 120\n",
    "#   \n",
    "#   # Independent probabilities\n",
    "#   probs = np.array([0.9, 0.8, 0.95])  # Success rates\n",
    "#   prob_all_succeed = np.prod(probs)  # 0.684\n",
    "#   \n",
    "#   # Array size from shape\n",
    "#   shape = (10, 20, 30)\n",
    "#   total_elements = np.prod(shape)  # 6000\n",
    "#   \n",
    "#   # Geometric mean\n",
    "#   values = np.array([2, 8, 4])\n",
    "#   geom_mean = np.prod(values) ** (1/len(values))  # 4.0\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Saare elements ka product (multiplication) compute karta hai\n",
    "# - Syntax: np.prod(array, axis=None)\n",
    "# - Formula: product = a₁ × a₂ × a₃ × ... × aₙ\n",
    "# - Kab use karein: Sum se kam common, lekin useful hai:\n",
    "#   * Naive Bayes mein probabilities calculate karne: P(A∩B) = P(A) × P(B)\n",
    "#   * Geometric mean compute karna: (prod(values))^(1/n)\n",
    "#   * Factorial calculations: np.prod(np.arange(1, n+1))\n",
    "#   * Dimensionality calculations: total_size = np.prod(shape)\n",
    "#   * Compound growth: final = initial × np.prod(1 + rates)\n",
    "# - Warning: \n",
    "#   * Bade numbers ke saath jaldi overflow ho sakta hai\n",
    "#   * Agar koi element 0 hai to 0 return karta hai\n",
    "#   * Bahut saare small decimals ke saath underflow ho sakta hai\n",
    "# - Common use cases:\n",
    "#   * Independent probability: P(all) = P(1) × P(2) × P(3)\n",
    "#   * Array size calculation: np.prod(arr.shape)\n",
    "#   * Factorial: np.prod(range(1, n+1))\n",
    "#   * Geometric mean: np.prod(arr) ** (1/len(arr))\n",
    "#   * Scaling factors: total_scale = np.prod(scale_factors)\n",
    "# - Example:\n",
    "#   # Factorial calculate karna\n",
    "#   n = 5\n",
    "#   factorial_5 = np.prod(np.arange(1, n+1))  # 120\n",
    "#   \n",
    "#   # Independent probabilities\n",
    "#   probs = np.array([0.9, 0.8, 0.95])  # Success rates\n",
    "#   prob_all_succeed = np.prod(probs)  # 0.684\n",
    "#   \n",
    "#   # Shape se array size\n",
    "#   shape = (10, 20, 30)\n",
    "#   total_elements = np.prod(shape)  # 6000\n",
    "#   \n",
    "#   # Geometric mean\n",
    "#   values = np.array([2, 8, 4])\n",
    "#   geom_mean = np.prod(values) ** (1/len(values))  # 4.0\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(np.prod(arr))\n",
    "# Output: 720\n",
    "# Calculation: 1 × 2 × 3 × 4 × 5 × 6 = 720\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCTION 3: np.min() - MINIMUM VALUE\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Finds the smallest (minimum) value in the array\n",
    "# - Syntax: np.min(array, axis=None)\n",
    "# - Why use it: Essential for:\n",
    "#   * Finding worst-case scenarios\n",
    "#   * Min-max normalization: (x - min) / (max - min)\n",
    "#   * Detecting lower bounds in data\n",
    "#   * Validation checks (ensuring values above threshold)\n",
    "#   * Clipping operations: np.clip(arr, min_val, max_val)\n",
    "# - How it works:\n",
    "#   * Compares all elements\n",
    "#   * Returns the smallest value found\n",
    "#   * Can work along specific axes\n",
    "# - Common use cases:\n",
    "#   * Min-max scaling: min_val = np.min(data)\n",
    "#   * Finding lowest score/error\n",
    "#   * Data validation: assert np.min(probabilities) >= 0\n",
    "#   * Lower bound detection\n",
    "#   * Finding minimum distance in clustering\n",
    "# - Example:\n",
    "#   # Min-max normalization\n",
    "#   data = np.array([10, 20, 30, 40, 50])\n",
    "#   min_val = np.min(data)  # 10\n",
    "#   max_val = np.max(data)  # 50\n",
    "#   normalized = (data - min_val) / (max_val - min_val)\n",
    "#   # [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "#   \n",
    "#   # Finding worst performance\n",
    "#   scores = np.array([0.85, 0.92, 0.78, 0.95])\n",
    "#   worst_score = np.min(scores)  # 0.78\n",
    "#   \n",
    "#   # Per-feature minimum\n",
    "#   X = np.array([[1, 10], [2, 5], [3, 15]])\n",
    "#   feature_mins = np.min(X, axis=0)  # [1, 5]\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Array mein sabse chhoti (minimum) value dhoondhta hai\n",
    "# - Syntax: np.min(array, axis=None)\n",
    "# - Kab use karein: Zaroori hai:\n",
    "#   * Worst-case scenarios dhoondhne ke liye\n",
    "#   * Min-max normalization: (x - min) / (max - min)\n",
    "#   * Data mein lower bounds detect karne\n",
    "#   * Validation checks (values threshold se upar ensure karna)\n",
    "#   * Clipping operations: np.clip(arr, min_val, max_val)\n",
    "# - Kaise kaam karta hai:\n",
    "#   * Saare elements compare karta hai\n",
    "#   * Sabse chhoti value return karta hai\n",
    "#   * Specific axes ke along kaam kar sakta hai\n",
    "# - Common use cases:\n",
    "#   * Min-max scaling: min_val = np.min(data)\n",
    "#   * Lowest score/error dhoondhna\n",
    "#   * Data validation: assert np.min(probabilities) >= 0\n",
    "#   * Lower bound detection\n",
    "#   * Clustering mein minimum distance dhoondhna\n",
    "# - Example:\n",
    "#   # Min-max normalization\n",
    "#   data = np.array([10, 20, 30, 40, 50])\n",
    "#   min_val = np.min(data)  # 10\n",
    "#   max_val = np.max(data)  # 50\n",
    "#   normalized = (data - min_val) / (max_val - min_val)\n",
    "#   # [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "#   \n",
    "#   # Worst performance dhoondhna\n",
    "#   scores = np.array([0.85, 0.92, 0.78, 0.95])\n",
    "#   worst_score = np.min(scores)  # 0.78\n",
    "#   \n",
    "#   # Har feature ka minimum\n",
    "#   X = np.array([[1, 10], [2, 5], [3, 15]])\n",
    "#   feature_mins = np.min(X, axis=0)  # [1, 5]\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(np.min(arr))\n",
    "# Output: 1\n",
    "# The smallest value in [1, 2, 3, 4, 5, 6] is 1\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCTION 4: np.max() - MAXIMUM VALUE\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Finds the largest (maximum) value in the array\n",
    "# - Syntax: np.max(array, axis=None)\n",
    "# - Why use it: Essential for:\n",
    "#   * Finding best-case scenarios\n",
    "#   * Min-max normalization (upper bound)\n",
    "#   * Detecting upper bounds/outliers\n",
    "#   * Softmax computation: exp(x) / sum(exp(x))\n",
    "#   * Gradient clipping: prevent exploding gradients\n",
    "# - How it works:\n",
    "#   * Compares all elements\n",
    "#   * Returns the largest value found\n",
    "#   * Can work along specific axes\n",
    "# - Common use cases:\n",
    "#   * Min-max scaling: max_val = np.max(data)\n",
    "#   * Finding highest score/accuracy\n",
    "#   * Confidence scores: max_prob = np.max(probabilities)\n",
    "#   * Data range checking\n",
    "#   * Peak detection in signals\n",
    "# - Example:\n",
    "#   # Finding best model\n",
    "#   accuracies = np.array([0.85, 0.92, 0.88, 0.95])\n",
    "#   best_accuracy = np.max(accuracies)  # 0.95\n",
    "#   \n",
    "#   # Softmax normalization (numerical stability)\n",
    "#   logits = np.array([1.0, 2.0, 3.0])\n",
    "#   max_logit = np.max(logits)  # 3.0\n",
    "#   exp_logits = np.exp(logits - max_logit)  # Subtract max for stability\n",
    "#   softmax = exp_logits / np.sum(exp_logits)\n",
    "#   \n",
    "#   # Per-class maximum\n",
    "#   predictions = np.array([[0.7, 0.2, 0.1],\n",
    "#                           [0.1, 0.8, 0.1],\n",
    "#                           [0.2, 0.3, 0.5]])\n",
    "#   max_per_sample = np.max(predictions, axis=1)  # [0.7, 0.8, 0.5]\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Array mein sabse badi (maximum) value dhoondhta hai\n",
    "# - Syntax: np.max(array, axis=None)\n",
    "# - Kab use karein: Zaroori hai:\n",
    "#   * Best-case scenarios dhoondhne ke liye\n",
    "#   * Min-max normalization (upper bound)\n",
    "#   * Upper bounds/outliers detect karne\n",
    "#   * Softmax computation: exp(x) / sum(exp(x))\n",
    "#   * Gradient clipping: exploding gradients se bachna\n",
    "# - Kaise kaam karta hai:\n",
    "#   * Saare elements compare karta hai\n",
    "#   * Sabse badi value return karta hai\n",
    "#   * Specific axes ke along kaam kar sakta hai\n",
    "# - Common use cases:\n",
    "#   * Min-max scaling: max_val = np.max(data)\n",
    "#   * Highest score/accuracy dhoondhna\n",
    "#   * Confidence scores: max_prob = np.max(probabilities)\n",
    "#   * Data range checking\n",
    "#   * Signals mein peak detection\n",
    "# - Example:\n",
    "#   # Best model dhoondhna\n",
    "#   accuracies = np.array([0.85, 0.92, 0.88, 0.95])\n",
    "#   best_accuracy = np.max(accuracies)  # 0.95\n",
    "#   \n",
    "#   # Softmax normalization (numerical stability)\n",
    "#   logits = np.array([1.0, 2.0, 3.0])\n",
    "#   max_logit = np.max(logits)  # 3.0\n",
    "#   exp_logits = np.exp(logits - max_logit)  # Stability ke liye max subtract\n",
    "#   softmax = exp_logits / np.sum(exp_logits)\n",
    "#   \n",
    "#   # Har class ka maximum\n",
    "#   predictions = np.array([[0.7, 0.2, 0.1],\n",
    "#                           [0.1, 0.8, 0.1],\n",
    "#                           [0.2, 0.3, 0.5]])\n",
    "#   max_per_sample = np.max(predictions, axis=1)  # [0.7, 0.8, 0.5]\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(np.max(arr))\n",
    "# Output: 6\n",
    "# The largest value in [1, 2, 3, 4, 5, 6] is 6\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCTION 5: np.mean() - ARITHMETIC MEAN (AVERAGE)\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Computes the arithmetic mean (average) of all elements\n",
    "# - Syntax: np.mean(array, axis=None)\n",
    "# - Formula: mean = (sum of all values) / (count of values)\n",
    "# - Why use it: MOST COMMONLY USED statistic for:\n",
    "#   * Data centering in normalization\n",
    "#   * Computing average performance\n",
    "#   * Expected value calculations\n",
    "#   * Baseline comparisons\n",
    "#   * Loss function aggregation\n",
    "# - How it works:\n",
    "#   * Sums all elements\n",
    "#   * Divides by number of elements\n",
    "#   * Returns average value\n",
    "# - Common use cases:\n",
    "#   * Normalization: (data - mean) / std\n",
    "#   * Average accuracy/loss\n",
    "#   * Expected returns in finance\n",
    "#   * Center of mass calculations\n",
    "#   * Feature statistics\n",
    "# - Example covered in normalization section above\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Saare elements ka arithmetic mean (average) compute karta hai\n",
    "# - Syntax: np.mean(array, axis=None)\n",
    "# - Formula: mean = (saari values ka sum) / (values ki count)\n",
    "# - Kab use karein: Statistics mein SABSE ZYADA use hone wala:\n",
    "#   * Normalization mein data centering\n",
    "#   * Average performance compute karna\n",
    "#   * Expected value calculations\n",
    "#   * Baseline comparisons\n",
    "#   * Loss function aggregation\n",
    "# - Kaise kaam karta hai:\n",
    "#   * Saare elements ko sum karta hai\n",
    "#   * Elements ki sankhya se divide karta hai\n",
    "#   * Average value return karta hai\n",
    "# - Common use cases:\n",
    "#   * Normalization: (data - mean) / std\n",
    "#   * Average accuracy/loss\n",
    "#   * Finance mein expected returns\n",
    "#   * Center of mass calculations\n",
    "#   * Feature statistics\n",
    "# - Example upar normalization section mein covered hai\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(np.mean(arr))\n",
    "# Output: 3.5\n",
    "# Calculation: (1 + 2 + 3 + 4 + 5 + 6) / 6 = 21 / 6 = 3.5\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCTION 6: np.argmin() - INDEX OF MINIMUM VALUE\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Returns the INDEX (position) of the minimum value, not the\n",
    "#   value itself\n",
    "# - Syntax: np.argmin(array, axis=None)\n",
    "# - Why use it: Essential for:\n",
    "#   * Finding which sample/feature has lowest value\n",
    "#   * Selecting best model (lowest loss)\n",
    "#   * Identifying nearest neighbor in KNN\n",
    "#   * Finding optimal hyperparameters\n",
    "#   * Locating minimum distance\n",
    "# - How it works:\n",
    "#   * Finds minimum value\n",
    "#   * Returns its position/index (0-based)\n",
    "#   * If multiple minimums exist, returns first occurrence\n",
    "# - Difference from np.min():\n",
    "#   * np.min() → returns the VALUE (e.g., 1)\n",
    "#   * np.argmin() → returns the INDEX (e.g., 0)\n",
    "# - Common use cases:\n",
    "#   * Best model selection: best_idx = np.argmin(losses)\n",
    "#   * Nearest neighbor: nearest = np.argmin(distances)\n",
    "#   * Optimal hyperparameter: best_lr_idx = np.argmin(validation_losses)\n",
    "#   * Classification: predicted_class = np.argmin(distances_to_centers)\n",
    "#   * Finding minimum along axis: np.argmin(matrix, axis=1)\n",
    "# - Example:\n",
    "#   # Finding best model by loss\n",
    "#   losses = np.array([0.45, 0.32, 0.28, 0.35])\n",
    "#   best_model_idx = np.argmin(losses)  # 2 (third model has lowest loss)\n",
    "#   \n",
    "#   # Nearest neighbor (KNN)\n",
    "#   test_point = np.array([2.5, 3.5])\n",
    "#   train_points = np.array([[1, 1], [2, 3], [4, 5]])\n",
    "#   distances = np.sqrt(np.sum((train_points - test_point)**2, axis=1))\n",
    "#   nearest_idx = np.argmin(distances)  # Index of closest point\n",
    "#   \n",
    "#   # Per-row minimum index\n",
    "#   matrix = np.array([[5, 2, 8],\n",
    "#                      [1, 9, 3],\n",
    "#                      [7, 4, 6]])\n",
    "#   min_indices = np.argmin(matrix, axis=1)  # [1, 0, 1]\n",
    "#   # Row 0: index 1 (value 2), Row 1: index 0 (value 1), Row 2: index 1 (value 4)\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Minimum value ka INDEX (position) return karta hai,\n",
    "#   value khud nahi\n",
    "# - Syntax: np.argmin(array, axis=None)\n",
    "# - Kab use karein: Zaroori hai:\n",
    "#   * Dhoondhne ke liye ki kaun sa sample/feature lowest value rakhta hai\n",
    "#   * Best model select karna (lowest loss)\n",
    "#   * KNN mein nearest neighbor identify karna\n",
    "#   * Optimal hyperparameters dhoondhna\n",
    "#   * Minimum distance locate karna\n",
    "# - Kaise kaam karta hai:\n",
    "#   * Minimum value dhoondhta hai\n",
    "#   * Uska position/index return karta hai (0-based)\n",
    "#   * Agar multiple minimums hain, to pehli occurrence return karta hai\n",
    "# - np.min() se difference:\n",
    "#   * np.min() → VALUE return karta hai (jaise 1)\n",
    "#   * np.argmin() → INDEX return karta hai (jaise 0)\n",
    "# - Common use cases:\n",
    "#   * Best model selection: best_idx = np.argmin(losses)\n",
    "#   * Nearest neighbor: nearest = np.argmin(distances)\n",
    "#   * Optimal hyperparameter: best_lr_idx = np.argmin(validation_losses)\n",
    "#   * Classification: predicted_class = np.argmin(distances_to_centers)\n",
    "#   * Axis ke along minimum: np.argmin(matrix, axis=1)\n",
    "# - Example:\n",
    "#   # Loss se best model dhoondhna\n",
    "#   losses = np.array([0.45, 0.32, 0.28, 0.35])\n",
    "#   best_model_idx = np.argmin(losses)  # 2 (teesre model ka loss sabse kam)\n",
    "#   \n",
    "#   # Nearest neighbor (KNN)\n",
    "#   test_point = np.array([2.5, 3.5])\n",
    "#   train_points = np.array([[1, 1], [2, 3], [4, 5]])\n",
    "#   distances = np.sqrt(np.sum((train_points - test_point)**2, axis=1))\n",
    "#   nearest_idx = np.argmin(distances)  # Sabse paas point ka index\n",
    "#   \n",
    "#   # Har row ka minimum index\n",
    "#   matrix = np.array([[5, 2, 8],\n",
    "#                      [1, 9, 3],\n",
    "#                      [7, 4, 6]])\n",
    "#   min_indices = np.argmin(matrix, axis=1)  # [1, 0, 1]\n",
    "#   # Row 0: index 1 (value 2), Row 1: index 0 (value 1), Row 2: index 1 (value 4)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(np.argmin(arr))\n",
    "# Output: 0\n",
    "# The minimum value (1) is at index 0\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCTION 7: np.argmax() - INDEX OF MAXIMUM VALUE\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Returns the INDEX (position) of the maximum value, not the\n",
    "#   value itself\n",
    "# - Syntax: np.argmax(array, axis=None)\n",
    "# - Why use it: EXTREMELY IMPORTANT in ML for:\n",
    "#   * Classification predictions: predicted_class = np.argmax(probabilities)\n",
    "#   * Finding best performing model/epoch\n",
    "#   * Selecting highest confidence prediction\n",
    "#   * Winner-takes-all mechanisms\n",
    "#   * One-hot encoding to labels\n",
    "# - How it works:\n",
    "#   * Finds maximum value\n",
    "#   * Returns its position/index (0-based)\n",
    "#   * If multiple maximums exist, returns first occurrence\n",
    "# - Difference from np.max():\n",
    "#   * np.max() → returns the VALUE (e.g., 6)\n",
    "#   * np.argmax() → returns the INDEX (e.g., 5)\n",
    "# - Common use cases:\n",
    "#   * Classification: class = np.argmax(predictions, axis=1)\n",
    "#   * Best epoch selection: best_epoch = np.argmax(val_accuracies)\n",
    "#   * Converting softmax to labels\n",
    "#   * Finding most confident prediction\n",
    "#   * Attention mechanism: attend_to = np.argmax(attention_weights)\n",
    "# - Example:\n",
    "#   # Multi-class classification\n",
    "#   predictions = np.array([[0.1, 0.7, 0.2],   # Sample 1\n",
    "#                           [0.8, 0.1, 0.1],   # Sample 2\n",
    "#                           [0.2, 0.3, 0.5]])  # Sample 3\n",
    "#   predicted_classes = np.argmax(predictions, axis=1)\n",
    "#   # [1, 0, 2] - Each sample's predicted class\n",
    "#   \n",
    "#   # Finding best training epoch\n",
    "#   val_accs = np.array([0.75, 0.82, 0.88, 0.85, 0.87])\n",
    "#   best_epoch = np.argmax(val_accs)  # 2 (third epoch has highest accuracy)\n",
    "#   \n",
    "#   # Attention mechanism\n",
    "#   attention_scores = np.array([0.1, 0.6, 0.2, 0.1])\n",
    "#   focus_on = np.argmax(attention_scores)  # 1 - attend to second element\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Maximum value ka INDEX (position) return karta hai,\n",
    "#   value khud nahi\n",
    "# - Syntax: np.argmax(array, axis=None)\n",
    "# - Kab use karein: ML mein BAHUT IMPORTANT:\n",
    "#   * Classification predictions: predicted_class = np.argmax(probabilities)\n",
    "#   * Best performing model/epoch dhoondhna\n",
    "#   * Highest confidence prediction select karna\n",
    "#   * Winner-takes-all mechanisms\n",
    "#   * One-hot encoding ko labels mein convert karna\n",
    "# - Kaise kaam karta hai:\n",
    "#   * Maximum value dhoondhta hai\n",
    "#   * Uska position/index return karta hai (0-based)\n",
    "#   * Agar multiple maximums hain, to pehli occurrence return karta hai\n",
    "# - np.max() se difference:\n",
    "#   * np.max() → VALUE return karta hai (jaise 6)\n",
    "#   * np.argmax() → INDEX return karta hai (jaise 5)\n",
    "# - Common use cases:\n",
    "#   * Classification: class = np.argmax(predictions, axis=1)\n",
    "#   * Best epoch selection: best_epoch = np.argmax(val_accuracies)\n",
    "#   * Softmax ko labels mein convert karna\n",
    "#   * Sabse confident prediction dhoondhna\n",
    "#   * Attention mechanism: attend_to = np.argmax(attention_weights)\n",
    "# - Example:\n",
    "#   # Multi-class classification\n",
    "#   predictions = np.array([[0.1, 0.7, 0.2],   # Sample 1\n",
    "#                           [0.8, 0.1, 0.1],   # Sample 2\n",
    "#                           [0.2, 0.3, 0.5]])  # Sample 3\n",
    "#   predicted_classes = np.argmax(predictions, axis=1)\n",
    "#   # [1, 0, 2] - Har sample ki predicted class\n",
    "#   \n",
    "#   # Best training epoch dhoondhna\n",
    "#   val_accs = np.array([0.75, 0.82, 0.88, 0.85, 0.87])\n",
    "#   best_epoch = np.argmax(val_accs)  # 2 (teesre epoch ka accuracy sabse zyada)\n",
    "#   \n",
    "#   # Attention mechanism\n",
    "#   attention_scores = np.array([0.1, 0.6, 0.2, 0.1])\n",
    "#   focus_on = np.argmax(attention_scores)  # 1 - doosre element par attend\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(np.argmax(arr))\n",
    "# Output: 5\n",
    "# The maximum value (6) is at index 5 (last position)\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCTION 8: np.std() - STANDARD DEVIATION\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - Covered in detail in the normalization section above\n",
    "# - Measures spread/variability of data\n",
    "# - Formula: σ = sqrt(mean((x - μ)²))\n",
    "# - Essential for normalization, outlier detection, and understanding data distribution\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Upar normalization section mein detail mein covered hai\n",
    "# - Data ke spread/variability ko measure karta hai\n",
    "# - Formula: σ = sqrt(mean((x - μ)²))\n",
    "# - Normalization, outlier detection, aur data distribution samajhne ke liye essential\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(np.std(arr))\n",
    "# Output: 1.707825127659933\n",
    "# Calculation:\n",
    "# Step 1: mean = 3.5\n",
    "# Step 2: differences = [-2.5, -1.5, -0.5, 0.5, 1.5, 2.5]\n",
    "# Step 3: squared = [6.25, 2.25, 0.25, 0.25, 2.25, 6.25]\n",
    "# Step 4: variance = 17.5 / 6 ≈ 2.917\n",
    "# Step 5: std = sqrt(2.917) ≈ 1.708\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCTION 9: np.median() - MEDIAN (MIDDLE VALUE)\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Finds the middle value when data is sorted. If even number\n",
    "#   of elements, returns average of two middle values.\n",
    "# - Syntax: np.median(array, axis=None)\n",
    "# - Why use it: Better than mean for data with outliers:\n",
    "#   * ROBUST to outliers (not affected by extreme values)\n",
    "#   * Represents \"typical\" value better when data is skewed\n",
    "#   * Used in robust statistics\n",
    "#   * Better for income, house prices (skewed distributions)\n",
    "# - How it works:\n",
    "#   * Sorts the array\n",
    "#   * If odd length: returns middle element\n",
    "#   * If even length: returns average of two middle elements\n",
    "# - Median vs Mean:\n",
    "#   * Mean: Affected by outliers (e.g., [1,2,3,1000] → mean=251.5)\n",
    "#   * Median: NOT affected by outliers (e.g., [1,2,3,1000] → median=2.5)\n",
    "# - Common use cases:\n",
    "#   * Income statistics (Bill Gates doesn't skew median)\n",
    "#   * House price analysis\n",
    "#   * Robust statistics in presence of outliers\n",
    "#   * Percentile calculations (median = 50th percentile)\n",
    "#   * Data with heavy tails\n",
    "# - Example:\n",
    "#   # With outlier - median is robust\n",
    "#   salaries = np.array([30000, 35000, 40000, 42000, 1000000])\n",
    "#   mean_salary = np.mean(salaries)    # 229,400 (skewed by outlier!)\n",
    "#   median_salary = np.median(salaries)  # 40,000 (more representative)\n",
    "#   \n",
    "#   # Even vs odd length\n",
    "#   odd_arr = np.array([1, 3, 5])\n",
    "#   np.median(odd_arr)  # 3 (middle element)\n",
    "#   \n",
    "#   even_arr = np.array([1, 2, 3, 4])\n",
    "#   np.median(even_arr)  # 2.5 (average of 2 and 3)\n",
    "#   \n",
    "#   # Along axis\n",
    "#   matrix = np.array([[1, 10, 2], [5, 3, 8]])\n",
    "#   col_medians = np.median(matrix, axis=0)  # [3, 6.5, 5]\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Data sort karne par beech ki value dhoondhta hai. Agar\n",
    "#   even number of elements hain, to do beech ki values ka average return karta hai.\n",
    "# - Syntax: np.median(array, axis=None)\n",
    "# - Kab use karein: Outliers wale data ke liye mean se better:\n",
    "#   * Outliers se ROBUST (extreme values affect nahi karte)\n",
    "#   * Jab data skewed ho to \"typical\" value better represent karta hai\n",
    "#   * Robust statistics mein use hota hai\n",
    "#   * Income, house prices ke liye better (skewed distributions)\n",
    "# - Kaise kaam karta hai:\n",
    "#   * Array ko sort karta hai\n",
    "#   * Agar odd length: beech ka element return karta hai\n",
    "#   * Agar even length: do beech ke elements ka average return karta hai\n",
    "# - Median vs Mean:\n",
    "#   * Mean: Outliers se affect hota hai (jaise [1,2,3,1000] → mean=251.5)\n",
    "#   * Median: Outliers se affect NAHI hota (jaise [1,2,3,1000] → median=2.5)\n",
    "# - Common use cases:\n",
    "#   * Income statistics (Bill Gates median ko skew nahi karta)\n",
    "#   * House price analysis\n",
    "#   * Outliers ki presence mein robust statistics\n",
    "#   * Percentile calculations (median = 50th percentile)\n",
    "#   * Heavy tails wala data\n",
    "# - Example:\n",
    "#   # Outlier ke saath - median robust hai\n",
    "#   salaries = np.array([30000, 35000, 40000, 42000, 1000000])\n",
    "#   mean_salary = np.mean(salaries)    # 229,400 (outlier se skewed!)\n",
    "#   median_salary = np.median(salaries)  # 40,000 (zyada representative)\n",
    "#   \n",
    "#   # Even vs odd length\n",
    "#   odd_arr = np.array([1, 3, 5])\n",
    "#   np.median(odd_arr)  # 3 (beech ka element)\n",
    "#   \n",
    "#   even_arr = np.array([1, 2, 3, 4])\n",
    "#   np.median(even_arr)  # 2.5 (2 aur 3 ka average)\n",
    "#   \n",
    "#   # Axis ke along\n",
    "#   matrix = np.array([[1, 10, 2], [5, 3, 8]])\n",
    "#   col_medians = np.median(matrix, axis=0)  # [3, 6.5, 5]\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(np.median(arr))\n",
    "# Output: 3.5\n",
    "# For [1, 2, 3, 4, 5, 6]:\n",
    "# - Even length (6 elements)\n",
    "# - Two middle values: 3 and 4\n",
    "# - Median = (3 + 4) / 2 = 3.5\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCTION 10: np.var() - VARIANCE\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Computes the variance - the average of squared deviations\n",
    "#   from the mean. Variance = (std)²\n",
    "# - Syntax: np.var(array, axis=None, ddof=0)\n",
    "# - Formula: variance = mean((x - μ)²)\n",
    "# - Why use it:\n",
    "#   * Measures how spread out data is (like std, but squared)\n",
    "#   * Used in many statistical formulas\n",
    "#   * Basis for standard deviation: std = sqrt(variance)\n",
    "#   * Feature selection: high variance = informative feature\n",
    "#   * ANOVA (Analysis of Variance)\n",
    "# - Relationship to std:\n",
    "#   * variance = std²\n",
    "#   * std = sqrt(variance)\n",
    "# - ddof parameter (Degrees of Freedom):\n",
    "#   * ddof=0 (default): Population variance (divide by N)\n",
    "#   * ddof=1: Sample variance (divide by N-1)\n",
    "# - Units issue:\n",
    "#   * Variance is in squared units (harder to interpret)\n",
    "#   * Std is in original units (easier to interpret)\n",
    "#   * Both measure same thing (spread)\n",
    "# - Common use cases:\n",
    "#   * ANOVA tests\n",
    "#   * Feature selection (remove low-variance features)\n",
    "#   * Calculating standard deviation\n",
    "#   * Statistical hypothesis testing\n",
    "#   * Quality control (process variation)\n",
    "# - Example:\n",
    "#   # Feature selection - remove low variance features\n",
    "#   X = np.array([[1, 10, 100],\n",
    "#                 [1, 20, 200],\n",
    "#                 [1, 30, 300],\n",
    "#                 [1, 40, 400]])\n",
    "#   feature_vars = np.var(X, axis=0)  # [0, 125, 12500]\n",
    "#   # Feature 0 has zero variance (constant) - can be removed!\n",
    "#   \n",
    "#   # Relationship to std\n",
    "#   data = np.array([2, 4, 6, 8])\n",
    "#   variance = np.var(data)  # 5.0\n",
    "#   std = np.std(data)       # 2.236... = sqrt(5.0)\n",
    "#   # Verify: std² = variance\n",
    "#   np.std(data) ** 2  # 5.0 ✓\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Variance compute karta hai - mean se squared deviations\n",
    "#   ka average. Variance = (std)²\n",
    "# - Syntax: np.var(array, axis=None, ddof=0)\n",
    "# - Formula: variance = mean((x - μ)²)\n",
    "# - Kab use karein:\n",
    "#   * Data kitna spread out hai measure karna (std jaisa, lekin squared)\n",
    "#   * Bahut saare statistical formulas mein use hota hai\n",
    "#   * Standard deviation ki basis: std = sqrt(variance)\n",
    "#   * Feature selection: high variance = informative feature\n",
    "#   * ANOVA (Analysis of Variance)\n",
    "# - Std ke saath relationship:\n",
    "#   * variance = std²\n",
    "#   * std = sqrt(variance)\n",
    "# - ddof parameter (Degrees of Freedom):\n",
    "#   * ddof=0 (default): Population variance (N se divide)\n",
    "#   * ddof=1: Sample variance (N-1 se divide)\n",
    "# - Units issue:\n",
    "#   * Variance squared units mein hai (interpret karna mushkil)\n",
    "#   * Std original units mein hai (interpret karna aasaan)\n",
    "#   * Dono same cheez measure karte hain (spread)\n",
    "# - Common use cases:\n",
    "#   * ANOVA tests\n",
    "#   * Feature selection (low-variance features remove karna)\n",
    "#   * Standard deviation calculate karna\n",
    "#   * Statistical hypothesis testing\n",
    "#   * Quality control (process variation)\n",
    "# - Example:\n",
    "#   # Feature selection - low variance features remove karna\n",
    "#   X = np.array([[1, 10, 100],\n",
    "#                 [1, 20, 200],\n",
    "#                 [1, 30, 300],\n",
    "#                 [1, 40, 400]])\n",
    "#   feature_vars = np.var(X, axis=0)  # [0, 125, 12500]\n",
    "#   # Feature 0 ka zero variance hai (constant) - remove kar sakte hain!\n",
    "#   \n",
    "#   # Std ke saath relationship\n",
    "#   data = np.array([2, 4, 6, 8])\n",
    "#   variance = np.var(data)  # 5.0\n",
    "#   std = np.std(data)       # 2.236... = sqrt(5.0)\n",
    "#   # Verify: std² = variance\n",
    "#   np.std(data) ** 2  # 5.0 ✓\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(np.var(arr))\n",
    "# Output: 2.9166666666666665\n",
    "# Calculation:\n",
    "# Step 1: mean = 3.5\n",
    "# Step 2: differences = [-2.5, -1.5, -0.5, 0.5, 1.5, 2.5]\n",
    "# Step 3: squared = [6.25, 2.25, 0.25, 0.25, 2.25, 6.25]\n",
    "# Step 4: variance = (6.25 + 2.25 + 0.25 + 0.25 + 2.25 + 6.25) / 6\n",
    "#                  = 17.5 / 6 ≈ 2.917\n",
    "# Note: std² = 1.708² ≈ 2.917 ✓\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY TABLE: ALL FUNCTIONS AT A GLANCE\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Function    | Returns        | Use Case\n",
    "# ------------|----------------|------------------------------------------\n",
    "# np.sum()    | Sum            | Total, counting, L1 norm\n",
    "# np.prod()   | Product        | Probabilities, factorial, geometric mean\n",
    "# np.min()    | Min value      | Lower bound, worst case, min-max scaling\n",
    "# np.max()    | Max value      | Upper bound, best case, min-max scaling\n",
    "# np.mean()   | Average        | Normalization, expected value, baseline\n",
    "# np.argmin() | Min index      | Best model (low loss), nearest neighbor\n",
    "# np.argmax() | Max index      | Classification, best epoch, attention\n",
    "# np.std()    | Std deviation  | Normalization, variability, outliers\n",
    "# np.median() | Middle value   | Robust statistics, skewed data\n",
    "# np.var()    | Variance       | Spread measure, feature selection, ANOVA\n",
    "#\n",
    "# HINGLISH:\n",
    "# Function    | Returns        | Use Case\n",
    "# ------------|----------------|------------------------------------------\n",
    "# np.sum()    | Sum            | Total, counting, L1 norm\n",
    "# np.prod()   | Product        | Probabilities, factorial, geometric mean\n",
    "# np.min()    | Min value      | Lower bound, worst case, min-max scaling\n",
    "# np.max()    | Max value      | Upper bound, best case, min-max scaling\n",
    "# np.mean()   | Average        | Normalization, expected value, baseline\n",
    "# np.argmin() | Min index      | Best model (kam loss), nearest neighbor\n",
    "# np.argmax() | Max index      | Classification, best epoch, attention\n",
    "# np.std()    | Std deviation  | Normalization, variability, outliers\n",
    "# np.median() | Beech ki value | Robust statistics, skewed data\n",
    "# np.var()    | Variance       | Spread measure, feature selection, ANOVA\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6683bdb9-17a0-4442-8064-1deb45e9927d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  4  9 16 25 36]\n",
      "[1.         1.41421356 1.73205081 2.         2.23606798 2.44948974]\n",
      "[ 1  4  9 16 25 36]\n",
      "[  1   8  27  64 125 216]\n",
      "[  2.71828183   7.3890561   20.08553692  54.59815003 148.4131591\n",
      " 403.42879349]\n",
      "[0.         0.69314718 1.09861229 1.38629436 1.60943791 1.79175947]\n",
      "[0.         0.30103    0.47712125 0.60205999 0.69897    0.77815125]\n",
      "[0.         1.         1.5849625  2.         2.32192809 2.5849625 ]\n",
      "[1.         1.25992105 1.44224957 1.58740105 1.70997595 1.81712059]\n",
      "[1 2 3 4 5 6]\n",
      "[-1 -1  1 -1  1 -1]\n",
      "np.ceil(): [2. 3. 4. 5.]\n",
      "np.floor(): [1. 2. 3. 4.]\n",
      "np.round(): [1. 2. 4. 4.]\n",
      "np.clip(): [2 2 3 4 5 5]\n",
      "np.reciprocal(): [1.         0.5        0.33333333 0.25       0.2        0.16666667]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# NUMPY POWER AND MATHEMATICAL FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# WHAT ARE POWER FUNCTIONS?\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Power functions perform mathematical operations involving exponents, roots,\n",
    "# and logarithms. These are FUNDAMENTAL in ML for:\n",
    "# - Activation functions (sigmoid, tanh, ReLU variants)\n",
    "# - Loss functions (log loss, cross-entropy)\n",
    "# - Feature transformations (polynomial features, log transforms)\n",
    "# - Gradient calculations (exponentials in backprop)\n",
    "# - Probability distributions (softmax uses exp)\n",
    "#\n",
    "# HINGLISH:\n",
    "# Power functions mathematical operations perform karte hain jo exponents,\n",
    "# roots, aur logarithms se related hain. ML mein yeh FUNDAMENTAL hain:\n",
    "# - Activation functions (sigmoid, tanh, ReLU variants)\n",
    "# - Loss functions (log loss, cross-entropy)\n",
    "# - Feature transformations (polynomial features, log transforms)\n",
    "# - Gradient calculations (backprop mein exponentials)\n",
    "# - Probability distributions (softmax exp use karta hai)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "arr = np.array([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCTION 1: np.square() - ELEMENT-WISE SQUARING\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Squares each element (raises to power 2). Equivalent to\n",
    "#   arr ** 2 but more explicit and readable.\n",
    "# - Syntax: np.square(array)\n",
    "# - Formula: square(x) = x²\n",
    "# - Why use it: Common in ML for:\n",
    "#   * Mean Squared Error (MSE): np.mean(np.square(y_pred - y_true))\n",
    "#   * L2 regularization: lambda * np.sum(np.square(weights))\n",
    "#   * Euclidean distance: np.sqrt(np.sum(np.square(x - y)))\n",
    "#   * Variance calculation: np.mean(np.square(x - mean))\n",
    "#   * Polynomial features: creating x² features\n",
    "# - Performance: Slightly faster than ** 2 (optimized operation)\n",
    "# - Common use cases:\n",
    "#   * MSE loss: mse = np.mean(np.square(predictions - targets))\n",
    "#   * L2 norm squared: l2_squared = np.sum(np.square(vector))\n",
    "#   * Distance calculations: dist² = np.sum(np.square(p1 - p2))\n",
    "#   * Ridge regression penalty: penalty = alpha * np.sum(np.square(weights))\n",
    "#   * Creating polynomial features for regression\n",
    "# - Example:\n",
    "#   # Mean Squared Error\n",
    "#   y_true = np.array([3.0, 5.0, 2.0, 7.0])\n",
    "#   y_pred = np.array([2.5, 5.5, 2.0, 6.8])\n",
    "#   mse = np.mean(np.square(y_pred - y_true))\n",
    "#   # [(0.5)², (0.5)², (0)², (0.2)²] → mean → 0.135\n",
    "#   \n",
    "#   # L2 regularization term\n",
    "#   weights = np.array([0.5, -0.3, 0.8, -0.2])\n",
    "#   l2_penalty = 0.01 * np.sum(np.square(weights))\n",
    "#   # 0.01 * (0.25 + 0.09 + 0.64 + 0.04) = 0.0102\n",
    "#   \n",
    "#   # Euclidean distance\n",
    "#   point1 = np.array([1, 2, 3])\n",
    "#   point2 = np.array([4, 6, 8])\n",
    "#   distance = np.sqrt(np.sum(np.square(point1 - point2)))\n",
    "#   # sqrt((3)² + (4)² + (5)²) = sqrt(50) ≈ 7.07\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Har element ko square karta hai (power 2 tak raise).\n",
    "#   arr ** 2 ke equivalent hai lekin zyada explicit aur readable.\n",
    "# - Syntax: np.square(array)\n",
    "# - Formula: square(x) = x²\n",
    "# - Kab use karein: ML mein common hai:\n",
    "#   * Mean Squared Error (MSE): np.mean(np.square(y_pred - y_true))\n",
    "#   * L2 regularization: lambda * np.sum(np.square(weights))\n",
    "#   * Euclidean distance: np.sqrt(np.sum(np.square(x - y)))\n",
    "#   * Variance calculation: np.mean(np.square(x - mean))\n",
    "#   * Polynomial features: x² features banana\n",
    "# - Performance: ** 2 se thoda faster (optimized operation)\n",
    "# - Common use cases:\n",
    "#   * MSE loss: mse = np.mean(np.square(predictions - targets))\n",
    "#   * L2 norm squared: l2_squared = np.sum(np.square(vector))\n",
    "#   * Distance calculations: dist² = np.sum(np.square(p1 - p2))\n",
    "#   * Ridge regression penalty: penalty = alpha * np.sum(np.square(weights))\n",
    "#   * Regression ke liye polynomial features banana\n",
    "# - Example:\n",
    "#   # Mean Squared Error\n",
    "#   y_true = np.array([3.0, 5.0, 2.0, 7.0])\n",
    "#   y_pred = np.array([2.5, 5.5, 2.0, 6.8])\n",
    "#   mse = np.mean(np.square(y_pred - y_true))\n",
    "#   # [(0.5)², (0.5)², (0)², (0.2)²] → mean → 0.135\n",
    "#   \n",
    "#   # L2 regularization term\n",
    "#   weights = np.array([0.5, -0.3, 0.8, -0.2])\n",
    "#   l2_penalty = 0.01 * np.sum(np.square(weights))\n",
    "#   # 0.01 * (0.25 + 0.09 + 0.64 + 0.04) = 0.0102\n",
    "#   \n",
    "#   # Euclidean distance\n",
    "#   point1 = np.array([1, 2, 3])\n",
    "#   point2 = np.array([4, 6, 8])\n",
    "#   distance = np.sqrt(np.sum(np.square(point1 - point2)))\n",
    "#   # sqrt((3)² + (4)² + (5)²) = sqrt(50) ≈ 7.07\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(np.square(arr))\n",
    "# Output: [ 1  4  9 16 25 36]\n",
    "# Calculation: [1², 2², 3², 4², 5², 6²]\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCTION 2: np.sqrt() - SQUARE ROOT\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Computes the square root of each element. Equivalent to\n",
    "#   arr ** 0.5 but more readable and optimized.\n",
    "# - Syntax: np.sqrt(array)\n",
    "# - Formula: sqrt(x) = √x = x^(1/2)\n",
    "# - Why use it: Essential for:\n",
    "#   * Euclidean distance: dist = np.sqrt(np.sum((x - y)**2))\n",
    "#   * Standard deviation: std = np.sqrt(variance)\n",
    "#   * RMSE (Root Mean Squared Error): np.sqrt(mse)\n",
    "#   * L2 norm: np.sqrt(np.sum(x**2))\n",
    "#   * Normalizing vectors to unit length\n",
    "# - Important notes:\n",
    "#   * Returns NaN for negative numbers (use np.sqrt(np.abs(x)) if needed)\n",
    "#   * Always returns float, even for perfect squares\n",
    "#   * Element-wise operation (vectorized)\n",
    "# - Common use cases:\n",
    "#   * RMSE: rmse = np.sqrt(np.mean((y_pred - y_true)**2))\n",
    "#   * Euclidean norm: norm = np.sqrt(np.sum(vector**2))\n",
    "#   * Standard deviation from variance: std = np.sqrt(var)\n",
    "#   * Normalizing embeddings: normalized = vec / np.sqrt(np.sum(vec**2))\n",
    "#   * Distance metrics in clustering/KNN\n",
    "# - Example:\n",
    "#   # Root Mean Squared Error\n",
    "#   errors = np.array([0.5, -0.3, 0.2, -0.1])\n",
    "#   mse = np.mean(np.square(errors))  # 0.0975\n",
    "#   rmse = np.sqrt(mse)  # 0.312\n",
    "#   \n",
    "#   # L2 normalization (unit vector)\n",
    "#   vector = np.array([3, 4])\n",
    "#   norm = np.sqrt(np.sum(np.square(vector)))  # 5.0\n",
    "#   unit_vector = vector / norm  # [0.6, 0.8]\n",
    "#   \n",
    "#   # Standard deviation from variance\n",
    "#   data = np.array([2, 4, 6, 8])\n",
    "#   variance = np.var(data)  # 5.0\n",
    "#   std = np.sqrt(variance)  # 2.236...\n",
    "#   # Verify:\n",
    "#   np.std(data)  # 2.236... ✓\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Har element ka square root compute karta hai.\n",
    "#   arr ** 0.5 ke equivalent hai lekin zyada readable aur optimized.\n",
    "# - Syntax: np.sqrt(array)\n",
    "# - Formula: sqrt(x) = √x = x^(1/2)\n",
    "# - Kab use karein: Zaroori hai:\n",
    "#   * Euclidean distance: dist = np.sqrt(np.sum((x - y)**2))\n",
    "#   * Standard deviation: std = np.sqrt(variance)\n",
    "#   * RMSE (Root Mean Squared Error): np.sqrt(mse)\n",
    "#   * L2 norm: np.sqrt(np.sum(x**2))\n",
    "#   * Vectors ko unit length tak normalize karna\n",
    "# - Important notes:\n",
    "#   * Negative numbers ke liye NaN return karta hai (zaroorat ho to np.sqrt(np.abs(x)))\n",
    "#   * Hamesha float return karta hai, perfect squares ke liye bhi\n",
    "#   * Element-wise operation (vectorized)\n",
    "# - Common use cases:\n",
    "#   * RMSE: rmse = np.sqrt(np.mean((y_pred - y_true)**2))\n",
    "#   * Euclidean norm: norm = np.sqrt(np.sum(vector**2))\n",
    "#   * Variance se standard deviation: std = np.sqrt(var)\n",
    "#   * Embeddings normalize karna: normalized = vec / np.sqrt(np.sum(vec**2))\n",
    "#   * Clustering/KNN mein distance metrics\n",
    "# - Example:\n",
    "#   # Root Mean Squared Error\n",
    "#   errors = np.array([0.5, -0.3, 0.2, -0.1])\n",
    "#   mse = np.mean(np.square(errors))  # 0.0975\n",
    "#   rmse = np.sqrt(mse)  # 0.312\n",
    "#   \n",
    "#   # L2 normalization (unit vector)\n",
    "#   vector = np.array([3, 4])\n",
    "#   norm = np.sqrt(np.sum(np.square(vector)))  # 5.0\n",
    "#   unit_vector = vector / norm  # [0.6, 0.8]\n",
    "#   \n",
    "#   # Variance se standard deviation\n",
    "#   data = np.array([2, 4, 6, 8])\n",
    "#   variance = np.var(data)  # 5.0\n",
    "#   std = np.sqrt(variance)  # 2.236...\n",
    "#   # Verify:\n",
    "#   np.std(data)  # 2.236... ✓\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(np.sqrt(arr))\n",
    "# Output: [1.         1.41421356 1.73205081 2.         2.23606798 2.44948975]\n",
    "# Calculation: [√1, √2, √3, √4, √5, √6]\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCTION 3: np.power() - ELEMENT-WISE POWER\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Raises first array elements to powers specified in second\n",
    "#   array (or scalar). More flexible than ** operator.\n",
    "# - Syntax: np.power(base, exponent)\n",
    "# - Formula: power(x, n) = x^n\n",
    "# - Why use it: Flexible exponentiation for:\n",
    "#   * Polynomial features: x, x², x³, x⁴\n",
    "#   * Custom transformations\n",
    "#   * Element-wise different powers\n",
    "#   * Box-Cox transformations\n",
    "# - Advantages over **:\n",
    "#   * Can take two arrays (element-wise powers)\n",
    "#   * More explicit in code\n",
    "#   * Better for broadcasting scenarios\n",
    "# - Common use cases:\n",
    "#   * Polynomial features: np.power(X, [1, 2, 3])\n",
    "#   * Custom scaling: np.power(data, 0.5)  # Same as sqrt\n",
    "#   * Box-Cox transform: (x^λ - 1) / λ\n",
    "#   * Nonlinear transformations\n",
    "# - Example:\n",
    "#   # Polynomial feature creation\n",
    "#   X = np.array([2, 3, 4])\n",
    "#   X_poly = np.column_stack([\n",
    "#       np.power(X, 1),  # [2, 3, 4]\n",
    "#       np.power(X, 2),  # [4, 9, 16]\n",
    "#       np.power(X, 3)   # [8, 27, 64]\n",
    "#   ])\n",
    "#   \n",
    "#   # Element-wise different powers\n",
    "#   bases = np.array([2, 3, 4, 5])\n",
    "#   exponents = np.array([1, 2, 3, 4])\n",
    "#   results = np.power(bases, exponents)\n",
    "#   # [2^1, 3^2, 4^3, 5^4] = [2, 9, 64, 625]\n",
    "#   \n",
    "#   # Fractional powers (roots)\n",
    "#   data = np.array([4, 9, 16, 25])\n",
    "#   square_roots = np.power(data, 0.5)  # [2, 3, 4, 5]\n",
    "#   cube_roots = np.power(data, 1/3)    # [1.587, 2.080, 2.520, 2.924]\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Pehle array ke elements ko doosre array (ya scalar)\n",
    "#   mein specify kiye powers tak raise karta hai. ** operator se zyada flexible.\n",
    "# - Syntax: np.power(base, exponent)\n",
    "# - Formula: power(x, n) = x^n\n",
    "# - Kab use karein: Flexible exponentiation ke liye:\n",
    "#   * Polynomial features: x, x², x³, x⁴\n",
    "#   * Custom transformations\n",
    "#   * Element-wise alag powers\n",
    "#   * Box-Cox transformations\n",
    "# - ** se advantages:\n",
    "#   * Do arrays le sakta hai (element-wise powers)\n",
    "#   * Code mein zyada explicit\n",
    "#   * Broadcasting scenarios ke liye better\n",
    "# - Common use cases:\n",
    "#   * Polynomial features: np.power(X, [1, 2, 3])\n",
    "#   * Custom scaling: np.power(data, 0.5)  # sqrt jaisa\n",
    "#   * Box-Cox transform: (x^λ - 1) / λ\n",
    "#   * Nonlinear transformations\n",
    "# - Example:\n",
    "#   # Polynomial feature creation\n",
    "#   X = np.array([2, 3, 4])\n",
    "#   X_poly = np.column_stack([\n",
    "#       np.power(X, 1),  # [2, 3, 4]\n",
    "#       np.power(X, 2),  # [4, 9, 16]\n",
    "#       np.power(X, 3)   # [8, 27, 64]\n",
    "#   ])\n",
    "#   \n",
    "#   # Element-wise alag powers\n",
    "#   bases = np.array([2, 3, 4, 5])\n",
    "#   exponents = np.array([1, 2, 3, 4])\n",
    "#   results = np.power(bases, exponents)\n",
    "#   # [2^1, 3^2, 4^3, 5^4] = [2, 9, 64, 625]\n",
    "#   \n",
    "#   # Fractional powers (roots)\n",
    "#   data = np.array([4, 9, 16, 25])\n",
    "#   square_roots = np.power(data, 0.5)  # [2, 3, 4, 5]\n",
    "#   cube_roots = np.power(data, 1/3)    # [1.587, 2.080, 2.520, 2.924]\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(np.power(arr, 2))\n",
    "# Output: [ 1  4  9 16 25 36]\n",
    "# Same as np.square(arr)\n",
    "\n",
    "print(np.power(arr, 3))\n",
    "# Output: [  1   8  27  64 125 216]\n",
    "# Cubes: [1³, 2³, 3³, 4³, 5³, 6³]\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCTION 4: np.exp() - EXPONENTIAL (e^x)\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Computes e^x (Euler's number raised to power x) for each\n",
    "#   element. e ≈ 2.71828...\n",
    "# - Syntax: np.exp(array)\n",
    "# - Formula: exp(x) = e^x\n",
    "# - Why use it: EXTREMELY IMPORTANT in ML for:\n",
    "#   * Sigmoid activation: σ(x) = 1 / (1 + e^(-x))\n",
    "#   * Softmax function: exp(x) / sum(exp(x))\n",
    "#   * Log-likelihood calculations\n",
    "#   * Exponential distributions\n",
    "#   * Gradient calculations in backpropagation\n",
    "# - Warning: Can overflow for large values (e^100 is huge!)\n",
    "# - Numerical stability trick: For softmax, subtract max before exp\n",
    "# - Common use cases:\n",
    "#   * Sigmoid: 1 / (1 + np.exp(-x))\n",
    "#   * Softmax: np.exp(x) / np.sum(np.exp(x))\n",
    "#   * Log probability to probability: np.exp(log_prob)\n",
    "#   * Exponential decay: value * np.exp(-decay_rate * time)\n",
    "#   * Gaussian distributions: np.exp(-(x-μ)²/(2σ²))\n",
    "# - Example:\n",
    "#   # Sigmoid activation function\n",
    "#   z = np.array([-2, -1, 0, 1, 2])\n",
    "#   sigmoid = 1 / (1 + np.exp(-z))\n",
    "#   # [0.119, 0.269, 0.5, 0.731, 0.881]\n",
    "#   \n",
    "#   # Softmax (with numerical stability)\n",
    "#   logits = np.array([1.0, 2.0, 3.0])\n",
    "#   max_logit = np.max(logits)\n",
    "#   exp_logits = np.exp(logits - max_logit)  # Subtract max for stability\n",
    "#   softmax = exp_logits / np.sum(exp_logits)\n",
    "#   # [0.090, 0.245, 0.665]\n",
    "#   \n",
    "#   # Converting log probabilities\n",
    "#   log_probs = np.array([-0.5, -1.0, -1.5])\n",
    "#   probs = np.exp(log_probs)\n",
    "#   # [0.607, 0.368, 0.223]\n",
    "#   \n",
    "#   # Exponential decay (learning rate scheduling)\n",
    "#   initial_lr = 0.1\n",
    "#   decay_rate = 0.05\n",
    "#   epochs = np.arange(10)\n",
    "#   lr_schedule = initial_lr * np.exp(-decay_rate * epochs)\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Har element ke liye e^x compute karta hai (Euler's\n",
    "#   number ko power x tak raise). e ≈ 2.71828...\n",
    "# - Syntax: np.exp(array)\n",
    "# - Formula: exp(x) = e^x\n",
    "# - Kab use karein: ML mein BAHUT IMPORTANT:\n",
    "#   * Sigmoid activation: σ(x) = 1 / (1 + e^(-x))\n",
    "#   * Softmax function: exp(x) / sum(exp(x))\n",
    "#   * Log-likelihood calculations\n",
    "#   * Exponential distributions\n",
    "#   * Backpropagation mein gradient calculations\n",
    "# - Warning: Badi values ke liye overflow ho sakta hai (e^100 bahut bada!)\n",
    "# - Numerical stability trick: Softmax ke liye exp se pehle max subtract karo\n",
    "# - Common use cases:\n",
    "#   * Sigmoid: 1 / (1 + np.exp(-x))\n",
    "#   * Softmax: np.exp(x) / np.sum(np.exp(x))\n",
    "#   * Log probability ko probability: np.exp(log_prob)\n",
    "#   * Exponential decay: value * np.exp(-decay_rate * time)\n",
    "#   * Gaussian distributions: np.exp(-(x-μ)²/(2σ²))\n",
    "# - Example:\n",
    "#   # Sigmoid activation function\n",
    "#   z = np.array([-2, -1, 0, 1, 2])\n",
    "#   sigmoid = 1 / (1 + np.exp(-z))\n",
    "#   # [0.119, 0.269, 0.5, 0.731, 0.881]\n",
    "#   \n",
    "#   # Softmax (numerical stability ke saath)\n",
    "#   logits = np.array([1.0, 2.0, 3.0])\n",
    "#   max_logit = np.max(logits)\n",
    "#   exp_logits = np.exp(logits - max_logit)  # Stability ke liye max subtract\n",
    "#   softmax = exp_logits / np.sum(exp_logits)\n",
    "#   # [0.090, 0.245, 0.665]\n",
    "#   \n",
    "#   # Log probabilities convert karna\n",
    "#   log_probs = np.array([-0.5, -1.0, -1.5])\n",
    "#   probs = np.exp(log_probs)\n",
    "#   # [0.607, 0.368, 0.223]\n",
    "#   \n",
    "#   # Exponential decay (learning rate scheduling)\n",
    "#   initial_lr = 0.1\n",
    "#   decay_rate = 0.05\n",
    "#   epochs = np.arange(10)\n",
    "#   lr_schedule = initial_lr * np.exp(-decay_rate * epochs)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(np.exp(arr))\n",
    "# Output: [  2.71828183   7.3890561   20.08553692  54.59815003 148.4131591  403.42879349]\n",
    "# Calculation: [e^1, e^2, e^3, e^4, e^5, e^6]\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCTION 5: np.log() - NATURAL LOGARITHM (ln)\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Computes natural logarithm (base e) of each element.\n",
    "#   Inverse of np.exp()\n",
    "# - Syntax: np.log(array)\n",
    "# - Formula: log(x) = ln(x) = log_e(x)\n",
    "# - Why use it: Critical in ML for:\n",
    "#   * Cross-entropy loss: -sum(y * log(p))\n",
    "#   * Log-likelihood in probabilistic models\n",
    "#   * Log transformations for skewed data\n",
    "#   * Information theory (entropy, KL divergence)\n",
    "#   * Numerical stability (log-sum-exp trick)\n",
    "# - Important notes:\n",
    "#   * Only defined for positive numbers (x > 0)\n",
    "#   * Returns -inf for 0, NaN for negative numbers\n",
    "#   * Converts multiplication to addition: log(ab) = log(a) + log(b)\n",
    "# - Common use cases:\n",
    "#   * Binary cross-entropy: -y*log(p) - (1-y)*log(1-p)\n",
    "#   * Log-likelihood: sum(log(probabilities))\n",
    "#   * Log transform skewed features: np.log(income + 1)\n",
    "#   * KL divergence: sum(p * log(p/q))\n",
    "#   * Numerical stability in products of small numbers\n",
    "# - Example:\n",
    "#   # Binary cross-entropy loss\n",
    "#   y_true = np.array([1, 0, 1, 1])\n",
    "#   y_pred = np.array([0.9, 0.1, 0.8, 0.7])\n",
    "#   bce = -np.mean(y_true * np.log(y_pred) + (1-y_true) * np.log(1-y_pred))\n",
    "#   # 0.178\n",
    "#   \n",
    "#   # Log transformation for skewed data\n",
    "#   income = np.array([30000, 50000, 100000, 500000, 1000000])\n",
    "#   log_income = np.log(income)  # Reduces skewness\n",
    "#   \n",
    "#   # Converting probabilities to log space\n",
    "#   probs = np.array([0.5, 0.3, 0.2])\n",
    "#   log_probs = np.log(probs)\n",
    "#   # Product in original space = sum in log space\n",
    "#   product = np.prod(probs)  # 0.03\n",
    "#   log_sum = np.sum(log_probs)  # -3.507\n",
    "#   np.exp(log_sum)  # 0.03 ✓\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Har element ka natural logarithm (base e) compute\n",
    "#   karta hai. np.exp() ka inverse.\n",
    "# - Syntax: np.log(array)\n",
    "# - Formula: log(x) = ln(x) = log_e(x)\n",
    "# - Kab use karein: ML mein critical:\n",
    "#   * Cross-entropy loss: -sum(y * log(p))\n",
    "#   * Probabilistic models mein log-likelihood\n",
    "#   * Skewed data ke liye log transformations\n",
    "#   * Information theory (entropy, KL divergence)\n",
    "#   * Numerical stability (log-sum-exp trick)\n",
    "# - Important notes:\n",
    "#   * Sirf positive numbers ke liye defined (x > 0)\n",
    "#   * 0 ke liye -inf return karta hai, negative numbers ke liye NaN\n",
    "#   * Multiplication ko addition mein convert: log(ab) = log(a) + log(b)\n",
    "# - Common use cases:\n",
    "#   * Binary cross-entropy: -y*log(p) - (1-y)*log(1-p)\n",
    "#   * Log-likelihood: sum(log(probabilities))\n",
    "#   * Skewed features ko log transform: np.log(income + 1)\n",
    "#   * KL divergence: sum(p * log(p/q))\n",
    "#   * Chote numbers ke products mein numerical stability\n",
    "# - Example:\n",
    "#   # Binary cross-entropy loss\n",
    "#   y_true = np.array([1, 0, 1, 1])\n",
    "#   y_pred = np.array([0.9, 0.1, 0.8, 0.7])\n",
    "#   bce = -np.mean(y_true * np.log(y_pred) + (1-y_true) * np.log(1-y_pred))\n",
    "#   # 0.178\n",
    "#   \n",
    "#   # Skewed data ke liye log transformation\n",
    "#   income = np.array([30000, 50000, 100000, 500000, 1000000])\n",
    "#   log_income = np.log(income)  # Skewness kam karta hai\n",
    "#   \n",
    "#   # Probabilities ko log space mein convert karna\n",
    "#   probs = np.array([0.5, 0.3, 0.2])\n",
    "#   log_probs = np.log(probs)\n",
    "#   # Original space mein product = log space mein sum\n",
    "#   product = np.prod(probs)  # 0.03\n",
    "#   log_sum = np.sum(log_probs)  # -3.507\n",
    "#   np.exp(log_sum)  # 0.03 ✓\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(np.log(arr))\n",
    "# Output: [0.         0.69314718 1.09861229 1.38629436 1.60943791 1.79175947]\n",
    "# Calculation: [ln(1), ln(2), ln(3), ln(4), ln(5), ln(6)]\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCTION 6: np.log10() - BASE-10 LOGARITHM\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Computes logarithm base 10 of each element\n",
    "# - Syntax: np.log10(array)\n",
    "# - Formula: log10(x) = log₁₀(x)\n",
    "# - Why use it: Useful for:\n",
    "#   * Measuring orders of magnitude\n",
    "#   * Scientific data (pH, decibels, Richter scale)\n",
    "#   * Converting to log scale for visualization\n",
    "#   * Power law relationships\n",
    "# - Relationship to ln: log10(x) = ln(x) / ln(10)\n",
    "# - Common use cases:\n",
    "#   * pH calculations\n",
    "#   * Decibel scale (sound intensity)\n",
    "#   * Earthquake magnitude\n",
    "#   * Log-scale plots\n",
    "# - Example:\n",
    "#   # Orders of magnitude\n",
    "#   values = np.array([1, 10, 100, 1000, 10000])\n",
    "#   log10_vals = np.log10(values)  # [0, 1, 2, 3, 4]\n",
    "#   \n",
    "#   # Converting between natural and base-10 log\n",
    "#   x = 100\n",
    "#   ln_x = np.log(x)      # 4.605\n",
    "#   log10_x = np.log10(x) # 2.0\n",
    "#   # Verify: ln(x) = log10(x) * ln(10)\n",
    "#   log10_x * np.log(10)  # 4.605 ✓\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Har element ka base 10 logarithm compute karta hai\n",
    "# - Syntax: np.log10(array)\n",
    "# - Formula: log10(x) = log₁₀(x)\n",
    "# - Kab use karein: Useful hai:\n",
    "#   * Orders of magnitude measure karne ke liye\n",
    "#   * Scientific data (pH, decibels, Richter scale)\n",
    "#   * Visualization ke liye log scale mein convert karna\n",
    "#   * Power law relationships\n",
    "# - ln ke saath relationship: log10(x) = ln(x) / ln(10)\n",
    "# - Common use cases:\n",
    "#   * pH calculations\n",
    "#   * Decibel scale (sound intensity)\n",
    "#   * Earthquake magnitude\n",
    "#   * Log-scale plots\n",
    "# - Example:\n",
    "#   # Orders of magnitude\n",
    "#   values = np.array([1, 10, 100, 1000, 10000])\n",
    "#   log10_vals = np.log10(values)  # [0, 1, 2, 3, 4]\n",
    "#   \n",
    "#   # Natural aur base-10 log ke beech convert karna\n",
    "#   x = 100\n",
    "#   ln_x = np.log(x)      # 4.605\n",
    "#   log10_x = np.log10(x) # 2.0\n",
    "#   # Verify: ln(x) = log10(x) * ln(10)\n",
    "#   log10_x * np.log(10)  # 4.605 ✓\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(np.log10(arr))\n",
    "# Output: [0.         0.30103    0.47712125 0.60205999 0.69897    0.77815125]\n",
    "# Calculation: [log₁₀(1), log₁₀(2), log₁₀(3), log₁₀(4), log₁₀(5), log₁₀(6)]\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCTION 7: np.log2() - BASE-2 LOGARITHM\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Computes logarithm base 2 of each element\n",
    "# - Syntax: np.log2(array)\n",
    "# - Formula: log2(x) = log₂(x)\n",
    "# - Why use it: Important in:\n",
    "#   * Information theory (entropy, bits)\n",
    "#   * Computer science (binary search complexity)\n",
    "#   * Image processing (bit depth)\n",
    "#   * Binary trees (depth calculations)\n",
    "# - Common use cases:\n",
    "#   * Entropy: -sum(p * log2(p))\n",
    "#   * Bit requirements: log2(n) bits to represent n values\n",
    "#   * Binary search complexity: O(log₂(n))\n",
    "# - Example:\n",
    "#   # Information entropy\n",
    "#   probs = np.array([0.5, 0.25, 0.25])\n",
    "#   entropy = -np.sum(probs * np.log2(probs))  # 1.5 bits\n",
    "#   \n",
    "#   # Bits needed to represent values\n",
    "#   n_classes = 256\n",
    "#   bits_needed = np.log2(n_classes)  # 8 bits\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Har element ka base 2 logarithm compute karta hai\n",
    "# - Syntax: np.log2(array)\n",
    "# - Formula: log2(x) = log₂(x)\n",
    "# - Kab use karein: Important hai:\n",
    "#   * Information theory (entropy, bits)\n",
    "#   * Computer science (binary search complexity)\n",
    "#   * Image processing (bit depth)\n",
    "#   * Binary trees (depth calculations)\n",
    "# - Common use cases:\n",
    "#   * Entropy: -sum(p * log2(p))\n",
    "#   * Bit requirements: log2(n) bits n values represent karne ke liye\n",
    "#   * Binary search complexity: O(log₂(n))\n",
    "# - Example:\n",
    "#   # Information entropy\n",
    "#   probs = np.array([0.5, 0.25, 0.25])\n",
    "#   entropy = -np.sum(probs * np.log2(probs))  # 1.5 bits\n",
    "#   \n",
    "#   # Values represent karne ke liye kitne bits chahiye\n",
    "#   n_classes = 256\n",
    "#   bits_needed = np.log2(n_classes)  # 8 bits\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(np.log2(arr))\n",
    "# Output: [0.        1.        1.5849625 2.        2.32192809 2.5849625 ]\n",
    "# Calculation: [log₂(1), log₂(2), log₂(3), log₂(4), log₂(5), log₂(6)]\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCTION 8: np.cbrt() - CUBE ROOT\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Computes cube root (∛x) of each element. Equivalent to\n",
    "#   x^(1/3) but handles negative numbers correctly.\n",
    "# - Syntax: np.cbrt(array)\n",
    "# - Formula: cbrt(x) = ∛x = x^(1/3)\n",
    "# - Advantage over power: Works with negative numbers\n",
    "#   * np.power(-8, 1/3) → complex number\n",
    "#   * np.cbrt(-8) → -2 (correct!)\n",
    "# - Common use cases:\n",
    "#   * Volume calculations\n",
    "#   * Statistical transformations\n",
    "#   * Feature engineering\n",
    "# - Example:\n",
    "#   # Cube roots\n",
    "#   cubes = np.array([1, 8, 27, 64, 125])\n",
    "#   roots = np.cbrt(cubes)  # [1, 2, 3, 4, 5]\n",
    "#   \n",
    "#   # Handles negatives\n",
    "#   np.cbrt(-8)  # -2.0 ✓\n",
    "#   np.power(-8, 1/3)  # NaN or complex ✗\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Har element ka cube root (∛x) compute karta hai.\n",
    "#   x^(1/3) ke equivalent hai lekin negative numbers sahi handle karta hai.\n",
    "# - Syntax: np.cbrt(array)\n",
    "# - Formula: cbrt(x) = ∛x = x^(1/3)\n",
    "# - Power se advantage: Negative numbers ke saath kaam karta hai\n",
    "#   * np.power(-8, 1/3) → complex number\n",
    "#   * np.cbrt(-8) → -2 (sahi!)\n",
    "# - Common use cases:\n",
    "#   * Volume calculations\n",
    "#   * Statistical transformations\n",
    "#   * Feature engineering\n",
    "# - Example:\n",
    "#   # Cube roots\n",
    "#   cubes = np.array([1, 8, 27, 64, 125])\n",
    "#   roots = np.cbrt(cubes)  # [1, 2, 3, 4, 5]\n",
    "#   \n",
    "#   # Negatives handle karta hai\n",
    "#   np.cbrt(-8)  # -2.0 ✓\n",
    "#   np.power(-8, 1/3)  # NaN ya complex ✗\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(np.cbrt(arr))\n",
    "# Output: [1.         1.25992105 1.44224957 1.58740105 1.70997595 1.81712059]\n",
    "# Calculation: [∛1, ∛2, ∛3, ∛4, ∛5, ∛6]\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCTION 9: np.abs() / np.absolute() - ABSOLUTE VALUE\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Computes absolute value (magnitude) of each element.\n",
    "#   Removes negative sign.\n",
    "# - Syntax: np.abs(array) or np.absolute(array)\n",
    "# - Formula: abs(x) = |x|\n",
    "# - Why use it: Essential for:\n",
    "#   * L1 regularization: sum(|weights|)\n",
    "#   * Mean Absolute Error: mean(|y_pred - y_true|)\n",
    "#   * Distance calculations\n",
    "#   * Removing sign information\n",
    "# - Works with complex numbers: returns magnitude\n",
    "# - Common use cases:\n",
    "#   * MAE loss: np.mean(np.abs(predictions - targets))\n",
    "#   * L1 norm: np.sum(np.abs(vector))\n",
    "#   * Lasso regression: alpha * np.sum(np.abs(weights))\n",
    "#   * Gradient clipping by value: np.clip(grads, -max_val, max_val)\n",
    "# - Example:\n",
    "#   # Mean Absolute Error\n",
    "#   y_true = np.array([3, 5, 2, 7])\n",
    "#   y_pred = np.array([2.5, 5.5, 2, 6.8])\n",
    "#   mae = np.mean(np.abs(y_pred - y_true))  # 0.3\n",
    "#   \n",
    "#   # L1 regularization\n",
    "#   weights = np.array([0.5, -0.3, 0.8, -0.2])\n",
    "#   l1_penalty = 0.01 * np.sum(np.abs(weights))  # 0.018\n",
    "#   \n",
    "#   # Handling errors (always positive)\n",
    "#   errors = np.array([0.5, -0.3, 0.2, -0.1])\n",
    "#   absolute_errors = np.abs(errors)  # [0.5, 0.3, 0.2, 0.1]\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Har element ka absolute value (magnitude) compute\n",
    "#   karta hai. Negative sign hata deta hai.\n",
    "# - Syntax: np.abs(array) ya np.absolute(array)\n",
    "# - Formula: abs(x) = |x|\n",
    "# - Kab use karein: Zaroori hai:\n",
    "#   * L1 regularization: sum(|weights|)\n",
    "#   * Mean Absolute Error: mean(|y_pred - y_true|)\n",
    "#   * Distance calculations\n",
    "#   * Sign information remove karna\n",
    "# - Complex numbers ke saath kaam karta hai: magnitude return karta hai\n",
    "# - Common use cases:\n",
    "#   * MAE loss: np.mean(np.abs(predictions - targets))\n",
    "#   * L1 norm: np.sum(np.abs(vector))\n",
    "#   * Lasso regression: alpha * np.sum(np.abs(weights))\n",
    "#   * Gradient clipping by value: np.clip(grads, -max_val, max_val)\n",
    "# - Example:\n",
    "#   # Mean Absolute Error\n",
    "#   y_true = np.array([3, 5, 2, 7])\n",
    "#   y_pred = np.array([2.5, 5.5, 2, 6.8])\n",
    "#   mae = np.mean(np.abs(y_pred - y_true))  # 0.3\n",
    "#   \n",
    "#   # L1 regularization\n",
    "#   weights = np.array([0.5, -0.3, 0.8, -0.2])\n",
    "#   l1_penalty = 0.01 * np.sum(np.abs(weights))  # 0.018\n",
    "#   \n",
    "#   # Errors handle karna (hamesha positive)\n",
    "#   errors = np.array([0.5, -0.3, 0.2, -0.1])\n",
    "#   absolute_errors = np.abs(errors)  # [0.5, 0.3, 0.2, 0.1]\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "negative_arr = np.array([-1, -2, 3, -4, 5, -6])\n",
    "print(np.abs(negative_arr))\n",
    "# Output: [1 2 3 4 5 6]\n",
    "# All negative values become positive\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCTION 10: np.sign() - SIGN FUNCTION\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Returns the sign of each element: -1, 0, or +1\n",
    "# - Syntax: np.sign(array)\n",
    "# - Formula: sign(x) = -1 if x<0, 0 if x=0, +1 if x>0\n",
    "# - Why use it: Useful for:\n",
    "#   * Direction information (ignoring magnitude)\n",
    "#   * Binary encoding\n",
    "#   * Gradient sign (sign descent)\n",
    "#   * Activation functions (sign activation)\n",
    "# - Common use cases:\n",
    "#   * Sign gradient descent: weights -= lr * np.sign(gradients)\n",
    "#   * Binary classification helper\n",
    "#   * Direction vectors\n",
    "# - Example:\n",
    "#   mixed = np.array([-5, -2, 0, 3, 7])\n",
    "#   signs = np.sign(mixed)  # [-1, -1, 0, 1, 1]\n",
    "#   \n",
    "#   # Sign gradient descent (robust to outliers)\n",
    "#   gradients = np.array([0.1, -5.0, 0.3, -0.05])\n",
    "#   weight_update = 0.01 * np.sign(gradients)\n",
    "#   # [0.01, -0.01, 0.01, -0.01] - all same magnitude!\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Har element ka sign return karta hai: -1, 0, ya +1\n",
    "# - Syntax: np.sign(array)\n",
    "# - Formula: sign(x) = -1 agar x<0, 0 agar x=0, +1 agar x>0\n",
    "# - Kab use karein: Useful hai:\n",
    "#   * Direction information (magnitude ignore karke)\n",
    "#   * Binary encoding\n",
    "#   * Gradient sign (sign descent)\n",
    "#   * Activation functions (sign activation)\n",
    "# - Common use cases:\n",
    "#   * Sign gradient descent: weights -= lr * np.sign(gradients)\n",
    "#   * Binary classification helper\n",
    "#   * Direction vectors\n",
    "# - Example:\n",
    "#   mixed = np.array([-5, -2, 0, 3, 7])\n",
    "#   signs = np.sign(mixed)  # [-1, -1, 0, 1, 1]\n",
    "#   \n",
    "#   # Sign gradient descent (outliers ke against robust)\n",
    "#   gradients = np.array([0.1, -5.0, 0.3, -0.05])\n",
    "#   weight_update = 0.01 * np.sign(gradients)\n",
    "#   # [0.01, -0.01, 0.01, -0.01] - sab same magnitude!\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(np.sign(negative_arr))\n",
    "# Output: [-1 -1  1 -1  1 -1]\n",
    "# Negative → -1, Positive → +1, Zero → 0\n",
    "\n",
    "# =============================================================================\n",
    "# BONUS: ADDITIONAL USEFUL MATHEMATICAL FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "# FUNCTION 11: np.ceil() - CEILING (ROUND UP)\n",
    "print(\"np.ceil():\", np.ceil(np.array([1.2, 2.5, 3.8, 4.1])))\n",
    "# Output: [2. 3. 4. 5.]\n",
    "# Always rounds UP to nearest integer\n",
    "\n",
    "# FUNCTION 12: np.floor() - FLOOR (ROUND DOWN)\n",
    "print(\"np.floor():\", np.floor(np.array([1.2, 2.5, 3.8, 4.1])))\n",
    "# Output: [1. 2. 3. 4.]\n",
    "# Always rounds DOWN to nearest integer\n",
    "\n",
    "# FUNCTION 13: np.round() - ROUNDING\n",
    "print(\"np.round():\", np.round(np.array([1.2, 2.5, 3.8, 4.1])))\n",
    "# Output: [1. 2. 4. 4.]\n",
    "# Rounds to nearest integer (0.5 rounds to nearest even)\n",
    "\n",
    "# FUNCTION 14: np.clip() - CLIPPING VALUES\n",
    "print(\"np.clip():\", np.clip(arr, 2, 5))\n",
    "# Output: [2 2 3 4 5 5]\n",
    "# Clips values to [min, max] range\n",
    "# Values < 2 become 2, values > 5 become 5\n",
    "\n",
    "# FUNCTION 15: np.reciprocal() - RECIPROCAL (1/x)\n",
    "print(\"np.reciprocal():\", np.reciprocal(arr.astype(float)))\n",
    "# Output: [1.  0.5 0.33333333 0.25 0.2 0.16666667]\n",
    "# Computes 1/x for each element\n",
    "\n",
    "# =============================================================================\n",
    "# PRACTICAL ML EXAMPLE: COMPLETE ACTIVATION FUNCTION LIBRARY\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Real-world scenario: Implementing common neural network activation functions\n",
    "#\n",
    "# def sigmoid(x):\n",
    "#     \"\"\"Sigmoid: σ(x) = 1 / (1 + e^(-x))\"\"\"\n",
    "#     return 1 / (1 + np.exp(-x))\n",
    "#\n",
    "# def tanh(x):\n",
    "#     \"\"\"Hyperbolic tangent (NumPy has np.tanh built-in)\"\"\"\n",
    "#     return np.tanh(x)\n",
    "#\n",
    "# def relu(x):\n",
    "#     \"\"\"ReLU: max(0, x)\"\"\"\n",
    "#     return np.maximum(0, x)\n",
    "#\n",
    "# def leaky_relu(x, alpha=0.01):\n",
    "#     \"\"\"Leaky ReLU: max(alpha*x, x)\"\"\"\n",
    "#     return np.where(x > 0, x, alpha * x)\n",
    "#\n",
    "# def softmax(x):\n",
    "#     \"\"\"Softmax with numerical stability\"\"\"\n",
    "#     exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "#     return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "#\n",
    "# def swish(x):\n",
    "#     \"\"\"Swish: x * sigmoid(x)\"\"\"\n",
    "#     return x * sigmoid(x)\n",
    "#\n",
    "# # Test\n",
    "# x = np.array([-2, -1, 0, 1, 2])\n",
    "# print(\"Sigmoid:\", sigmoid(x))\n",
    "# print(\"ReLU:\", relu(x))\n",
    "# print(\"Softmax:\", softmax(x))\n",
    "#\n",
    "# HINGLISH:\n",
    "# Real-world scenario: Neural network activation functions implement karna\n",
    "#\n",
    "# (Same code as English)\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f68fba7f-c807-4618-b785-a91fb86514a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.69314718 1.09861229 1.38629436 1.60943791 1.79175947]\n",
      "[  2.71828183   7.3890561   20.08553692  54.59815003 148.4131591\n",
      " 403.42879349]\n",
      "exp(log(x)) = [  2.   5.  10. 100.]\n",
      "log(exp(x)) = [1. 2. 3. 4.]\n",
      "Binary Cross-Entropy Loss: 0.2027\n",
      "Categorical Cross-Entropy Loss: 0.3635\n",
      "Softmax (stable):\n",
      "[[0.65900114 0.24243297 0.09856589]\n",
      " [0.66524096 0.24472847 0.09003057]]\n",
      "Sums: [1. 1.]\n",
      "Sigmoid: [4.53978687e-05 1.19202922e-01 5.00000000e-01 8.80797078e-01\n",
      " 9.99954602e-01]\n",
      "log1p (better for small x): [0.0009995  0.00995033 0.09531018]\n",
      "expm1 (better for small x): [0.0010005  0.01005017 0.10517092]\n",
      "Log add exp: [-1.02592302 -0.02592302]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# NUMPY LOGARITHMIC AND EXPONENTIAL FUNCTIONS - DEEP DIVE\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# WHY LOGARITHMS AND EXPONENTIALS ARE CRITICAL IN ML\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Logarithms (log) and exponentials (exp) are INVERSE operations and form\n",
    "# the mathematical foundation of modern machine learning:\n",
    "#\n",
    "# KEY RELATIONSHIPS:\n",
    "# - exp(log(x)) = x  (they cancel each other out)\n",
    "# - log(exp(x)) = x  (inverse relationship)\n",
    "# - log(a × b) = log(a) + log(b)  (converts multiplication to addition)\n",
    "# - log(a / b) = log(a) - log(b)  (converts division to subtraction)\n",
    "# - log(a^n) = n × log(a)  (brings exponents down)\n",
    "#\n",
    "# WHY THEY MATTER IN ML:\n",
    "# 1. Numerical Stability: Working in log space prevents overflow/underflow\n",
    "# 2. Loss Functions: Cross-entropy uses log for stable gradient computation\n",
    "# 3. Probability: Converting products to sums (log-likelihood)\n",
    "# 4. Activation Functions: Sigmoid and softmax use exp\n",
    "# 5. Optimization: Many algorithms work better in log space\n",
    "#\n",
    "# HINGLISH:\n",
    "# Logarithms (log) aur exponentials (exp) INVERSE operations hain aur modern\n",
    "# machine learning ki mathematical foundation banate hain:\n",
    "#\n",
    "# KEY RELATIONSHIPS:\n",
    "# - exp(log(x)) = x  (ek doosre ko cancel kar dete hain)\n",
    "# - log(exp(x)) = x  (inverse relationship)\n",
    "# - log(a × b) = log(a) + log(b)  (multiplication ko addition mein convert)\n",
    "# - log(a / b) = log(a) - log(b)  (division ko subtraction mein convert)\n",
    "# - log(a^n) = n × log(a)  (exponents ko neeche laata hai)\n",
    "#\n",
    "# ML MEIN KYUN IMPORTANT HAIN:\n",
    "# 1. Numerical Stability: Log space mein kaam karne se overflow/underflow nahi hota\n",
    "# 2. Loss Functions: Cross-entropy stable gradient computation ke liye log use karta hai\n",
    "# 3. Probability: Products ko sums mein convert karna (log-likelihood)\n",
    "# 4. Activation Functions: Sigmoid aur softmax exp use karte hain\n",
    "# 5. Optimization: Bahut saare algorithms log space mein better kaam karte hain\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "arr = np.array([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCTION: np.log() - NATURAL LOGARITHM (DETAILED)\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Computes the natural logarithm (base e, where e ≈ 2.71828)\n",
    "#   of each element. This is the INVERSE of np.exp()\n",
    "# - Syntax: np.log(array)\n",
    "# - Mathematical notation: ln(x) or log_e(x)\n",
    "# - Formula: If y = e^x, then x = log(y)\n",
    "# - Domain: Only defined for POSITIVE numbers (x > 0)\n",
    "#   * log(0) = -∞ (negative infinity)\n",
    "#   * log(negative) = NaN (not a number)\n",
    "#   * log(1) = 0 (important property!)\n",
    "#   * log(e) = 1 (by definition)\n",
    "# - Why use it: EXTREMELY important in ML for:\n",
    "#   * Cross-entropy loss (classification)\n",
    "#   * Log-likelihood (probabilistic models)\n",
    "#   * Numerical stability (log-space computations)\n",
    "#   * Information theory (entropy, KL divergence)\n",
    "#   * Feature transformations (reducing skewness)\n",
    "#   * Converting multiplications to additions\n",
    "# - Key properties:\n",
    "#   * Monotonic increasing: if x₁ < x₂, then log(x₁) < log(x₂)\n",
    "#   * Compresses large values: log(1000) = 6.9, log(1000000) = 13.8\n",
    "#   * Stretches small values: makes differences more visible\n",
    "#   * Converts exponential growth to linear\n",
    "# - Common use cases:\n",
    "#   * BINARY CROSS-ENTROPY LOSS:\n",
    "#     loss = -[y*log(p) + (1-y)*log(1-p)]\n",
    "#     Used in binary classification (logistic regression)\n",
    "#   \n",
    "#   * CATEGORICAL CROSS-ENTROPY:\n",
    "#     loss = -sum(y_true * log(y_pred))\n",
    "#     Used in multi-class classification\n",
    "#   \n",
    "#   * LOG-LIKELIHOOD (Probabilistic Models):\n",
    "#     log_likelihood = sum(log(P(data|model)))\n",
    "#     Instead of product which can underflow\n",
    "#   \n",
    "#   * LOG TRANSFORMATION (Skewed Features):\n",
    "#     log_income = np.log(income + 1)  # +1 to handle zeros\n",
    "#     Reduces right skewness in data\n",
    "#   \n",
    "#   * KL DIVERGENCE (Distribution Comparison):\n",
    "#     kl_div = sum(P * log(P/Q))\n",
    "#     Measures difference between distributions\n",
    "#   \n",
    "#   * ENTROPY (Information Content):\n",
    "#     entropy = -sum(p * log(p))\n",
    "#     Measures uncertainty in distribution\n",
    "# - Numerical stability considerations:\n",
    "#   * Always add small epsilon to avoid log(0):\n",
    "#     np.log(probs + 1e-15)\n",
    "#   * For very small probabilities, use log-space arithmetic\n",
    "#   * NumPy has np.log1p(x) = log(1+x) for better precision near 0\n",
    "# - Example:\n",
    "#   # Binary Cross-Entropy Loss (detailed)\n",
    "#   y_true = np.array([1, 0, 1, 1, 0])  # Actual labels\n",
    "#   y_pred = np.array([0.9, 0.1, 0.8, 0.7, 0.2])  # Predicted probabilities\n",
    "#   \n",
    "#   # Avoid log(0) by clipping\n",
    "#   epsilon = 1e-15\n",
    "#   y_pred_safe = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "#   \n",
    "#   # Binary cross-entropy formula\n",
    "#   bce = -np.mean(\n",
    "#       y_true * np.log(y_pred_safe) + \n",
    "#       (1 - y_true) * np.log(1 - y_pred_safe)\n",
    "#   )\n",
    "#   # Result: ~0.163\n",
    "#   \n",
    "#   # Why it works:\n",
    "#   # - When y=1: loss = -log(p) → penalizes low predictions\n",
    "#   # - When y=0: loss = -log(1-p) → penalizes high predictions\n",
    "#   # - Log makes gradient smooth and well-behaved\n",
    "#   \n",
    "#   # Categorical Cross-Entropy (multi-class)\n",
    "#   y_true_onehot = np.array([[1, 0, 0],  # Class 0\n",
    "#                             [0, 1, 0],  # Class 1\n",
    "#                             [0, 0, 1]]) # Class 2\n",
    "#   y_pred_probs = np.array([[0.7, 0.2, 0.1],\n",
    "#                            [0.1, 0.8, 0.1],\n",
    "#                            [0.2, 0.2, 0.6]])\n",
    "#   \n",
    "#   cce = -np.mean(np.sum(y_true_onehot * np.log(y_pred_probs + 1e-15), axis=1))\n",
    "#   # Result: ~0.356\n",
    "#   \n",
    "#   # Log-likelihood for Naive Bayes\n",
    "#   # Instead of: P(data) = P(x₁) × P(x₂) × P(x₃) × ... (underflow risk!)\n",
    "#   # Use: log P(data) = log P(x₁) + log P(x₂) + log P(x₃) + ... (stable!)\n",
    "#   probabilities = np.array([0.8, 0.7, 0.9, 0.6, 0.85])\n",
    "#   \n",
    "#   # Bad way (can underflow):\n",
    "#   joint_prob = np.prod(probabilities)  # 0.2286\n",
    "#   \n",
    "#   # Good way (numerically stable):\n",
    "#   log_joint_prob = np.sum(np.log(probabilities))  # -1.476\n",
    "#   # Convert back if needed:\n",
    "#   joint_prob_from_log = np.exp(log_joint_prob)  # 0.2286 ✓\n",
    "#   \n",
    "#   # Feature transformation (reduce skewness)\n",
    "#   income = np.array([30000, 50000, 100000, 500000, 1000000, 5000000])\n",
    "#   # Income is heavily right-skewed\n",
    "#   \n",
    "#   log_income = np.log(income)\n",
    "#   # [10.31, 10.82, 11.51, 13.12, 13.82, 15.42]\n",
    "#   # Much more evenly distributed!\n",
    "#   \n",
    "#   # Entropy calculation (information theory)\n",
    "#   probs = np.array([0.5, 0.3, 0.2])\n",
    "#   entropy = -np.sum(probs * np.log(probs))\n",
    "#   # Result: 1.029 nats (natural units)\n",
    "#   # High entropy = high uncertainty\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Har element ka natural logarithm (base e, jahan e ≈ 2.71828)\n",
    "#   compute karta hai. Yeh np.exp() ka INVERSE hai\n",
    "# - Syntax: np.log(array)\n",
    "# - Mathematical notation: ln(x) ya log_e(x)\n",
    "# - Formula: Agar y = e^x, to x = log(y)\n",
    "# - Domain: Sirf POSITIVE numbers ke liye defined (x > 0)\n",
    "#   * log(0) = -∞ (negative infinity)\n",
    "#   * log(negative) = NaN (not a number)\n",
    "#   * log(1) = 0 (important property!)\n",
    "#   * log(e) = 1 (definition se)\n",
    "# - Kab use karein: ML mein BAHUT important:\n",
    "#   * Cross-entropy loss (classification)\n",
    "#   * Log-likelihood (probabilistic models)\n",
    "#   * Numerical stability (log-space computations)\n",
    "#   * Information theory (entropy, KL divergence)\n",
    "#   * Feature transformations (skewness kam karna)\n",
    "#   * Multiplications ko additions mein convert karna\n",
    "# - Key properties:\n",
    "#   * Monotonic increasing: agar x₁ < x₂, to log(x₁) < log(x₂)\n",
    "#   * Badi values ko compress karta: log(1000) = 6.9, log(1000000) = 13.8\n",
    "#   * Chhoti values ko stretch karta: differences zyada visible\n",
    "#   * Exponential growth ko linear mein convert\n",
    "# - Common use cases:\n",
    "#   * BINARY CROSS-ENTROPY LOSS:\n",
    "#     loss = -[y*log(p) + (1-y)*log(1-p)]\n",
    "#     Binary classification mein use (logistic regression)\n",
    "#   \n",
    "#   * CATEGORICAL CROSS-ENTROPY:\n",
    "#     loss = -sum(y_true * log(y_pred))\n",
    "#     Multi-class classification mein use\n",
    "#   \n",
    "#   * LOG-LIKELIHOOD (Probabilistic Models):\n",
    "#     log_likelihood = sum(log(P(data|model)))\n",
    "#     Product ki jagah jo underflow kar sakta hai\n",
    "#   \n",
    "#   * LOG TRANSFORMATION (Skewed Features):\n",
    "#     log_income = np.log(income + 1)  # zeros handle karne ke liye +1\n",
    "#     Data mein right skewness kam karta hai\n",
    "#   \n",
    "#   * KL DIVERGENCE (Distribution Comparison):\n",
    "#     kl_div = sum(P * log(P/Q))\n",
    "#     Distributions ke beech difference measure karta hai\n",
    "#   \n",
    "#   * ENTROPY (Information Content):\n",
    "#     entropy = -sum(p * log(p))\n",
    "#     Distribution mein uncertainty measure karta hai\n",
    "# - Numerical stability considerations:\n",
    "#   * Hamesha log(0) se bachne ke liye chhota epsilon add karo:\n",
    "#     np.log(probs + 1e-15)\n",
    "#   * Bahut chhoti probabilities ke liye log-space arithmetic use karo\n",
    "#   * NumPy mein np.log1p(x) = log(1+x) hai jo 0 ke paas better precision deta hai\n",
    "# - Examples upar English section mein detail se diye gaye hain\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(np.log(arr))\n",
    "# Output: [0.         0.69314718 1.09861229 1.38629436 1.60943791 1.79175947]\n",
    "#\n",
    "# Detailed calculation:\n",
    "# ln(1) = 0         (because e^0 = 1)\n",
    "# ln(2) ≈ 0.693     (because e^0.693 ≈ 2)\n",
    "# ln(3) ≈ 1.099     (because e^1.099 ≈ 3)\n",
    "# ln(4) ≈ 1.386     (because e^1.386 ≈ 4)\n",
    "# ln(5) ≈ 1.609     (because e^1.609 ≈ 5)\n",
    "# ln(6) ≈ 1.792     (because e^1.792 ≈ 6)\n",
    "#\n",
    "# Notice: Values grow slowly - log compresses large numbers\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCTION: np.exp() - EXPONENTIAL FUNCTION (DETAILED)\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# - What it does: Computes e^x (Euler's number raised to power x) for each\n",
    "#   element. This is the INVERSE of np.log()\n",
    "# - Syntax: np.exp(array)\n",
    "# - Mathematical constant: e ≈ 2.718281828459045...\n",
    "# - Formula: If x = log(y), then y = exp(x)\n",
    "# - Domain: Defined for ALL real numbers (-∞ to +∞)\n",
    "# - Range: Always POSITIVE (0 to +∞), never negative or zero\n",
    "# - Why e is special:\n",
    "#   * Derivative of e^x is e^x (only function with this property!)\n",
    "#   * Natural base for continuous growth/decay\n",
    "#   * Appears everywhere in calculus and probability\n",
    "# - Why use it: ABSOLUTELY CRITICAL in ML for:\n",
    "#   * SIGMOID ACTIVATION: σ(x) = 1/(1 + e^(-x))\n",
    "#     Used in binary classification, gates in LSTM/GRU\n",
    "#   \n",
    "#   * SOFTMAX ACTIVATION: softmax(x) = e^x / sum(e^x)\n",
    "#     Used in multi-class classification output layer\n",
    "#   \n",
    "#   * PROBABILITY DISTRIBUTIONS:\n",
    "#     - Gaussian/Normal: (1/√(2πσ²)) × e^(-(x-μ)²/(2σ²))\n",
    "#     - Exponential: λe^(-λx)\n",
    "#     - Poisson: (λ^k × e^(-λ)) / k!\n",
    "#   \n",
    "#   * LOG-SPACE TO NORMAL-SPACE:\n",
    "#     Converting log probabilities back to probabilities\n",
    "#   \n",
    "#   * GRADIENT COMPUTATIONS:\n",
    "#     Derivatives in backpropagation involve exp\n",
    "#   \n",
    "#   * GROWTH/DECAY MODELS:\n",
    "#     Exponential learning rate decay, population growth\n",
    "# - Warning - NUMERICAL ISSUES:\n",
    "#   * OVERFLOW: e^100 ≈ 2.7 × 10^43 (huge!)\n",
    "#     e^1000 causes overflow (returns inf)\n",
    "#   \n",
    "#   * Solution: For softmax, subtract max before exp:\n",
    "#     exp(x - max(x)) / sum(exp(x - max(x)))\n",
    "#   \n",
    "#   * Never compute exp of very large numbers directly\n",
    "# - Common use cases:\n",
    "#   # SIGMOID ACTIVATION FUNCTION (detailed)\n",
    "#   def sigmoid(x):\n",
    "#       \"\"\"\n",
    "#       Sigmoid: Maps any input to (0, 1) range\n",
    "#       σ(x) = 1 / (1 + e^(-x))\n",
    "#       \n",
    "#       Properties:\n",
    "#       - Output range: (0, 1) - perfect for probabilities\n",
    "#       - σ(0) = 0.5 (midpoint)\n",
    "#       - σ(large positive) → 1\n",
    "#       - σ(large negative) → 0\n",
    "#       - Derivative: σ'(x) = σ(x) × (1 - σ(x))\n",
    "#       \"\"\"\n",
    "#       return 1 / (1 + np.exp(-x))\n",
    "#   \n",
    "#   z = np.array([-5, -2, 0, 2, 5])\n",
    "#   probs = sigmoid(z)\n",
    "#   # [0.007, 0.119, 0.5, 0.881, 0.993]\n",
    "#   \n",
    "#   # Use in logistic regression:\n",
    "#   # 1. Linear combination: z = w·x + b\n",
    "#   # 2. Sigmoid: p = sigmoid(z)\n",
    "#   # 3. Prediction: class = 1 if p > 0.5 else 0\n",
    "#   \n",
    "#   # SOFTMAX ACTIVATION (with numerical stability)\n",
    "#   def softmax(x):\n",
    "#       \"\"\"\n",
    "#       Softmax: Converts logits to probability distribution\n",
    "#       \n",
    "#       Properties:\n",
    "#       - Output sums to 1.0\n",
    "#       - All outputs in (0, 1)\n",
    "#       - Differentiable everywhere\n",
    "#       - Used in multi-class classification\n",
    "#       \"\"\"\n",
    "#       # Numerical stability: subtract max\n",
    "#       exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "#       return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "#   \n",
    "#   # Example: 3-class classification\n",
    "#   logits = np.array([[2.0, 1.0, 0.1],    # Sample 1\n",
    "#                      [0.5, 2.5, 1.0],    # Sample 2\n",
    "#                      [3.0, 0.5, 2.0]])   # Sample 3\n",
    "#   \n",
    "#   probs = softmax(logits)\n",
    "#   # [[0.659, 0.242, 0.099],  # Class 0 most likely\n",
    "#   #  [0.131, 0.717, 0.152],  # Class 1 most likely\n",
    "#   #  [0.705, 0.058, 0.237]]  # Class 0 most likely\n",
    "#   \n",
    "#   # Verify: sums to 1\n",
    "#   np.sum(probs, axis=1)  # [1.0, 1.0, 1.0] ✓\n",
    "#   \n",
    "#   # Converting log probabilities to probabilities\n",
    "#   log_probs = np.array([-0.5, -1.0, -1.5, -2.0])\n",
    "#   # These might come from log-space computations\n",
    "#   \n",
    "#   probs = np.exp(log_probs)\n",
    "#   # [0.607, 0.368, 0.223, 0.135]\n",
    "#   \n",
    "#   # Exponential decay (learning rate scheduling)\n",
    "#   def exponential_decay(initial_lr, decay_rate, step):\n",
    "#       \"\"\"\n",
    "#       Learning rate decay: lr = lr₀ × e^(-decay_rate × step)\n",
    "#       Smooth continuous decay\n",
    "#       \"\"\"\n",
    "#       return initial_lr * np.exp(-decay_rate * step)\n",
    "#   \n",
    "#   initial_lr = 0.1\n",
    "#   decay_rate = 0.05\n",
    "#   steps = np.arange(0, 100, 10)\n",
    "#   lr_schedule = exponential_decay(initial_lr, decay_rate, steps)\n",
    "#   # [0.1, 0.061, 0.037, 0.022, 0.014, 0.008, ...]\n",
    "#   \n",
    "#   # Gaussian (Normal) distribution\n",
    "#   def gaussian(x, mu=0, sigma=1):\n",
    "#       \"\"\"Normal distribution PDF\"\"\"\n",
    "#       return (1 / np.sqrt(2 * np.pi * sigma**2)) * \\\n",
    "#              np.exp(-(x - mu)**2 / (2 * sigma**2))\n",
    "#   \n",
    "#   x = np.linspace(-3, 3, 100)\n",
    "#   pdf = gaussian(x, mu=0, sigma=1)\n",
    "#   # Bell curve centered at 0\n",
    "#\n",
    "# HINGLISH:\n",
    "# - Yeh kya karta hai: Har element ke liye e^x compute karta hai (Euler's\n",
    "#   number ko power x tak raise). Yeh np.log() ka INVERSE hai\n",
    "# - Syntax: np.exp(array)\n",
    "# - Mathematical constant: e ≈ 2.718281828459045...\n",
    "# - Formula: Agar x = log(y), to y = exp(x)\n",
    "# - Domain: SAARE real numbers ke liye defined (-∞ se +∞ tak)\n",
    "# - Range: Hamesha POSITIVE (0 se +∞ tak), kabhi negative ya zero nahi\n",
    "# - e kyun special hai:\n",
    "#   * e^x ka derivative e^x hai (yeh property sirf isi function mein!)\n",
    "#   * Continuous growth/decay ke liye natural base\n",
    "#   * Calculus aur probability mein har jagah aata hai\n",
    "# - Kab use karein: ML mein BILKUL CRITICAL:\n",
    "#   * SIGMOID ACTIVATION: σ(x) = 1/(1 + e^(-x))\n",
    "#     Binary classification, LSTM/GRU mein gates\n",
    "#   \n",
    "#   * SOFTMAX ACTIVATION: softmax(x) = e^x / sum(e^x)\n",
    "#     Multi-class classification output layer\n",
    "#   \n",
    "#   * PROBABILITY DISTRIBUTIONS:\n",
    "#     - Gaussian/Normal: (1/√(2πσ²)) × e^(-(x-μ)²/(2σ²))\n",
    "#     - Exponential: λe^(-λx)\n",
    "#     - Poisson: (λ^k × e^(-λ)) / k!\n",
    "#   \n",
    "#   * LOG-SPACE SE NORMAL-SPACE:\n",
    "#     Log probabilities ko wapas probabilities mein convert karna\n",
    "#   \n",
    "#   * GRADIENT COMPUTATIONS:\n",
    "#     Backpropagation mein derivatives exp involve karte hain\n",
    "#   \n",
    "#   * GROWTH/DECAY MODELS:\n",
    "#     Exponential learning rate decay, population growth\n",
    "# - Warning - NUMERICAL ISSUES:\n",
    "#   * OVERFLOW: e^100 ≈ 2.7 × 10^43 (bahut bada!)\n",
    "#     e^1000 overflow karta hai (inf return)\n",
    "#   \n",
    "#   * Solution: Softmax ke liye, exp se pehle max subtract:\n",
    "#     exp(x - max(x)) / sum(exp(x - max(x)))\n",
    "#   \n",
    "#   * Kabhi bahut bade numbers ka directly exp compute mat karo\n",
    "# - Common use cases ke examples upar English section mein detail se hain\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(np.exp(arr))\n",
    "# Output: [  2.71828183   7.3890561   20.08553692  54.59815003 148.4131591  403.42879349]\n",
    "#\n",
    "# Detailed calculation:\n",
    "# e^1 ≈ 2.718      (definition of e)\n",
    "# e^2 ≈ 7.389      (e × e)\n",
    "# e^3 ≈ 20.086     (e × e × e)\n",
    "# e^4 ≈ 54.598\n",
    "# e^5 ≈ 148.413\n",
    "# e^6 ≈ 403.429\n",
    "#\n",
    "# Notice: Values grow VERY FAST - exponential growth!\n",
    "\n",
    "# =============================================================================\n",
    "# VERIFYING INVERSE RELATIONSHIP\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Demonstrating that log and exp are inverse operations:\n",
    "#\n",
    "# Property 1: exp(log(x)) = x\n",
    "original = np.array([2.0, 5.0, 10.0, 100.0])\n",
    "log_values = np.log(original)\n",
    "back_to_original = np.exp(log_values)\n",
    "print(\"exp(log(x)) =\", back_to_original)\n",
    "# Output: [  2.   5.  10. 100.]  ✓ (same as original)\n",
    "\n",
    "# Property 2: log(exp(x)) = x\n",
    "original2 = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "exp_values = np.exp(original2)\n",
    "back_to_original2 = np.log(exp_values)\n",
    "print(\"log(exp(x)) =\", back_to_original2)\n",
    "# Output: [1. 2. 3. 4.]  ✓ (same as original)\n",
    "#\n",
    "# HINGLISH:\n",
    "# Demonstrate kar rahe hain ki log aur exp inverse operations hain:\n",
    "# (Same code as above)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# =============================================================================\n",
    "# PRACTICAL ML EXAMPLE: COMPLETE LOSS FUNCTION IMPLEMENTATIONS\n",
    "# =============================================================================\n",
    "# ENGLISH:\n",
    "# Real-world scenario: Implementing common ML loss functions\n",
    "\n",
    "# 1. BINARY CROSS-ENTROPY LOSS (Logistic Regression)\n",
    "def binary_crossentropy(y_true, y_pred, epsilon=1e-15):\n",
    "    \"\"\"\n",
    "    Binary classification loss\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels (0 or 1)\n",
    "        y_pred: Predicted probabilities (0 to 1)\n",
    "        epsilon: Small value to avoid log(0)\n",
    "    \n",
    "    Returns:\n",
    "        Average loss across samples\n",
    "    \"\"\"\n",
    "    # Clip predictions to avoid log(0) and log(1)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    \n",
    "    # BCE formula: -[y*log(p) + (1-y)*log(1-p)]\n",
    "    loss = -np.mean(\n",
    "        y_true * np.log(y_pred) + \n",
    "        (1 - y_true) * np.log(1 - y_pred)\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "# Test\n",
    "y_true_binary = np.array([1, 0, 1, 1, 0])\n",
    "y_pred_binary = np.array([0.9, 0.1, 0.8, 0.7, 0.2])\n",
    "bce_loss = binary_crossentropy(y_true_binary, y_pred_binary)\n",
    "print(f\"Binary Cross-Entropy Loss: {bce_loss:.4f}\")\n",
    "# Output: ~0.1630\n",
    "\n",
    "# 2. CATEGORICAL CROSS-ENTROPY (Multi-class Classification)\n",
    "def categorical_crossentropy(y_true, y_pred, epsilon=1e-15):\n",
    "    \"\"\"\n",
    "    Multi-class classification loss\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels (one-hot encoded)\n",
    "        y_pred: Predicted probabilities (softmax output)\n",
    "        epsilon: Small value to avoid log(0)\n",
    "    \n",
    "    Returns:\n",
    "        Average loss across samples\n",
    "    \"\"\"\n",
    "    # Clip predictions\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    \n",
    "    # CCE formula: -sum(y_true * log(y_pred)) per sample\n",
    "    loss = -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "    return loss\n",
    "\n",
    "# Test\n",
    "y_true_cat = np.array([[1, 0, 0],   # Class 0\n",
    "                       [0, 1, 0],   # Class 1\n",
    "                       [0, 0, 1]])  # Class 2\n",
    "y_pred_cat = np.array([[0.7, 0.2, 0.1],\n",
    "                       [0.1, 0.8, 0.1],\n",
    "                       [0.2, 0.2, 0.6]])\n",
    "cce_loss = categorical_crossentropy(y_true_cat, y_pred_cat)\n",
    "print(f\"Categorical Cross-Entropy Loss: {cce_loss:.4f}\")\n",
    "# Output: ~0.3567\n",
    "\n",
    "# 3. SOFTMAX FUNCTION (with numerical stability)\n",
    "def softmax_stable(x):\n",
    "    \"\"\"\n",
    "    Numerically stable softmax\n",
    "    Subtracts max to prevent overflow\n",
    "    \"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "# Test\n",
    "logits = np.array([[2.0, 1.0, 0.1],\n",
    "                   [100.0, 99.0, 98.0]])  # Large values!\n",
    "probs = softmax_stable(logits)\n",
    "print(\"Softmax (stable):\")\n",
    "print(probs)\n",
    "print(\"Sums:\", np.sum(probs, axis=1))  # Should be [1.0, 1.0]\n",
    "\n",
    "# 4. SIGMOID FUNCTION\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation with overflow protection\"\"\"\n",
    "    # For very negative values, exp(-x) overflows\n",
    "    # Use np.exp(x) / (1 + np.exp(x)) for positive x\n",
    "    return np.where(\n",
    "        x >= 0,\n",
    "        1 / (1 + np.exp(-x)),\n",
    "        np.exp(x) / (1 + np.exp(x))\n",
    "    )\n",
    "\n",
    "# Test\n",
    "z_values = np.array([-10, -2, 0, 2, 10])\n",
    "sig_output = sigmoid(z_values)\n",
    "print(\"Sigmoid:\", sig_output)\n",
    "# Output: [0.0000454, 0.119, 0.5, 0.881, 0.99995]\n",
    "\n",
    "#\n",
    "# HINGLISH:\n",
    "# Real-world scenario: Common ML loss functions implement karna\n",
    "# (Same code as English - code is universal!)\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# ADDITIONAL USEFUL VARIATIONS\n",
    "# =============================================================================\n",
    "\n",
    "# np.log1p(x) - Computes log(1 + x) with better precision for small x\n",
    "small_values = np.array([0.001, 0.01, 0.1])\n",
    "print(\"log1p (better for small x):\", np.log1p(small_values))\n",
    "# More accurate than np.log(1 + small_values) for tiny values\n",
    "\n",
    "# np.expm1(x) - Computes exp(x) - 1 with better precision for small x\n",
    "print(\"expm1 (better for small x):\", np.expm1(small_values))\n",
    "# Inverse of log1p: expm1(log1p(x)) = x\n",
    "\n",
    "# np.logaddexp(x1, x2) - Computes log(exp(x1) + exp(x2)) stably\n",
    "# Useful for combining log probabilities\n",
    "log_p1 = np.array([-2.0, -1.0])\n",
    "log_p2 = np.array([-1.5, -0.5])\n",
    "log_sum = np.logaddexp(log_p1, log_p2)\n",
    "print(\"Log add exp:\", log_sum)\n",
    "# Equivalent to: np.log(np.exp(log_p1) + np.exp(log_p2))\n",
    "# But numerically stable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7bfd5e3-0e77-41be-8ffe-e4e412175977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "3.0\n",
      "3.0\n",
      "[-2  1  2  3  4  5  6]\n",
      "[-2  1  2  3  4  5  6  6]\n",
      "[1 2 3 4 5 6 6 2]\n"
     ]
    }
   ],
   "source": [
    "#rounding\n",
    "print(np.round(3.13))\n",
    "print(np.floor(3.44))\n",
    "print(np.trunc(3.22))\n",
    "\n",
    "arr = np.array([1,2,3,4,5,6,6,-2])\n",
    "print(np.unique(arr))\n",
    "print(np.sort(arr))\n",
    "print(np.abs(arr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da086042-6ec8-4464-9c32-a9368aedcbdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
