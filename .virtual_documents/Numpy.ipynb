import numpy as np 



# =============================================================================
# NUMPY ARRAY CREATION AND ATTRIBUTES
# =============================================================================

# -----------------------------------------------------------------------------
# FUNCTION: np.array()
# -----------------------------------------------------------------------------
# ENGLISH:
# - What it does: Creates a NumPy array (ndarray) from a Python list, tuple, 
#   or other array-like objects. This is the most common way to create arrays.
# - Why use it: NumPy arrays are faster and more memory-efficient than Python 
#   lists for numerical operations. They support vectorized operations and 
#   broadcasting.
# - Parameters: 
#   * object: Array-like input (list, tuple, etc.)
#   * dtype: (optional) Data type of array elements (int, float, etc.)
# - Returns: ndarray object
# - Common use cases:
#   * Converting Python lists to NumPy arrays for ML/data processing
#   * Creating feature vectors, training data, or weight matrices
#   * Performing mathematical operations on collections of numbers
# - Example:
#   arr1 = np.array([1, 2, 3])           # 1D array
#   arr2 = np.array([[1, 2], [3, 4]])    # 2D array (matrix)
#   arr3 = np.array([1.5, 2.5], dtype=int)  # Specify data type
#
# HINGLISH:
# - Yeh kya karta hai: Python list, tuple ya kisi bhi array-jaisi cheez se 
#   NumPy array (ndarray) banata hai. Array banana ka yeh sabse common tarika hai.
# - Kab use karein: NumPy arrays Python lists se zyada fast aur memory-efficient
#   hote hain numerical operations ke liye. Yeh vectorized operations aur 
#   broadcasting support karte hain.
# - Parameters:
#   * object: Array jaisa input (list, tuple, etc.)
#   * dtype: (optional) Array elements ka data type (int, float, etc.)
# - Returns: ndarray object
# - Common use cases:
#   * Python lists ko NumPy arrays mein convert karna ML/data processing ke liye
#   * Feature vectors, training data, ya weight matrices banana
#   * Numbers ke collections par mathematical operations karna
# - Example:
#   arr1 = np.array([1, 2, 3])           # 1D array
#   arr2 = np.array([[1, 2], [3, 4]])    # 2D array (matrix)
#   arr3 = np.array([1.5, 2.5], dtype=int)  # Data type specify karna
# -----------------------------------------------------------------------------

arr = np.array([1, 2, 3, 4, 5, 6, 7, 8])

# -----------------------------------------------------------------------------
# ATTRIBUTE: type()
# -----------------------------------------------------------------------------
# ENGLISH:
# - What it does: Returns the class/type of the Python object
# - Why use it: To verify that you've created a NumPy array and not a regular
#   Python list. Useful for debugging and type checking.
# - Returns: <class 'numpy.ndarray'> for NumPy arrays
# - Common use cases:
#   * Debugging: Checking if conversion from list to array worked
#   * Type validation before performing NumPy-specific operations
#   * Understanding what kind of object you're working with
# - Example:
#   lst = [1, 2, 3]
#   type(lst)  # Returns: <class 'list'>
#   arr = np.array(lst)
#   type(arr)  # Returns: <class 'numpy.ndarray'>
#
# HINGLISH:
# - Yeh kya karta hai: Python object ka class/type return karta hai
# - Kab use karein: Yeh verify karne ke liye ki aapne NumPy array banaya hai
#   ya regular Python list. Debugging aur type checking ke liye useful hai.
# - Returns: NumPy arrays ke liye <class 'numpy.ndarray'>
# - Common use cases:
#   * Debugging: Check karna ki list se array conversion hua ya nahi
#   * NumPy-specific operations karne se pehle type validation
#   * Samajhna ki aap kis tarah ke object ke saath kaam kar rahe hain
# - Example:
#   lst = [1, 2, 3]
#   type(lst)  # Returns: <class 'list'>
#   arr = np.array(lst)
#   type(arr)  # Returns: <class 'numpy.ndarray'>
# -----------------------------------------------------------------------------

# -----------------------------------------------------------------------------
# ATTRIBUTE: .dtype
# -----------------------------------------------------------------------------
# ENGLISH:
# - What it does: Returns the data type of elements stored in the array
# - Why use it: Knowing data types is crucial for memory management and 
#   numerical precision in ML. Different dtypes use different memory amounts.
# - Returns: Data type object (e.g., int64, float64, int32, etc.)
# - Common use cases:
#   * Memory optimization: int8 uses less memory than int64
#   * Ensuring correct precision for calculations (float32 vs float64)
#   * Preventing type-related bugs in ML models
#   * Checking data types before feeding to neural networks
# - Example:
#   arr_int = np.array([1, 2, 3])        # dtype: int64 (default on 64-bit)
#   arr_float = np.array([1.0, 2.0])     # dtype: float64
#   arr_specific = np.array([1, 2], dtype=np.float32)  # dtype: float32
#   # int8: -128 to 127 (1 byte), int64: much larger range (8 bytes)
#
# HINGLISH:
# - Yeh kya karta hai: Array mein stored elements ka data type return karta hai
# - Kab use karein: Data types jaanna ML mein memory management aur numerical 
#   precision ke liye bahut zaroori hai. Different dtypes different memory use 
#   karte hain.
# - Returns: Data type object (jaise int64, float64, int32, etc.)
# - Common use cases:
#   * Memory optimization: int8, int64 se kam memory use karta hai
#   * Calculations ke liye sahi precision ensure karna (float32 vs float64)
#   * ML models mein type-related bugs se bachna
#   * Neural networks ko data dene se pehle data types check karna
# - Example:
#   arr_int = np.array([1, 2, 3])        # dtype: int64 (64-bit par default)
#   arr_float = np.array([1.0, 2.0])     # dtype: float64
#   arr_specific = np.array([1, 2], dtype=np.float32)  # dtype: float32
#   # int8: -128 se 127 (1 byte), int64: bahut bada range (8 bytes)
# -----------------------------------------------------------------------------

# -----------------------------------------------------------------------------
# ATTRIBUTE: .shape
# -----------------------------------------------------------------------------
# ENGLISH:
# - What it does: Returns a tuple representing the dimensions of the array
# - Why use it: Essential for understanding array structure, especially in ML
#   where you need to know dimensions for matrix operations, reshaping, and
#   ensuring compatibility between layers in neural networks.
# - Returns: Tuple of integers (e.g., (8,) for 1D, (3, 4) for 2D)
# - Common use cases:
#   * Verifying data dimensions before training ML models
#   * Debugging shape mismatches in matrix multiplication
#   * Reshaping data for neural network input layers
#   * Understanding feature dimensions in datasets
# - Example:
#   arr_1d = np.array([1, 2, 3, 4])           # shape: (4,)
#   arr_2d = np.array([[1, 2], [3, 4]])       # shape: (2, 2)
#   arr_3d = np.array([[[1, 2]], [[3, 4]]])   # shape: (2, 1, 2)
#   # In ML: X_train.shape might be (1000, 784) meaning 1000 samples, 
#   # 784 features
#
# HINGLISH:
# - Yeh kya karta hai: Array ke dimensions ko represent karne wala tuple return
#   karta hai
# - Kab use karein: Array structure samajhne ke liye bahut zaroori hai, 
#   khaaskar ML mein jahan aapko matrix operations, reshaping, aur neural 
#   networks mein layers ke beech compatibility ke liye dimensions jaanna 
#   padta hai.
# - Returns: Integers ka tuple (jaise (8,) for 1D, (3, 4) for 2D)
# - Common use cases:
#   * ML models train karne se pehle data dimensions verify karna
#   * Matrix multiplication mein shape mismatches debug karna
#   * Neural network input layers ke liye data reshape karna
#   * Datasets mein feature dimensions samajhna
# - Example:
#   arr_1d = np.array([1, 2, 3, 4])           # shape: (4,)
#   arr_2d = np.array([[1, 2], [3, 4]])       # shape: (2, 2)
#   arr_3d = np.array([[[1, 2]], [[3, 4]]])   # shape: (2, 1, 2)
#   # ML mein: X_train.shape (1000, 784) ho sakta hai matlab 1000 samples,
#   # 784 features
# -----------------------------------------------------------------------------

print(arr, type(arr), arr.dtype, arr.shape)
# Output: [1 2 3 4 5 6 7 8] <class 'numpy.ndarray'> int64 (8,)
# Explanation: Prints array values, confirms it's ndarray type, shows int64 
# data type, and (8,) shape meaning 1D array with 8 elements


# =============================================================================
# 2D NUMPY ARRAY (MATRIX) CREATION
# =============================================================================

# -----------------------------------------------------------------------------
# CONCEPT: 2D Array / Matrix Creation
# -----------------------------------------------------------------------------
# ENGLISH:
# - What it does: Creates a 2-dimensional NumPy array (matrix) using nested 
#   lists. Each inner list becomes a row in the matrix.
# - Why use it: 2D arrays are fundamental in ML for representing datasets,
#   images, weight matrices in neural networks, and performing linear algebra
#   operations like matrix multiplication, dot products, etc.
# - Structure: [[row1], [row2], [row3], ...]
#   * Each inner list must have the same length (rectangular shape required)
#   * Rows are the first dimension, columns are the second dimension
# - Common use cases:
#   * Storing tabular data (like CSV data with rows and columns)
#   * Representing images (though images are often 3D with color channels)
#   * Creating weight matrices for neural network layers
#   * Linear algebra operations (matrix multiplication, eigenvalues, etc.)
#   * Storing feature matrices in ML (rows=samples, columns=features)
# - Example:
#   # Dataset with 3 samples and 4 features each
#   X = np.array([[1.2, 2.3, 3.4, 4.5],    # Sample 1
#                 [5.6, 6.7, 7.8, 8.9],    # Sample 2
#                 [9.0, 1.1, 2.2, 3.3]])   # Sample 3
#   # Shape: (3, 4) means 3 rows, 4 columns
#   
#   # Grayscale image (5x5 pixels)
#   img = np.array([[0, 50, 100, 150, 200],
#                   [25, 75, 125, 175, 225],
#                   [50, 100, 150, 200, 250],
#                   [75, 125, 175, 225, 255],
#                   [100, 150, 200, 250, 255]])
#
# HINGLISH:
# - Yeh kya karta hai: Nested lists use karke 2-dimensional NumPy array 
#   (matrix) banata hai. Har inner list matrix mein ek row ban jati hai.
# - Kab use karein: 2D arrays ML mein bahut fundamental hain - datasets 
#   represent karne ke liye, images ke liye, neural networks mein weight 
#   matrices ke liye, aur linear algebra operations jaise matrix 
#   multiplication, dot products, etc. ke liye.
# - Structure: [[row1], [row2], [row3], ...]
#   * Har inner list ki length same honi chahiye (rectangular shape zaroori)
#   * Rows pehla dimension hain, columns doosra dimension hain
# - Common use cases:
#   * Tabular data store karna (jaise CSV data with rows aur columns)
#   * Images represent karna (lekin images aksar 3D hote hain color channels ke saath)
#   * Neural network layers ke liye weight matrices banana
#   * Linear algebra operations (matrix multiplication, eigenvalues, etc.)
#   * ML mein feature matrices store karna (rows=samples, columns=features)
# - Example:
#   # Dataset with 3 samples aur har ek mein 4 features
#   X = np.array([[1.2, 2.3, 3.4, 4.5],    # Sample 1
#                 [5.6, 6.7, 7.8, 8.9],    # Sample 2
#                 [9.0, 1.1, 2.2, 3.3]])   # Sample 3
#   # Shape: (3, 4) matlab 3 rows, 4 columns
#   
#   # Grayscale image (5x5 pixels)
#   img = np.array([[0, 50, 100, 150, 200],
#                   [25, 75, 125, 175, 225],
#                   [50, 100, 150, 200, 250],
#                   [75, 125, 175, 225, 255],
#                   [100, 150, 200, 250, 255]])
# -----------------------------------------------------------------------------

arr2D = np.array([[1, 2, 3],    # Row 0 (first row)
                  [4, 5, 6],    # Row 1 (second row)
                  [7, 8, 9]])   # Row 2 (third row)

# -----------------------------------------------------------------------------
# ATTRIBUTE: .shape for 2D Arrays
# -----------------------------------------------------------------------------
# ENGLISH:
# - What it does: For 2D arrays, returns a tuple (rows, columns) representing
#   the matrix dimensions
# - Why use it: Critical for understanding data structure in ML. The shape
#   tells you how many samples and features you have, or the dimensions of
#   weight matrices that need to match for matrix operations.
# - Returns: Tuple (rows, columns) e.g., (3, 3) for a 3x3 matrix
# - Shape interpretation in ML:
#   * (m, n) where m = number of samples/rows, n = number of features/columns
#   * For images: (height, width) or (height, width, channels)
#   * For weight matrices: (input_features, output_features)
# - Common use cases:
#   * Verifying dataset dimensions: "Do I have 1000 samples with 20 features?"
#   * Checking matrix multiplication compatibility: A(m,n) @ B(n,p) = C(m,p)
#   * Debugging reshape operations
#   * Ensuring data matches model input requirements
# - Example:
#   X_train = np.array([[1, 2, 3], [4, 5, 6]])  # shape: (2, 3)
#   # 2 samples (rows), 3 features (columns)
#   
#   weights = np.array([[0.1, 0.2],
#                       [0.3, 0.4],
#                       [0.5, 0.6]])  # shape: (3, 2)
#   # Can multiply: X_train @ weights because (2,3) @ (3,2) = (2,2) ✓
#   
#   img_gray = np.array([[0, 255], [128, 64]])  # shape: (2, 2)
#   # 2x2 pixel grayscale image
#
# HINGLISH:
# - Yeh kya karta hai: 2D arrays ke liye, tuple (rows, columns) return karta
#   hai jo matrix dimensions represent karta hai
# - Kab use karein: ML mein data structure samajhne ke liye bahut zaroori hai.
#   Shape aapko batata hai ki kitne samples aur features hain, ya weight 
#   matrices ke dimensions kya hain jo matrix operations ke liye match hone 
#   chahiye.
# - Returns: Tuple (rows, columns) jaise (3, 3) for a 3x3 matrix
# - ML mein shape interpretation:
#   * (m, n) jahan m = samples/rows ki sankhya, n = features/columns ki sankhya
#   * Images ke liye: (height, width) ya (height, width, channels)
#   * Weight matrices ke liye: (input_features, output_features)
# - Common use cases:
#   * Dataset dimensions verify karna: "Kya mere paas 1000 samples hain with 20 features?"
#   * Matrix multiplication compatibility check karna: A(m,n) @ B(n,p) = C(m,p)
#   * Reshape operations debug karna
#   * Data ko model input requirements se match karna
# - Example:
#   X_train = np.array([[1, 2, 3], [4, 5, 6]])  # shape: (2, 3)
#   # 2 samples (rows), 3 features (columns)
#   
#   weights = np.array([[0.1, 0.2],
#                       [0.3, 0.4],
#                       [0.5, 0.6]])  # shape: (3, 2)
#   # Multiply kar sakte hain: X_train @ weights kyunki (2,3) @ (3,2) = (2,2) ✓
#   
#   img_gray = np.array([[0, 255], [128, 64]])  # shape: (2, 2)
#   # 2x2 pixel grayscale image
# -----------------------------------------------------------------------------

print(arr2D, arr2D.shape)
# Output: 
# [[1 2 3]
#  [4 5 6]
#  [7 8 9]] (3, 3)
# 
# Explanation: 
# - Prints the 2D array showing all 9 elements arranged in 3 rows and 3 columns
# - Shape (3, 3) confirms it's a 3x3 square matrix
# - First 3 means 3 rows, second 3 means 3 columns
# - Total elements = 3 × 3 = 9

# =============================================================================
# ADDITIONAL IMPORTANT CONCEPTS FOR 2D ARRAYS
# =============================================================================
# ENGLISH:
# Row vs Column Access:
# - arr2D[0] gives first row: [1, 2, 3]
# - arr2D[:, 0] gives first column: [1, 4, 7]
# - arr2D[1, 2] gives element at row 1, column 2: 6
#
# Matrix Operations:
# - arr2D.T or arr2D.transpose() for transpose (flip rows/columns)
# - arr2D @ arr2D for matrix multiplication
# - arr2D * arr2D for element-wise multiplication
#
# HINGLISH:
# Row vs Column Access:
# - arr2D[0] pehli row deta hai: [1, 2, 3]
# - arr2D[:, 0] pehla column deta hai: [1, 4, 7]
# - arr2D[1, 2] row 1, column 2 par element deta hai: 6
#
# Matrix Operations:
# - arr2D.T ya arr2D.transpose() transpose ke liye (rows/columns flip)
# - arr2D @ arr2D matrix multiplication ke liye
# - arr2D * arr2D element-wise multiplication ke liye
# =============================================================================


# =============================================================================
# NUMPY ARRAY PROPERTIES - COMPREHENSIVE OVERVIEW
# =============================================================================

# -----------------------------------------------------------------------------
# CREATING A NON-SQUARE 2D ARRAY FOR PROPERTY EXPLORATION
# -----------------------------------------------------------------------------
# ENGLISH:
# - What it does: Creates a 2D array (matrix) with different number of rows
#   and columns (3 rows × 4 columns = non-square matrix)
# - Why use it: Non-square matrices are very common in ML - your dataset
#   usually has different number of samples than features (e.g., 1000 samples
#   with 784 features for MNIST digit images)
# - Structure: This is a 3×4 matrix (3 rows, 4 columns)
# - Common use cases:
#   * Storing datasets where rows=samples, columns=features
#   * Weight matrices connecting layers of different sizes in neural networks
#   * Transformation matrices that change dimensionality
# - Example:
#   # Customer data: 3 customers, 4 features (age, income, score, tenure)
#   customers = np.array([[25, 50000, 720, 2],
#                         [35, 75000, 680, 5],
#                         [45, 90000, 750, 10]])
#   # Shape: (3, 4) - 3 customers, 4 features each
#
# HINGLISH:
# - Yeh kya karta hai: Ek 2D array (matrix) banata hai jismein rows aur 
#   columns ki sankhya alag hai (3 rows × 4 columns = non-square matrix)
# - Kab use karein: Non-square matrices ML mein bahut common hain - aapka
#   dataset mein usually samples aur features ki sankhya alag hoti hai
#   (jaise 1000 samples with 784 features for MNIST digit images)
# - Structure: Yeh ek 3×4 matrix hai (3 rows, 4 columns)
# - Common use cases:
#   * Datasets store karna jahan rows=samples, columns=features
#   * Neural networks mein different sizes ki layers ko connect karne ke liye
#     weight matrices
#   * Transformation matrices jo dimensionality change karti hain
# - Example:
#   # Customer data: 3 customers, 4 features (age, income, score, tenure)
#   customers = np.array([[25, 50000, 720, 2],
#                         [35, 75000, 680, 5],
#                         [45, 90000, 750, 10]])
#   # Shape: (3, 4) - 3 customers, har ek mein 4 features
# -----------------------------------------------------------------------------

test_arra = np.array([[1, 2, 3, 4],      # Row 0: 4 elements
                      [5, 6, 7, 8],      # Row 1: 4 elements
                      [9, 10, 11, 12]])  # Row 2: 4 elements

# =============================================================================
# PROPERTY 1: .shape
# =============================================================================
# ENGLISH:
# - What it does: Returns the dimensions of the array as a tuple
# - Why use it: Most important property for understanding array structure.
#   Essential for debugging shape mismatches in ML operations.
# - Returns: Tuple of integers representing size of each dimension
#   * For this array: (3, 4) means 3 rows, 4 columns
# - How to interpret:
#   * 1D array: (n,) - single dimension with n elements
#   * 2D array: (m, n) - m rows, n columns
#   * 3D array: (l, m, n) - l matrices, each with m rows and n columns
#   * nD array: (d1, d2, d3, ..., dn) - n dimensions
# - Common use cases:
#   * Checking if data matches model input requirements
#   * Verifying matrix multiplication compatibility: (m,n) @ (n,p) → (m,p)
#   * Understanding data layout before reshaping
#   * Debugging "shape mismatch" errors in neural networks
# - Example:
#   arr_1d = np.array([1, 2, 3])                    # shape: (3,)
#   arr_2d = np.array([[1, 2], [3, 4], [5, 6]])    # shape: (3, 2)
#   arr_3d = np.array([[[1, 2]], [[3, 4]]])        # shape: (2, 1, 2)
#   
#   # ML example: MNIST dataset
#   X_train = np.zeros((60000, 784))  # 60000 images, 784 features (28×28)
#   X_train.shape  # Returns: (60000, 784)
#
# HINGLISH:
# - Yeh kya karta hai: Array ke dimensions ko tuple ke roop mein return karta hai
# - Kab use karein: Array structure samajhne ke liye sabse important property.
#   ML operations mein shape mismatches debug karne ke liye essential hai.
# - Returns: Integers ka tuple jo har dimension ka size represent karta hai
#   * Is array ke liye: (3, 4) matlab 3 rows, 4 columns
# - Kaise interpret karein:
#   * 1D array: (n,) - ek hi dimension with n elements
#   * 2D array: (m, n) - m rows, n columns
#   * 3D array: (l, m, n) - l matrices, har ek mein m rows aur n columns
#   * nD array: (d1, d2, d3, ..., dn) - n dimensions
# - Common use cases:
#   * Check karna ki data model input requirements se match kar raha hai
#   * Matrix multiplication compatibility verify karna: (m,n) @ (n,p) → (m,p)
#   * Reshape karne se pehle data layout samajhna
#   * Neural networks mein "shape mismatch" errors debug karna
# - Example:
#   arr_1d = np.array([1, 2, 3])                    # shape: (3,)
#   arr_2d = np.array([[1, 2], [3, 4], [5, 6]])    # shape: (3, 2)
#   arr_3d = np.array([[[1, 2]], [[3, 4]]])        # shape: (2, 1, 2)
#   
#   # ML example: MNIST dataset
#   X_train = np.zeros((60000, 784))  # 60000 images, 784 features (28×28)
#   X_train.shape  # Returns: (60000, 784)
# -----------------------------------------------------------------------------

print(test_arra.shape)
# Output: (3, 4)
# Meaning: 3 rows (first dimension), 4 columns (second dimension)

# =============================================================================
# PROPERTY 2: .size
# =============================================================================
# ENGLISH:
# - What it does: Returns the total number of elements in the array
# - Why use it: Useful for calculating memory usage, understanding total data
#   points, and verifying array operations. Size = product of all dimensions.
# - Returns: Single integer (total element count)
# - Formula: For shape (3, 4), size = 3 × 4 = 12
# - Difference from .shape:
#   * .shape gives dimensions: (3, 4)
#   * .size gives total elements: 12
# - Common use cases:
#   * Calculating memory consumption: size × dtype.itemsize = bytes
#   * Verifying flatten/ravel operations didn't lose data
#   * Checking if two arrays have same number of elements
#   * Understanding data volume for processing time estimates
# - Example:
#   arr = np.array([[1, 2, 3], [4, 5, 6]])
#   arr.shape  # (2, 3)
#   arr.size   # 6 (because 2 × 3 = 6)
#   
#   # Memory calculation
#   arr.size * arr.dtype.itemsize  # 6 × 8 = 48 bytes (for int64)
#   
#   # 3D array
#   arr_3d = np.zeros((2, 3, 4))
#   arr_3d.shape  # (2, 3, 4)
#   arr_3d.size   # 24 (because 2 × 3 × 4 = 24)
#
# HINGLISH:
# - Yeh kya karta hai: Array mein total elements ki sankhya return karta hai
# - Kab use karein: Memory usage calculate karne ke liye, total data points
#   samajhne ke liye, aur array operations verify karne ke liye useful hai.
#   Size = sabhi dimensions ka product.
# - Returns: Single integer (total element count)
# - Formula: Shape (3, 4) ke liye, size = 3 × 4 = 12
# - .shape se difference:
#   * .shape dimensions deta hai: (3, 4)
#   * .size total elements deta hai: 12
# - Common use cases:
#   * Memory consumption calculate karna: size × dtype.itemsize = bytes
#   * Flatten/ravel operations verify karna ki data loss nahi hua
#   * Check karna ki do arrays mein same number of elements hain
#   * Processing time estimate ke liye data volume samajhna
# - Example:
#   arr = np.array([[1, 2, 3], [4, 5, 6]])
#   arr.shape  # (2, 3)
#   arr.size   # 6 (kyunki 2 × 3 = 6)
#   
#   # Memory calculation
#   arr.size * arr.dtype.itemsize  # 6 × 8 = 48 bytes (int64 ke liye)
#   
#   # 3D array
#   arr_3d = np.zeros((2, 3, 4))
#   arr_3d.shape  # (2, 3, 4)
#   arr_3d.size   # 24 (kyunki 2 × 3 × 4 = 24)
# -----------------------------------------------------------------------------

print(test_arra.size)
# Output: 12
# Meaning: Total 12 elements in the array (3 rows × 4 columns = 12)

# =============================================================================
# PROPERTY 3: .dtype
# =============================================================================
# ENGLISH:
# - What it does: Returns the data type of array elements
# - Why use it: Critical for memory optimization and numerical precision in ML.
#   Different dtypes consume different memory and have different precision.
# - Returns: Data type object (int8, int16, int32, int64, float32, float64, etc.)
# - Common data types:
#   * int8: -128 to 127 (1 byte) - good for small integers, saves memory
#   * int16: -32768 to 32767 (2 bytes)
#   * int32: ~-2 billion to 2 billion (4 bytes)
#   * int64: very large range (8 bytes) - default for integers
#   * float32: ~7 decimal digits precision (4 bytes) - common in deep learning
#   * float64: ~15 decimal digits precision (8 bytes) - default for floats
#   * bool: True/False (1 byte)
# - Memory impact:
#   * Array with 1 million int8 elements = 1 MB
#   * Same array with int64 elements = 8 MB (8× larger!)
# - Common use cases:
#   * Reducing model memory footprint by using float32 instead of float64
#   * Ensuring correct data type before feeding to neural networks
#   * Converting data types to prevent overflow/underflow
#   * Matching data types for operations (can't mix int and float sometimes)
# - Example:
#   arr_int = np.array([1, 2, 3])           # dtype: int64 (default)
#   arr_float = np.array([1.0, 2.0, 3.0])  # dtype: float64 (default)
#   arr_int8 = np.array([1, 2, 3], dtype=np.int8)      # dtype: int8
#   arr_float32 = np.array([1.0, 2.0], dtype=np.float32)  # dtype: float32
#   
#   # Type conversion
#   arr_int.astype(np.float32)  # Convert int64 to float32
#
# HINGLISH:
# - Yeh kya karta hai: Array elements ka data type return karta hai
# - Kab use karein: ML mein memory optimization aur numerical precision ke 
#   liye bahut important hai. Different dtypes alag memory consume karte hain
#   aur alag precision dete hain.
# - Returns: Data type object (int8, int16, int32, int64, float32, float64, etc.)
# - Common data types:
#   * int8: -128 se 127 (1 byte) - small integers ke liye, memory bachata hai
#   * int16: -32768 se 32767 (2 bytes)
#   * int32: ~-2 billion se 2 billion (4 bytes)
#   * int64: bahut bada range (8 bytes) - integers ke liye default
#   * float32: ~7 decimal digits precision (4 bytes) - deep learning mein common
#   * float64: ~15 decimal digits precision (8 bytes) - floats ke liye default
#   * bool: True/False (1 byte)
# - Memory impact:
#   * 1 million int8 elements wala array = 1 MB
#   * Same array with int64 elements = 8 MB (8× zyada bada!)
# - Common use cases:
#   * float32 use karke float64 ki jagah model memory footprint kam karna
#   * Neural networks ko data dene se pehle sahi data type ensure karna
#   * Overflow/underflow se bachne ke liye data types convert karna
#   * Operations ke liye data types match karna (kabhi int aur float mix nahi kar sakte)
# - Example:
#   arr_int = np.array([1, 2, 3])           # dtype: int64 (default)
#   arr_float = np.array([1.0, 2.0, 3.0])  # dtype: float64 (default)
#   arr_int8 = np.array([1, 2, 3], dtype=np.int8)      # dtype: int8
#   arr_float32 = np.array([1.0, 2.0], dtype=np.float32)  # dtype: float32
#   
#   # Type conversion
#   arr_int.astype(np.float32)  # int64 ko float32 mein convert karna
# -----------------------------------------------------------------------------

print(test_arra.dtype)
# Output: int64
# Meaning: Each element is a 64-bit integer (8 bytes per element)
# Total memory for this array: 12 elements × 8 bytes = 96 bytes

# =============================================================================
# PROPERTY 4: .ndim
# =============================================================================
# ENGLISH:
# - What it does: Returns the number of dimensions (axes) of the array
# - Why use it: Helps understand array complexity. Essential for knowing if
#   you're working with vectors (1D), matrices (2D), tensors (3D+), etc.
# - Returns: Integer representing number of dimensions
#   * 0D: scalar (single value) - rare, usually just numbers
#   * 1D: vector (list of values) - feature vector, time series
#   * 2D: matrix (table of values) - datasets, images (grayscale)
#   * 3D: tensor (cube of values) - RGB images, video frames, batches
#   * 4D+: higher-order tensors - video batches, medical scans
# - Relationship with .shape:
#   * .ndim = length of .shape tuple
#   * shape (3, 4) → ndim = 2
#   * shape (10, 28, 28, 3) → ndim = 4
# - Common use cases:
#   * Checking if data needs reshaping before processing
#   * Validating input dimensions for neural network layers
#   * Determining if data is batched (extra dimension for batch)
#   * Understanding data structure complexity
# - Example:
#   arr_0d = np.array(42)                    # ndim: 0 (scalar)
#   arr_1d = np.array([1, 2, 3])            # ndim: 1 (vector)
#   arr_2d = np.array([[1, 2], [3, 4]])     # ndim: 2 (matrix)
#   arr_3d = np.zeros((10, 28, 28))         # ndim: 3 (tensor)
#   arr_4d = np.zeros((32, 28, 28, 3))      # ndim: 4 (batch of RGB images)
#   
#   # ML example: Image data
#   single_image = np.zeros((28, 28, 3))     # ndim: 3 (height, width, channels)
#   batch_images = np.zeros((100, 28, 28, 3)) # ndim: 4 (batch, height, width, channels)
#
# HINGLISH:
# - Yeh kya karta hai: Array ke dimensions (axes) ki sankhya return karta hai
# - Kab use karein: Array complexity samajhne mein madad karta hai. Yeh
#   jaanne ke liye essential hai ki aap vectors (1D), matrices (2D), ya 
#   tensors (3D+) ke saath kaam kar rahe hain.
# - Returns: Integer jo dimensions ki sankhya represent karta hai
#   * 0D: scalar (single value) - rare, usually sirf numbers
#   * 1D: vector (values ki list) - feature vector, time series
#   * 2D: matrix (values ki table) - datasets, images (grayscale)
#   * 3D: tensor (values ka cube) - RGB images, video frames, batches
#   * 4D+: higher-order tensors - video batches, medical scans
# - .shape ke saath relationship:
#   * .ndim = .shape tuple ki length
#   * shape (3, 4) → ndim = 2
#   * shape (10, 28, 28, 3) → ndim = 4
# - Common use cases:
#   * Check karna ki processing se pehle data ko reshape karna hai
#   * Neural network layers ke liye input dimensions validate karna
#   * Determine karna ki data batched hai (batch ke liye extra dimension)
#   * Data structure complexity samajhna
# - Example:
#   arr_0d = np.array(42)                    # ndim: 0 (scalar)
#   arr_1d = np.array([1, 2, 3])            # ndim: 1 (vector)
#   arr_2d = np.array([[1, 2], [3, 4]])     # ndim: 2 (matrix)
#   arr_3d = np.zeros((10, 28, 28))         # ndim: 3 (tensor)
#   arr_4d = np.zeros((32, 28, 28, 3))      # ndim: 4 (RGB images ka batch)
#   
#   # ML example: Image data
#   single_image = np.zeros((28, 28, 3))     # ndim: 3 (height, width, channels)
#   batch_images = np.zeros((100, 28, 28, 3)) # ndim: 4 (batch, height, width, channels)
# -----------------------------------------------------------------------------

print(test_arra.ndim)
# Output: 2
# Meaning: This is a 2-dimensional array (matrix with rows and columns)

# =============================================================================
# SUMMARY OF ALL PROPERTIES FOR test_arra
# =============================================================================
# Array: [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]
# 
# .shape  → (3, 4)   : 3 rows, 4 columns
# .size   → 12       : Total 12 elements (3 × 4)
# .dtype  → int64    : Each element is 64-bit integer (8 bytes)
# .ndim   → 2        : 2-dimensional array (matrix)
# 
# Memory calculation: 12 elements × 8 bytes = 96 bytes total
# =============================================================================

# =============================================================================
# BONUS: HOW THESE PROPERTIES RELATE IN ML
# =============================================================================
# ENGLISH:
# In Machine Learning, these properties help you understand your data:
# - .ndim tells you the type: 1D=features, 2D=dataset, 3D=images, 4D=batches
# - .shape tells you exact dimensions: (samples, features) or (height, width)
# - .size tells you total data points: affects memory and processing time
# - .dtype tells you precision and memory: float32 vs float64 can halve memory
#
# Example workflow:
# 1. Load data → check .shape to verify dimensions
# 2. Check .dtype → convert to float32 if needed for memory
# 3. Verify .ndim → add/remove dimensions for model compatibility
# 4. Monitor .size → estimate processing time and memory needs
#
# HINGLISH:
# Machine Learning mein, yeh properties aapko data samajhne mein madad karti hain:
# - .ndim type batata hai: 1D=features, 2D=dataset, 3D=images, 4D=batches
# - .shape exact dimensions batata hai: (samples, features) ya (height, width)
# - .size total data points batata hai: memory aur processing time ko affect karta hai
# - .dtype precision aur memory batata hai: float32 vs float64 memory half kar sakta hai
#
# Example workflow:
# 1. Data load karo → dimensions verify karne ke liye .shape check karo
# 2. .dtype check karo → memory ke liye zaroorat ho to float32 mein convert karo
# 3. .ndim verify karo → model compatibility ke liye dimensions add/remove karo
# 4. .size monitor karo → processing time aur memory needs estimate karo
# =============================================================================


# =============================================================================
# NUMPY ARRAY RESHAPING OPERATIONS
# =============================================================================

# -----------------------------------------------------------------------------
# CREATING BASE ARRAY FOR RESHAPING DEMONSTRATIONS
# -----------------------------------------------------------------------------
# ENGLISH:
# - What it does: Creates a 2D array with 2 rows and 3 columns (2×3 matrix)
# - Why use it: This array will be used to demonstrate various reshaping
#   operations which are fundamental in ML for transforming data between
#   different formats (e.g., image to vector, batch to single sample)
# - Structure: 2 rows, 3 columns, total 6 elements
# - Note: Total elements (6) must be preserved during reshaping operations
#
# HINGLISH:
# - Yeh kya karta hai: 2 rows aur 3 columns ke saath ek 2D array banata hai
#   (2×3 matrix)
# - Kab use karein: Is array ko various reshaping operations demonstrate
#   karne ke liye use karenge jo ML mein data ko different formats mein
#   transform karne ke liye fundamental hain (jaise image to vector, batch
#   to single sample)
# - Structure: 2 rows, 3 columns, total 6 elements
# - Note: Reshaping operations ke dauraan total elements (6) preserve hone chahiye
# -----------------------------------------------------------------------------

arr_test = np.array([[1, 2, 3],    # Row 0
                     [4, 5, 6]])   # Row 1

print(arr_test, arr_test.shape)
# Output: [[1 2 3]
#          [4 5 6]] (2, 3)
# Original array: 2 rows × 3 columns = 6 total elements

# =============================================================================
# OPERATION 1: .reshape()
# =============================================================================
# ENGLISH:
# - What it does: Changes the shape/dimensions of an array WITHOUT changing
#   the data or total number of elements. Returns a new view of the array.
# - Why use it: Critical for preparing data for neural networks. Different
#   layers expect different input shapes. You often need to convert between
#   formats (matrix → vector, vector → matrix, add batch dimension, etc.)
# - Syntax: array.reshape(new_shape) or array.reshape((dim1, dim2, ...))
# - CRITICAL RULE: new_shape must have same total elements as original
#   * Original: (2, 3) = 6 elements
#   * Valid reshapes: (3, 2), (6,), (1, 6), (6, 1), (1, 2, 3), etc.
#   * Invalid: (2, 2) = 4 elements ❌ (doesn't match 6)
# - Special value -1: NumPy auto-calculates that dimension
#   * arr.reshape(-1) → flattens to 1D
#   * arr.reshape(3, -1) → 3 rows, auto-calculate columns → (3, 2)
#   * arr.reshape(-1, 1) → column vector (6, 1)
# - Common use cases:
#   * Flattening images: (28, 28) → (784,) for fully connected layers
#   * Adding batch dimension: (784,) → (1, 784) for single sample prediction
#   * Preparing data for CNNs: (60000, 784) → (60000, 28, 28, 1)
#   * Converting between row/column vectors: (n,) → (n, 1) or (1, n)
#   * Reshaping model outputs: (batch, classes) → desired format
# - reshape() vs resize():
#   * reshape() returns new view, original unchanged
#   * resize() modifies array in-place (changes original)
# - Example:
#   # Image preprocessing
#   img_flat = np.array([...])  # shape: (784,) - flattened
#   img_2d = img_flat.reshape(28, 28)  # shape: (28, 28) - for visualization
#   
#   # Adding batch dimension
#   single_sample = np.array([1, 2, 3, 4])  # shape: (4,)
#   batched = single_sample.reshape(1, -1)   # shape: (1, 4) - batch of 1
#   
#   # Auto-calculate dimension
#   arr = np.arange(12)  # shape: (12,)
#   arr.reshape(3, -1)   # shape: (3, 4) - auto-calculates 4 columns
#   arr.reshape(-1, 2)   # shape: (6, 2) - auto-calculates 6 rows
#
# HINGLISH:
# - Yeh kya karta hai: Array ka shape/dimensions change karta hai BINA data
#   ya total elements ki sankhya change kiye. Array ka naya view return karta hai.
# - Kab use karein: Neural networks ke liye data prepare karne mein critical
#   hai. Different layers alag input shapes expect karti hain. Aapko aksar
#   formats ke beech convert karna padta hai (matrix → vector, vector → matrix,
#   batch dimension add karna, etc.)
# - Syntax: array.reshape(new_shape) ya array.reshape((dim1, dim2, ...))
# - CRITICAL RULE: new_shape mein same total elements hone chahiye jitne original mein
#   * Original: (2, 3) = 6 elements
#   * Valid reshapes: (3, 2), (6,), (1, 6), (6, 1), (1, 2, 3), etc.
#   * Invalid: (2, 2) = 4 elements ❌ (6 se match nahi karta)
# - Special value -1: NumPy us dimension ko auto-calculate karta hai
#   * arr.reshape(-1) → 1D mein flatten karta hai
#   * arr.reshape(3, -1) → 3 rows, auto-calculate columns → (3, 2)
#   * arr.reshape(-1, 1) → column vector (6, 1)
# - Common use cases:
#   * Images flatten karna: (28, 28) → (784,) fully connected layers ke liye
#   * Batch dimension add karna: (784,) → (1, 784) single sample prediction ke liye
#   * CNNs ke liye data prepare karna: (60000, 784) → (60000, 28, 28, 1)
#   * Row/column vectors ke beech convert karna: (n,) → (n, 1) ya (1, n)
#   * Model outputs reshape karna: (batch, classes) → desired format
# - reshape() vs resize():
#   * reshape() naya view return karta hai, original unchanged rehta hai
#   * resize() array ko in-place modify karta hai (original change hota hai)
# - Example:
#   # Image preprocessing
#   img_flat = np.array([...])  # shape: (784,) - flattened
#   img_2d = img_flat.reshape(28, 28)  # shape: (28, 28) - visualization ke liye
#   
#   # Batch dimension add karna
#   single_sample = np.array([1, 2, 3, 4])  # shape: (4,)
#   batched = single_sample.reshape(1, -1)   # shape: (1, 4) - 1 ka batch
#   
#   # Auto-calculate dimension
#   arr = np.arange(12)  # shape: (12,)
#   arr.reshape(3, -1)   # shape: (3, 4) - 4 columns auto-calculate
#   arr.reshape(-1, 2)   # shape: (6, 2) - 6 rows auto-calculate
# -----------------------------------------------------------------------------

reshaped = arr_test.reshape((3, 2))
# Reshaping from (2, 3) to (3, 2)
# Original layout:     New layout:
# [[1, 2, 3],     →    [[1, 2],
#  [4, 5, 6]]           [3, 4],
#                       [5, 6]]
# Elements are read in row-major order (C-style) and rearranged

print(reshaped, reshaped.shape)
# Output: [[1 2]
#          [3 4]
#          [5 6]] (3, 2)
# New array: 3 rows × 2 columns = 6 total elements (same as original)

# =============================================================================
# UNDERSTANDING RESHAPE ELEMENT ORDERING
# =============================================================================
# ENGLISH:
# How reshape reads and rearranges elements:
# 1. Elements are read in "row-major" (C-style) order: [1,2,3,4,5,6]
# 2. They are then written into new shape in same order
# 3. Original (2,3): [[1,2,3], [4,5,6]] → flattened: [1,2,3,4,5,6]
# 4. New (3,2): Take 2 elements per row → [[1,2], [3,4], [5,6]]
#
# HINGLISH:
# Reshape elements ko kaise read aur rearrange karta hai:
# 1. Elements "row-major" (C-style) order mein read hote hain: [1,2,3,4,5,6]
# 2. Phir same order mein naye shape mein likhe jaate hain
# 3. Original (2,3): [[1,2,3], [4,5,6]] → flattened: [1,2,3,4,5,6]
# 4. New (3,2): Har row mein 2 elements lo → [[1,2], [3,4], [5,6]]
# -----------------------------------------------------------------------------

# =============================================================================
# OPERATION 2: .flatten()
# =============================================================================
# ENGLISH:
# - What it does: Converts a multi-dimensional array into a 1D array (vector).
#   Always returns a COPY of the data (not a view).
# - Why use it: Essential for converting structured data (like images) into
#   vectors for machine learning algorithms that expect 1D input (e.g., 
#   logistic regression, SVMs, fully connected neural network layers)
# - Syntax: array.flatten(order='C')
# - Parameters:
#   * order='C': Row-major (C-style, default) - read rows left-to-right
#   * order='F': Column-major (Fortran-style) - read columns top-to-bottom
#   * order='A': Use 'F' if array is Fortran-contiguous, else 'C'
# - Returns: Always 1D array with shape (n,) where n = total elements
# - flatten() vs ravel():
#   * flatten() → ALWAYS returns a COPY (new memory allocation)
#   * ravel() → returns a VIEW when possible (more memory efficient)
#   * Use flatten() when you need independent copy
#   * Use ravel() when you want to save memory
# - Common use cases:
#   * Preprocessing images for traditional ML: (28, 28) → (784,)
#   * Converting feature matrices to vectors
#   * Preparing data for algorithms expecting 1D input
#   * Flattening CNN feature maps before fully connected layers
#   * Creating feature vectors from multi-dimensional data
# - Example:
#   # Image flattening for ML
#   image = np.random.rand(28, 28)  # shape: (28, 28) - grayscale image
#   features = image.flatten()       # shape: (784,) - feature vector
#   
#   # RGB image flattening
#   rgb_img = np.random.rand(64, 64, 3)  # shape: (64, 64, 3)
#   flat_img = rgb_img.flatten()          # shape: (12288,) = 64×64×3
#   
#   # Different ordering
#   arr = np.array([[1, 2], [3, 4]])
#   arr.flatten(order='C')  # [1, 2, 3, 4] - row-major
#   arr.flatten(order='F')  # [1, 3, 2, 4] - column-major
#   
#   # MNIST example
#   mnist_image = np.zeros((28, 28))  # Single MNIST digit
#   flat_digit = mnist_image.flatten()  # (784,) for neural network input
#
# HINGLISH:
# - Yeh kya karta hai: Multi-dimensional array ko 1D array (vector) mein
#   convert karta hai. Hamesha data ki COPY return karta hai (view nahi).
# - Kab use karein: Structured data (jaise images) ko vectors mein convert
#   karne ke liye essential hai, ML algorithms ke liye jo 1D input expect
#   karte hain (jaise logistic regression, SVMs, fully connected neural
#   network layers)
# - Syntax: array.flatten(order='C')
# - Parameters:
#   * order='C': Row-major (C-style, default) - rows ko left-to-right padhna
#   * order='F': Column-major (Fortran-style) - columns ko top-to-bottom padhna
#   * order='A': Array Fortran-contiguous hai to 'F' use karo, warna 'C'
# - Returns: Hamesha 1D array with shape (n,) jahan n = total elements
# - flatten() vs ravel():
#   * flatten() → HAMESHA COPY return karta hai (new memory allocation)
#   * ravel() → jab possible ho VIEW return karta hai (zyada memory efficient)
#   * flatten() use karo jab aapko independent copy chahiye
#   * ravel() use karo jab memory save karni ho
# - Common use cases:
#   * Traditional ML ke liye images preprocess karna: (28, 28) → (784,)
#   * Feature matrices ko vectors mein convert karna
#   * 1D input expect karne wale algorithms ke liye data prepare karna
#   * Fully connected layers se pehle CNN feature maps flatten karna
#   * Multi-dimensional data se feature vectors banana
# - Example:
#   # ML ke liye image flattening
#   image = np.random.rand(28, 28)  # shape: (28, 28) - grayscale image
#   features = image.flatten()       # shape: (784,) - feature vector
#   
#   # RGB image flattening
#   rgb_img = np.random.rand(64, 64, 3)  # shape: (64, 64, 3)
#   flat_img = rgb_img.flatten()          # shape: (12288,) = 64×64×3
#   
#   # Different ordering
#   arr = np.array([[1, 2], [3, 4]])
#   arr.flatten(order='C')  # [1, 2, 3, 4] - row-major
#   arr.flatten(order='F')  # [1, 3, 2, 4] - column-major
#   
#   # MNIST example
#   mnist_image = np.zeros((28, 28))  # Single MNIST digit
#   flat_digit = mnist_image.flatten()  # (784,) neural network input ke liye
# -----------------------------------------------------------------------------

flattened = arr_test.flatten()
# Converts 2D array [[1,2,3], [4,5,6]] into 1D array [1,2,3,4,5,6]
# Reads elements row by row (row-major order)
# Creates a COPY (modifying flattened won't affect arr_test)

print(flattened, flattened.shape)
# Output: [1 2 3 4 5 6] (6,)
# 1D array with 6 elements

# =============================================================================
# COMPARING THE THREE OPERATIONS
# =============================================================================
# ENGLISH:
# Original arr_test:      (2, 3) shape
# [[1, 2, 3],
#  [4, 5, 6]]
#
# After reshape(3, 2):    (3, 2) shape - Same elements, different arrangement
# [[1, 2],
#  [3, 4],
#  [5, 6]]
#
# After flatten():        (6,) shape - All elements in single row
# [1, 2, 3, 4, 5, 6]
#
# Key differences:
# - reshape(): Changes shape while preserving dimensionality concept
# - flatten(): Always produces 1D array (loses dimensional structure)
# - Both preserve total element count (6 elements throughout)
#
# HINGLISH:
# Original arr_test:      (2, 3) shape
# [[1, 2, 3],
#  [4, 5, 6]]
#
# reshape(3, 2) ke baad:  (3, 2) shape - Same elements, alag arrangement
# [[1, 2],
#  [3, 4],
#  [5, 6]]
#
# flatten() ke baad:      (6,) shape - Saare elements ek hi row mein
# [1, 2, 3, 4, 5, 6]
#
# Key differences:
# - reshape(): Shape change karta hai dimensionality concept preserve rakhte hue
# - flatten(): Hamesha 1D array produce karta hai (dimensional structure kho jata hai)
# - Dono total element count preserve karte hain (throughout 6 elements)
# -----------------------------------------------------------------------------

# =============================================================================
# ADDITIONAL RESHAPING TECHNIQUES
# =============================================================================
# ENGLISH:
# Other useful reshaping methods:
# 
# 1. ravel() - Similar to flatten() but returns view (not copy)
#    flattened_view = arr_test.ravel()  # More memory efficient
#
# 2. reshape with -1 (auto-calculate dimension)
#    auto_reshaped = arr_test.reshape(-1)     # Same as flatten: (6,)
#    col_vector = arr_test.reshape(-1, 1)     # Column vector: (6, 1)
#    row_vector = arr_test.reshape(1, -1)     # Row vector: (1, 6)
#
# 3. transpose() or .T - Swap rows and columns
#    transposed = arr_test.T  # (2,3) → (3,2) but different arrangement
#
# 4. squeeze() - Remove single-dimensional entries
#    arr_with_extra_dim = np.array([[[1, 2, 3]]])  # shape: (1, 1, 3)
#    squeezed = arr_with_extra_dim.squeeze()        # shape: (3,)
#
# 5. expand_dims() - Add new axis/dimension
#    arr_1d = np.array([1, 2, 3])  # shape: (3,)
#    arr_2d = np.expand_dims(arr_1d, axis=0)  # shape: (1, 3) - row vector
#    arr_2d = np.expand_dims(arr_1d, axis=1)  # shape: (3, 1) - column vector
#
# HINGLISH:
# Reshaping ke aur useful methods:
# 
# 1. ravel() - flatten() jaisa hai par view return karta hai (copy nahi)
#    flattened_view = arr_test.ravel()  # Zyada memory efficient
#
# 2. -1 ke saath reshape (dimension auto-calculate)
#    auto_reshaped = arr_test.reshape(-1)     # flatten jaisa: (6,)
#    col_vector = arr_test.reshape(-1, 1)     # Column vector: (6, 1)
#    row_vector = arr_test.reshape(1, -1)     # Row vector: (1, 6)
#
# 3. transpose() ya .T - Rows aur columns swap karna
#    transposed = arr_test.T  # (2,3) → (3,2) lekin alag arrangement
#
# 4. squeeze() - Single-dimensional entries remove karna
#    arr_with_extra_dim = np.array([[[1, 2, 3]]])  # shape: (1, 1, 3)
#    squeezed = arr_with_extra_dim.squeeze()        # shape: (3,)
#
# 5. expand_dims() - Naya axis/dimension add karna
#    arr_1d = np.array([1, 2, 3])  # shape: (3,)
#    arr_2d = np.expand_dims(arr_1d, axis=0)  # shape: (1, 3) - row vector
#    arr_2d = np.expand_dims(arr_1d, axis=1)  # shape: (3, 1) - column vector
# =============================================================================

# =============================================================================
# PRACTICAL ML EXAMPLE: MNIST DIGIT PREPROCESSING
# =============================================================================
# ENGLISH:
# Real-world scenario: Preparing MNIST handwritten digits for neural network
#
# # Load image (28x28 pixels)
# digit_image = np.random.rand(28, 28)  # Simulating grayscale image
# print(f"Original shape: {digit_image.shape}")  # (28, 28)
#
# # For CNN (Convolutional Neural Network) - needs (height, width, channels)
# cnn_input = digit_image.reshape(28, 28, 1)  # Add channel dimension
# print(f"CNN input shape: {cnn_input.shape}")  # (28, 28, 1)
#
# # For Fully Connected Network - needs flattened vector
# fc_input = digit_image.flatten()
# print(f"FC input shape: {fc_input.shape}")  # (784,)
#
# # For batch processing - add batch dimension
# batch_input = digit_image.reshape(1, 28, 28, 1)  # Single image in batch
# print(f"Batch input shape: {batch_input.shape}")  # (1, 28, 28, 1)
#
# HINGLISH:
# Real-world scenario: Neural network ke liye MNIST handwritten digits prepare karna
#
# # Image load karo (28x28 pixels)
# digit_image = np.random.rand(28, 28)  # Grayscale image simulate kar rahe hain
# print(f"Original shape: {digit_image.shape}")  # (28, 28)
#
# # CNN (Convolutional Neural Network) ke liye - (height, width, channels) chahiye
# cnn_input = digit_image.reshape(28, 28, 1)  # Channel dimension add karo
# print(f"CNN input shape: {cnn_input.shape}")  # (28, 28, 1)
#
# # Fully Connected Network ke liye - flattened vector chahiye
# fc_input = digit_image.flatten()
# print(f"FC input shape: {fc_input.shape}")  # (784,)
#
# # Batch processing ke liye - batch dimension add karo
# batch_input = digit_image.reshape(1, 28, 28, 1)  # Batch mein single image
# print(f"Batch input shape: {batch_input.shape}")  # (1, 28, 28, 1)
# =============================================================================


# =============================================================================
# NUMPY ARRAY INDEXING AND SLICING TECHNIQUES
# =============================================================================

# =============================================================================
# BASIC 1D ARRAY INDEXING
# =============================================================================
# ENGLISH:
# - What it does: Creates a 1D array and accesses individual elements using
#   their position (index)
# - Why use it: Basic element access is fundamental for data manipulation,
#   feature extraction, and working with individual data points in ML
# - Indexing rules:
#   * Indexing starts at 0 (first element is arr[0])
#   * Negative indexing: arr[-1] is last element, arr[-2] is second-to-last
#   * Out-of-bounds index raises IndexError
# - Common use cases:
#   * Accessing specific features from feature vectors
#   * Getting predictions for specific samples
#   * Extracting labels or targets from datasets
#   * Debugging by checking individual values
# - Example:
#   arr = np.array([10, 20, 30, 40, 50])
#   arr[0]   # 10 - first element
#   arr[2]   # 30 - third element
#   arr[-1]  # 50 - last element
#   arr[-2]  # 40 - second-to-last element
#   arr[4]   # 50 - fifth element (last)
#   # arr[5] would raise IndexError (out of bounds)
#
# HINGLISH:
# - Yeh kya karta hai: 1D array banata hai aur elements ko unki position
#   (index) use karke access karta hai
# - Kab use karein: ML mein data manipulation, feature extraction, aur
#   individual data points ke saath kaam karne ke liye basic element access
#   fundamental hai
# - Indexing rules:
#   * Indexing 0 se shuru hoti hai (pehla element arr[0] hai)
#   * Negative indexing: arr[-1] last element hai, arr[-2] second-to-last hai
#   * Out-of-bounds index IndexError raise karta hai
# - Common use cases:
#   * Feature vectors se specific features access karna
#   * Specific samples ke liye predictions lena
#   * Datasets se labels ya targets extract karna
#   * Individual values check karke debugging karna
# - Example:
#   arr = np.array([10, 20, 30, 40, 50])
#   arr[0]   # 10 - pehla element
#   arr[2]   # 30 - teesra element
#   arr[-1]  # 50 - last element
#   arr[-2]  # 40 - second-to-last element
#   arr[4]   # 50 - paanchva element (last)
#   # arr[5] IndexError raise karega (out of bounds)
# -----------------------------------------------------------------------------

arr = np.array([1, 2, 3, 4, 5])

print(arr[0])
# Output: 1
# Accesses the first element (index 0)

# =============================================================================
# 2D ARRAY INDEXING (MATRIX INDEXING)
# =============================================================================
# ENGLISH:
# - What it does: Accesses elements in multi-dimensional arrays using
#   [row_index][column_index] or [row_index, column_index] notation
# - Why use it: Essential for working with datasets, images, and matrices in
#   ML where you need to access specific rows, columns, or individual cells
# - Two indexing notations:
#   * arr[row][col] - chained indexing (less efficient, two operations)
#   * arr[row, col] - tuple indexing (preferred, single operation, faster)
# - Indexing rules:
#   * First index = row (vertical position)
#   * Second index = column (horizontal position)
#   * Both use 0-based indexing
#   * Negative indices work for both dimensions
# - Common use cases:
#   * Accessing specific pixels in images: image[y, x]
#   * Getting specific feature values: dataset[sample_idx, feature_idx]
#   * Extracting matrix elements for calculations
#   * Accessing weights in neural network weight matrices
#   * Selecting specific data points from tabular data
# - Example:
#   matrix = np.array([[10, 20, 30],
#                      [40, 50, 60],
#                      [70, 80, 90]])
#   matrix[0, 0]    # 10 - top-left corner
#   matrix[0, 2]    # 30 - first row, third column
#   matrix[2, 1]    # 80 - third row, second column
#   matrix[-1, -1]  # 90 - bottom-right corner
#   matrix[1]       # [40, 50, 60] - entire second row
#   matrix[:, 0]    # [10, 40, 70] - entire first column
#
# HINGLISH:
# - Yeh kya karta hai: Multi-dimensional arrays mein elements ko
#   [row_index][column_index] ya [row_index, column_index] notation use
#   karke access karta hai
# - Kab use karein: ML mein datasets, images, aur matrices ke saath kaam
#   karte waqt specific rows, columns, ya individual cells access karne
#   ke liye essential hai
# - Do indexing notations:
#   * arr[row][col] - chained indexing (kam efficient, do operations)
#   * arr[row, col] - tuple indexing (preferred, single operation, faster)
# - Indexing rules:
#   * Pehla index = row (vertical position)
#   * Doosra index = column (horizontal position)
#   * Dono 0-based indexing use karte hain
#   * Negative indices dono dimensions ke liye kaam karte hain
# - Common use cases:
#   * Images mein specific pixels access karna: image[y, x]
#   * Specific feature values lena: dataset[sample_idx, feature_idx]
#   * Calculations ke liye matrix elements extract karna
#   * Neural network weight matrices mein weights access karna
#   * Tabular data se specific data points select karna
# - Example:
#   matrix = np.array([[10, 20, 30],
#                      [40, 50, 60],
#                      [70, 80, 90]])
#   matrix[0, 0]    # 10 - top-left corner
#   matrix[0, 2]    # 30 - pehli row, teesra column
#   matrix[2, 1]    # 80 - teesri row, doosra column
#   matrix[-1, -1]  # 90 - bottom-right corner
#   matrix[1]       # [40, 50, 60] - poori doosri row
#   matrix[:, 0]    # [10, 40, 70] - poora pehla column
# -----------------------------------------------------------------------------

index_arr = np.array([[1, 2, 3, 4],    # Row 0: index 0
                      [5, 6, 7, 8]])   # Row 1: index 1

print(index_arr[1][2])
# Output: 7
# Accessing row 1, column 2
# Better way: index_arr[1, 2] (single operation, more efficient)
# Visual representation:
# [[1, 2, 3, 4],     Row 0
#  [5, 6, 7, 8]]     Row 1 ← We want this row
#     ↑
#  Column 2 - Element is 7

# =============================================================================
# FANCY INDEXING (ADVANCED INDEXING WITH ARRAYS)
# =============================================================================
# ENGLISH:
# - What it does: Uses an array or list of indices to select multiple elements
#   at once from another array. Returns a new array with selected elements.
# - Why use it: Extremely powerful for selecting non-contiguous or specific
#   elements based on conditions, indices, or patterns. Essential for data
#   sampling, feature selection, and batch processing in ML.
# - Syntax: array[index_array] where index_array is list/array of indices
# - Key features:
#   * Can select elements in any order
#   * Can repeat indices to duplicate elements
#   * Can use with multi-dimensional arrays
#   * Always returns a COPY (not a view)
# - Common use cases:
#   * Selecting specific samples from dataset: X_train[selected_indices]
#   * Random sampling: X[np.random.choice(len(X), size=100)]
#   * Feature selection: features[:, important_feature_indices]
#   * Creating mini-batches for training
#   * Reordering data based on custom sequence
#   * Extracting elements at specific positions
# - Example:
#   data = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90])
#   
#   # Select elements at indices 0, 3, 7
#   indices = [0, 3, 7]
#   selected = data[indices]  # [10, 40, 80]
#   
#   # Select in different order
#   indices = [7, 0, 3]
#   reordered = data[indices]  # [80, 10, 40]
#   
#   # Duplicate elements
#   indices = [0, 0, 1, 1]
#   duplicated = data[indices]  # [10, 10, 20, 20]
#   
#   # Random sampling (mini-batch creation)
#   batch_size = 32
#   random_indices = np.random.choice(len(data), size=batch_size, replace=False)
#   mini_batch = data[random_indices]
#   
#   # 2D fancy indexing
#   matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
#   row_indices = [0, 2]    # Select rows 0 and 2
#   selected_rows = matrix[row_indices]  # [[1, 2, 3], [7, 8, 9]]
#
# HINGLISH:
# - Yeh kya karta hai: Indices ki array ya list use karke doosre array se
#   ek saath multiple elements select karta hai. Selected elements ka naya
#   array return karta hai.
# - Kab use karein: Conditions, indices, ya patterns ke basis par
#   non-contiguous ya specific elements select karne ke liye bahut powerful
#   hai. ML mein data sampling, feature selection, aur batch processing ke
#   liye essential hai.
# - Syntax: array[index_array] jahan index_array indices ki list/array hai
# - Key features:
#   * Kisi bhi order mein elements select kar sakte hain
#   * Elements duplicate karne ke liye indices repeat kar sakte hain
#   * Multi-dimensional arrays ke saath use kar sakte hain
#   * Hamesha COPY return karta hai (view nahi)
# - Common use cases:
#   * Dataset se specific samples select karna: X_train[selected_indices]
#   * Random sampling: X[np.random.choice(len(X), size=100)]
#   * Feature selection: features[:, important_feature_indices]
#   * Training ke liye mini-batches banana
#   * Custom sequence ke basis par data reorder karna
#   * Specific positions par elements extract karna
# - Example:
#   data = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90])
#   
#   # Indices 0, 3, 7 par elements select karo
#   indices = [0, 3, 7]
#   selected = data[indices]  # [10, 40, 80]
#   
#   # Alag order mein select karo
#   indices = [7, 0, 3]
#   reordered = data[indices]  # [80, 10, 40]
#   
#   # Elements duplicate karo
#   indices = [0, 0, 1, 1]
#   duplicated = data[indices]  # [10, 10, 20, 20]
#   
#   # Random sampling (mini-batch creation)
#   batch_size = 32
#   random_indices = np.random.choice(len(data), size=batch_size, replace=False)
#   mini_batch = data[random_indices]
#   
#   # 2D fancy indexing
#   matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
#   row_indices = [0, 2]    # Rows 0 aur 2 select karo
#   selected_rows = matrix[row_indices]  # [[1, 2, 3], [7, 8, 9]]
# -----------------------------------------------------------------------------

idx = [0, 2, 4]  # Indices to select: first, third, and fifth elements

print(arr[idx])
# Output: [1 3 5]
# Selects elements at positions 0, 2, 4 from [1, 2, 3, 4, 5]
# arr[0]=1, arr[2]=3, arr[4]=5

# =============================================================================
# BOOLEAN INDEXING (CONDITIONAL SELECTION)
# =============================================================================
# ENGLISH:
# - What it does: Uses boolean conditions to filter and select elements from
#   an array. Creates a boolean mask (True/False array) and returns elements
#   where mask is True.
# - Why use it: Most powerful feature for data filtering in ML. Essential for
#   data cleaning, outlier removal, feature engineering, and conditional
#   operations. Works like SQL WHERE clause or pandas query.
# - How it works:
#   1. Condition creates boolean array: arr > 2 → [False, False, True, True, True]
#   2. Boolean array acts as mask: True = include, False = exclude
#   3. Returns new array with only True elements
# - Key features:
#   * Can use any comparison: >, <, >=, <=, ==, !=
#   * Can combine conditions: & (and), | (or), ~ (not)
#   * Always returns a COPY (not a view)
#   * Result length depends on condition (can be 0 to original length)
# - Common use cases:
#   * Data filtering: Remove outliers, select specific ranges
#   * Feature engineering: Create binary features based on conditions
#   * Data cleaning: Remove invalid values (NaN, negative values, etc.)
#   * Conditional statistics: Mean of positive values, count of outliers
#   * Threshold-based selection: Select predictions above confidence threshold
#   * Class separation: Select samples of specific class
# - Example:
#   ages = np.array([18, 25, 32, 45, 67, 23, 51])
#   
#   # Filter adults over 30
#   adults = ages[ages > 30]  # [32, 45, 67, 51]
#   
#   # Filter specific range (20-40 years old)
#   young_adults = ages[(ages >= 20) & (ages <= 40)]  # [25, 32, 23]
#   
#   # Exclude seniors (not over 65)
#   non_seniors = ages[~(ages > 65)]  # [18, 25, 32, 45, 23, 51]
#   
#   # Multiple conditions with OR
#   extremes = ages[(ages < 25) | (ages > 60)]  # [18, 67, 23]
#   
#   # ML example: Filter predictions
#   predictions = np.array([0.2, 0.8, 0.6, 0.3, 0.9])
#   high_confidence = predictions[predictions > 0.5]  # [0.8, 0.6, 0.9]
#   
#   # Data cleaning: Remove negative values
#   data_with_errors = np.array([5, -1, 10, -3, 15, 20])
#   clean_data = data_with_errors[data_with_errors >= 0]  # [5, 10, 15, 20]
#
# HINGLISH:
# - Yeh kya karta hai: Boolean conditions use karke array se elements filter
#   aur select karta hai. Boolean mask (True/False array) banata hai aur
#   elements return karta hai jahan mask True hai.
# - Kab use karein: ML mein data filtering ke liye sabse powerful feature hai.
#   Data cleaning, outlier removal, feature engineering, aur conditional
#   operations ke liye essential hai. SQL WHERE clause ya pandas query jaisa
#   kaam karta hai.
# - Kaise kaam karta hai:
#   1. Condition boolean array banata hai: arr > 2 → [False, False, True, True, True]
#   2. Boolean array mask ki tarah kaam karta hai: True = include, False = exclude
#   3. Sirf True elements ka naya array return karta hai
# - Key features:
#   * Koi bhi comparison use kar sakte hain: >, <, >=, <=, ==, !=
#   * Conditions combine kar sakte hain: & (and), | (or), ~ (not)
#   * Hamesha COPY return karta hai (view nahi)
#   * Result ki length condition par depend karti hai (0 se original length tak)
# - Common use cases:
#   * Data filtering: Outliers remove karna, specific ranges select karna
#   * Feature engineering: Conditions ke basis par binary features banana
#   * Data cleaning: Invalid values remove karna (NaN, negative values, etc.)
#   * Conditional statistics: Positive values ka mean, outliers ki count
#   * Threshold-based selection: Confidence threshold se upar predictions select karna
#   * Class separation: Specific class ke samples select karna
# - Example:
#   ages = np.array([18, 25, 32, 45, 67, 23, 51])
#   
#   # 30 se zyada umar wale adults filter karo
#   adults = ages[ages > 30]  # [32, 45, 67, 51]
#   
#   # Specific range filter karo (20-40 saal ke log)
#   young_adults = ages[(ages >= 20) & (ages <= 40)]  # [25, 32, 23]
#   
#   # Seniors exclude karo (65 se zyada nahi)
#   non_seniors = ages[~(ages > 65)]  # [18, 25, 32, 45, 23, 51]
#   
#   # OR ke saath multiple conditions
#   extremes = ages[(ages < 25) | (ages > 60)]  # [18, 67, 23]
#   
#   # ML example: Predictions filter karo
#   predictions = np.array([0.2, 0.8, 0.6, 0.3, 0.9])
#   high_confidence = predictions[predictions > 0.5]  # [0.8, 0.6, 0.9]
#   
#   # Data cleaning: Negative values remove karo
#   data_with_errors = np.array([5, -1, 10, -3, 15, 20])
#   clean_data = data_with_errors[data_with_errors >= 0]  # [5, 10, 15, 20]
# -----------------------------------------------------------------------------

# Boolean Indexing Example 1: Greater than condition
print(arr[arr > 2])
# Output: [3 4 5]
# Process:
# 1. arr > 2 creates: [False, False, True, True, True]
# 2. Selects elements where True: [3, 4, 5]

# Boolean Indexing Example 2: Even numbers (modulo operation)
print(arr[arr % 2 == 0])
# Output: [2 4]
# Process:
# 1. arr % 2 gives remainders: [1, 0, 1, 0, 1]
# 2. arr % 2 == 0 creates: [False, True, False, True, False]
# 3. Selects even numbers: [2, 4]

# Boolean Indexing Example 3: Odd numbers (inequality)
print(arr[arr % 2 != 0])
# Output: [1 3 5]
# Process:
# 1. arr % 2 gives remainders: [1, 0, 1, 0, 1]
# 2. arr % 2 != 0 creates: [True, False, True, False, True]
# 3. Selects odd numbers: [1, 3, 5]

# =============================================================================
# ADVANCED BOOLEAN INDEXING PATTERNS
# =============================================================================
# ENGLISH:
# Combining multiple conditions (use parentheses!):
# 
# # AND condition: Both must be True
# result = arr[(arr > 2) & (arr < 5)]  # [3, 4]
# 
# # OR condition: At least one must be True
# result = arr[(arr < 2) | (arr > 4)]  # [1, 5]
# 
# # NOT condition: Inverse of condition
# result = arr[~(arr > 3)]  # [1, 2, 3] - elements NOT greater than 3
# 
# # Complex nested conditions
# result = arr[((arr > 1) & (arr < 4)) | (arr == 5)]  # [2, 3, 5]
# 
# # Using np.where for conditional replacement
# modified = np.where(arr > 3, 100, arr)  # Replace >3 with 100, keep rest
# # Result: [1, 2, 3, 100, 100]
# 
# HINGLISH:
# Multiple conditions combine karna (parentheses use karo!):
# 
# # AND condition: Dono True hone chahiye
# result = arr[(arr > 2) & (arr < 5)]  # [3, 4]
# 
# # OR condition: Kam se kam ek True hona chahiye
# result = arr[(arr < 2) | (arr > 4)]  # [1, 5]
# 
# # NOT condition: Condition ka inverse
# result = arr[~(arr > 3)]  # [1, 2, 3] - elements jo 3 se bade NAHI hain
# 
# # Complex nested conditions
# result = arr[((arr > 1) & (arr < 4)) | (arr == 5)]  # [2, 3, 5]
# 
# # Conditional replacement ke liye np.where
# modified = np.where(arr > 3, 100, arr)  # >3 ko 100 se replace karo, baaki rakho
# # Result: [1, 2, 3, 100, 100]
# =============================================================================

# =============================================================================
# PRACTICAL ML EXAMPLE: DATA FILTERING
# =============================================================================
# ENGLISH:
# Real-world scenario: Cleaning and filtering a dataset
#
# # Simulated dataset: [age, income, credit_score]
# customers = np.array([[25, 45000, 720],
#                       [35, 75000, 680],
#                       [45, -5000, 750],  # Error: negative income
#                       [52, 90000, 850],
#                       [28, 55000, 0]])   # Error: invalid credit score
# 
# # Filter 1: Remove invalid income (negatives)
# valid_income = customers[customers[:, 1] >= 0]
# 
# # Filter 2: Select only valid credit scores (300-850)
# valid_credit = customers[(customers[:, 2] >= 300) & (customers[:, 2] <= 850)]
# 
# # Filter 3: High-value customers (income > 60k AND credit > 700)
# high_value = customers[(customers[:, 1] > 60000) & (customers[:, 2] > 700)]
# 
# # Filter 4: Young professionals (age 25-35 AND income > 50k)
# young_prof = customers[(customers[:, 0] >= 25) & 
#                        (customers[:, 0] <= 35) & 
#                        (customers[:, 1] > 50000)]
#
# HINGLISH:
# Real-world scenario: Dataset ko clean aur filter karna
#
# # Simulated dataset: [age, income, credit_score]
# customers = np.array([[25, 45000, 720],
#                       [35, 75000, 680],
#                       [45, -5000, 750],  # Error: negative income
#                       [52, 90000, 850],
#                       [28, 55000, 0]])   # Error: invalid credit score
# 
# # Filter 1: Invalid income remove karo (negatives)
# valid_income = customers[customers[:, 1] >= 0]
# 
# # Filter 2: Sirf valid credit scores select karo (300-850)
# valid_credit = customers[(customers[:, 2] >= 300) & (customers[:, 2] <= 850)]
# 
# # Filter 3: High-value customers (income > 60k AUR credit > 700)
# high_value = customers[(customers[:, 1] > 60000) & (customers[:, 2] > 700)]
# 
# # Filter 4: Young professionals (age 25-35 AUR income > 50k)
# young_prof = customers[(customers[:, 0] >= 25) & 
#                        (customers[:, 0] <= 35) & 
#                        (customers[:, 1] > 50000)]
# =============================================================================


# =============================================================================
# NUMPY ARRAY SLICING - EXTRACTING SUBARRAYS
# =============================================================================

# =============================================================================
# CREATING BASE ARRAY FOR SLICING DEMONSTRATIONS
# =============================================================================
# ENGLISH:
# - What it does: Creates a 1D array with 8 sequential elements for
#   demonstrating various slicing operations
# - Why use it: Slicing is fundamental for extracting portions of data,
#   creating training/validation splits, selecting feature ranges, and
#   working with subsequences in time series or sequences
# - Note: Unlike indexing (which returns single elements), slicing returns
#   a VIEW of the original array (not a copy) for efficiency
#
# HINGLISH:
# - Yeh kya karta hai: 8 sequential elements ke saath ek 1D array banata hai
#   various slicing operations demonstrate karne ke liye
# - Kab use karein: Data ke portions extract karne, training/validation
#   splits banana, feature ranges select karne, aur time series ya sequences
#   mein subsequences ke saath kaam karne ke liye slicing fundamental hai
# - Note: Indexing ke unlike (jo single elements return karta hai), slicing
#   efficiency ke liye original array ka VIEW return karta hai (copy nahi)
# -----------------------------------------------------------------------------

arr = np.array([1, 2, 3, 4, 5, 6, 7, 8])
# Array visualization with indices:
# Index:    0  1  2  3  4  5  6  7
# Value:    1  2  3  4  5  6  7  8
# Negative: -8 -7 -6 -5 -4 -3 -2 -1

# =============================================================================
# SLICING SYNTAX: arr[start:stop:step]
# =============================================================================
# ENGLISH:
# General syntax: array[start:stop:step]
# - start: Index where slice begins (inclusive) - default is 0
# - stop: Index where slice ends (exclusive, not included) - default is len(array)
# - step: Increment between indices (default is 1)
# 
# Key rules:
# 1. start is INCLUSIVE - element at start index IS included
# 2. stop is EXCLUSIVE - element at stop index is NOT included
# 3. Omitting values uses defaults: [:] means entire array
# 4. Negative indices count from end: -1 is last element
# 5. Slicing returns a VIEW (not copy) - modifying slice affects original
# 6. Out-of-bounds indices don't raise errors (unlike indexing)
#
# HINGLISH:
# General syntax: array[start:stop:step]
# - start: Index jahan slice shuru hota hai (inclusive) - default 0 hai
# - stop: Index jahan slice khatam hota hai (exclusive, include nahi hota) - default len(array) hai
# - step: Indices ke beech increment (default 1 hai)
# 
# Key rules:
# 1. start INCLUSIVE hai - start index par element include hota hai
# 2. stop EXCLUSIVE hai - stop index par element include NAHI hota
# 3. Values omit karne par defaults use hote hain: [:] matlab poora array
# 4. Negative indices end se count karte hain: -1 last element hai
# 5. Slicing VIEW return karta hai (copy nahi) - slice modify karne se original affect hota hai
# 6. Out-of-bounds indices error nahi raise karte (indexing ke unlike)
# -----------------------------------------------------------------------------

# =============================================================================
# SLICE 1: arr[start:stop] - Range Selection
# =============================================================================
# ENGLISH:
# - What it does: Extracts elements from start index (inclusive) to stop
#   index (exclusive). The stop element itself is NOT included.
# - Syntax: arr[start:stop]
# - Why use it: Most common slicing pattern for selecting a range of elements.
#   Essential for creating subsets, batches, or extracting specific ranges.
# - Formula: Returns elements at indices [start, start+1, ..., stop-1]
# - Length of result: stop - start elements
# - Common use cases:
#   * Creating train/test splits: X_train = X[:800], X_test = X[800:]
#   * Extracting time windows: recent_data = timeseries[-100:]
#   * Selecting feature ranges: features = data[:, 10:50]
#   * Creating mini-batches: batch = data[i:i+batch_size]
#   * Removing first/last elements: middle = arr[1:-1]
# - Example:
#   data = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90])
#   data[2:6]    # [30, 40, 50, 60] - indices 2,3,4,5 (6 excluded)
#   data[0:3]    # [10, 20, 30] - first 3 elements
#   data[5:9]    # [60, 70, 80, 90] - last 4 elements
#   data[-4:-1]  # [60, 70, 80] - 4th-last to 2nd-last (last excluded)
#   
#   # ML example: Train-test split (80-20)
#   dataset = np.arange(1000)
#   split_idx = int(0.8 * len(dataset))  # 800
#   train = dataset[:split_idx]    # First 800 samples
#   test = dataset[split_idx:]     # Last 200 samples
#
# HINGLISH:
# - Yeh kya karta hai: Start index (inclusive) se stop index (exclusive) tak
#   elements extract karta hai. Stop element khud include NAHI hota.
# - Syntax: arr[start:stop]
# - Kab use karein: Elements ki range select karne ka sabse common slicing
#   pattern. Subsets, batches banana, ya specific ranges extract karne ke
#   liye essential hai.
# - Formula: Indices [start, start+1, ..., stop-1] par elements return karta hai
# - Result ki length: stop - start elements
# - Common use cases:
#   * Train/test splits banana: X_train = X[:800], X_test = X[800:]
#   * Time windows extract karna: recent_data = timeseries[-100:]
#   * Feature ranges select karna: features = data[:, 10:50]
#   * Mini-batches banana: batch = data[i:i+batch_size]
#   * Pehle/aakhri elements remove karna: middle = arr[1:-1]
# - Example:
#   data = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90])
#   data[2:6]    # [30, 40, 50, 60] - indices 2,3,4,5 (6 excluded)
#   data[0:3]    # [10, 20, 30] - pehle 3 elements
#   data[5:9]    # [60, 70, 80, 90] - aakhri 4 elements
#   data[-4:-1]  # [60, 70, 80] - 4th-last se 2nd-last (last excluded)
#   
#   # ML example: Train-test split (80-20)
#   dataset = np.arange(1000)
#   split_idx = int(0.8 * len(dataset))  # 800
#   train = dataset[:split_idx]    # Pehle 800 samples
#   test = dataset[split_idx:]     # Aakhri 200 samples
# -----------------------------------------------------------------------------

print(arr[0:5])
# Output: [1 2 3 4 5]
# Extracts elements from index 0 to 4 (5 is excluded)
# Visual: [1, 2, 3, 4, 5, 6, 7, 8]
#          ↑           ↑
#       start=0    stop=5 (not included)
# Returns 5 elements: indices 0,1,2,3,4

# =============================================================================
# SLICE 2: arr[start:] - From Start to End
# =============================================================================
# ENGLISH:
# - What it does: Extracts all elements from start index to the end of array.
#   Omitting the stop value means "go until the end".
# - Syntax: arr[start:]
# - Why use it: Convenient shorthand for getting all remaining elements after
#   a certain point. Very common in data splitting and sequence processing.
# - Equivalent to: arr[start:len(arr)]
# - Common use cases:
#   * Getting test set after train split: X_test = X[train_size:]
#   * Removing first n elements: without_header = data[5:]
#   * Processing from middle onwards: second_half = arr[len(arr)//2:]
#   * Skipping warmup period: actual_data = timeseries[warmup_steps:]
#   * Getting tail of sequence: recent = data[-100:]
# - Example:
#   prices = np.array([100, 102, 98, 105, 110, 108, 115])
#   prices[3:]    # [105, 110, 108, 115] - from index 3 to end
#   prices[-3:]   # [108, 115] - last 3 elements (from 3rd-last to end)
#   prices[0:]    # [100, 102, 98, 105, 110, 108, 115] - entire array
#   
#   # ML example: Create validation set
#   X_train_full = np.arange(1000)
#   val_size = 200
#   X_train = X_train_full[:-val_size]  # First 800
#   X_val = X_train_full[-val_size:]    # Last 200 (from -200 to end)
#
# HINGLISH:
# - Yeh kya karta hai: Start index se array ke end tak saare elements extract
#   karta hai. Stop value omit karne ka matlab hai "end tak jao".
# - Syntax: arr[start:]
# - Kab use karein: Kisi point ke baad ke saare remaining elements lene ka
#   convenient shorthand. Data splitting aur sequence processing mein bahut
#   common hai.
# - Equivalent to: arr[start:len(arr)]
# - Common use cases:
#   * Train split ke baad test set lena: X_test = X[train_size:]
#   * Pehle n elements remove karna: without_header = data[5:]
#   * Middle se aage process karna: second_half = arr[len(arr)//2:]
#   * Warmup period skip karna: actual_data = timeseries[warmup_steps:]
#   * Sequence ki tail lena: recent = data[-100:]
# - Example:
#   prices = np.array([100, 102, 98, 105, 110, 108, 115])
#   prices[3:]    # [105, 110, 108, 115] - index 3 se end tak
#   prices[-3:]   # [108, 115] - aakhri 3 elements (3rd-last se end tak)
#   prices[0:]    # [100, 102, 98, 105, 110, 108, 115] - poora array
#   
#   # ML example: Validation set banana
#   X_train_full = np.arange(1000)
#   val_size = 200
#   X_train = X_train_full[:-val_size]  # Pehle 800
#   X_val = X_train_full[-val_size:]    # Aakhri 200 (-200 se end tak)
# -----------------------------------------------------------------------------

print(arr[1:])
# Output: [2 3 4 5 6 7 8]
# Extracts elements from index 1 to end
# Visual: [1, 2, 3, 4, 5, 6, 7, 8]
#             ↑                 ↑
#          start=1            end
# Returns 7 elements: indices 1,2,3,4,5,6,7

# =============================================================================
# SLICE 3: arr[:stop] - From Beginning to Stop
# =============================================================================
# ENGLISH:
# - What it does: Extracts elements from the beginning (index 0) up to but
#   not including the stop index. Omitting start means "start from beginning".
# - Syntax: arr[:stop]
# - Why use it: Convenient for getting first n elements. Very common for
#   creating training sets, limiting data size, or getting prefixes.
# - Equivalent to: arr[0:stop]
# - Common use cases:
#   * Getting first n samples: X_train = X[:train_size]
#   * Limiting dataset size: limited_data = large_data[:10000]
#   * Getting top-k results: top_10 = sorted_results[:10]
#   * Creating training set: train = data[:int(0.8*len(data))]
#   * Removing last elements: without_tail = arr[:-5]
# - Example:
#   scores = np.array([95, 87, 92, 78, 88, 94, 85, 90])
#   scores[:3]    # [95, 87, 92] - first 3 elements
#   scores[:5]    # [95, 87, 92, 78, 88] - first 5 elements
#   scores[:-2]   # [95, 87, 92, 78, 88, 94] - all except last 2
#   
#   # ML example: Quick prototyping with small subset
#   full_dataset = np.arange(100000)
#   quick_test = full_dataset[:1000]  # Just first 1000 for testing
#   
#   # Getting top predictions
#   probs = np.array([0.1, 0.9, 0.3, 0.8, 0.6])
#   sorted_indices = np.argsort(probs)[::-1]  # Sort descending
#   top_3_indices = sorted_indices[:3]  # Get top 3
#
# HINGLISH:
# - Yeh kya karta hai: Beginning (index 0) se lekar stop index tak (excluding)
#   elements extract karta hai. Start omit karne ka matlab "beginning se shuru".
# - Syntax: arr[:stop]
#- Kab use karein: Pehle n elements lene ka convenient tarika. Training sets
#   banane, data size limit karne, ya prefixes lene ke liye bahut common.
# - Equivalent to: arr[0:stop]
# - Common use cases:
#   * Pehle n samples lena: X_train = X[:train_size]
#   * Dataset size limit karna: limited_data = large_data[:10000]
#   * Top-k results lena: top_10 = sorted_results[:10]
#   * Training set banana: train = data[:int(0.8*len(data))]
#   * Aakhri elements remove karna: without_tail = arr[:-5]
# - Example:
#   scores = np.array([95, 87, 92, 78, 88, 94, 85, 90])
#   scores[:3]    # [95, 87, 92] - pehle 3 elements
#   scores[:5]    # [95, 87, 92, 78, 88] - pehle 5 elements
#   scores[:-2]   # [95, 87, 92, 78, 88, 94] - aakhri 2 ko chhod ke sab
#   
#   # ML example: Small subset ke saath quick prototyping
#   full_dataset = np.arange(100000)
#   quick_test = full_dataset[:1000]  # Testing ke liye sirf pehle 1000
#   
#   # Top predictions lena
#   probs = np.array([0.1, 0.9, 0.3, 0.8, 0.6])
#   sorted_indices = np.argsort(probs)[::-1]  # Descending sort
#   top_3_indices = sorted_indices[:3]  # Top 3 lo
# -----------------------------------------------------------------------------

print(arr[:5])
# Output: [1 2 3 4 5]
# Extracts elements from beginning to index 4 (5 is excluded)
# Visual: [1, 2, 3, 4, 5, 6, 7, 8]
#          ↑           ↑
#       start=0    stop=5 (not included)
# Returns 5 elements: indices 0,1,2,3,4
# Same output as arr[0:5]

# =============================================================================
# SLICE 4: arr[::step] - Every Nth Element
# =============================================================================
# ENGLISH:
# - What it does: Extracts elements at regular intervals (every step-th element)
#   from the entire array. Omitting start and stop means "entire array".
# - Syntax: arr[::step] or arr[start:stop:step]
# - Why use it: Essential for downsampling data, selecting every nth sample,
#   creating alternating patterns, or reversing arrays (step=-1).
# - Step mechanics:
#   * step > 0: Move forward through array
#   * step < 0: Move backward through array (reversal)
#   * step = 2: Every other element (0, 2, 4, 6, ...)
#   * step = 3: Every third element (0, 3, 6, 9, ...)
# - Common use cases:
#   * Downsampling time series: downsampled = signal[::10]
#   * Getting odd/even indices: even = arr[::2], odd = arr[1::2]
#   * Reversing arrays: reversed_arr = arr[::-1]
#   * Selecting alternating features: selected = features[:, ::2]
#   * Creating training batches with stride: batch = data[start::stride]
#   * Decimation in signal processing: reduced = data[::decimation_factor]
# - Example:
#   sequence = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
#   sequence[::2]    # [0, 2, 4, 6, 8] - every 2nd element (even indices)
#   sequence[1::2]   # [1, 3, 5, 7, 9] - every 2nd starting from 1 (odd indices)
#   sequence[::3]    # [0, 3, 6, 9] - every 3rd element
#   sequence[::-1]   # [9, 8, 7, 6, 5, 4, 3, 2, 1, 0] - reversed
#   sequence[::-2]   # [9, 7, 5, 3, 1] - every 2nd element, reversed
#   
#   # ML example: Downsampling high-frequency sensor data
#   sensor_data = np.sin(np.linspace(0, 10, 1000))  # 1000 samples
#   downsampled = sensor_data[::10]  # Keep every 10th sample (100 samples)
#   
#   # Creating train/val splits with alternating samples
#   indices = np.arange(1000)
#   train_indices = indices[::2]  # Even indices for training
#   val_indices = indices[1::2]   # Odd indices for validation
#
# HINGLISH:
# - Yeh kya karta hai: Poore array se regular intervals par (har step-th
#   element) elements extract karta hai. Start aur stop omit karne ka matlab
#   "poora array".
# - Syntax: arr[::step] ya arr[start:stop:step]
# - Kab use karein: Data downsample karne, har nth sample select karne,
#   alternating patterns banana, ya arrays reverse karne (step=-1) ke liye
#   essential hai.
# - Step mechanics:
#   * step > 0: Array mein aage move karo
#   * step < 0: Array mein peeche move karo (reversal)
#   * step = 2: Har doosra element (0, 2, 4, 6, ...)
#   * step = 3: Har teesra element (0, 3, 6, 9, ...)
# - Common use cases:
#   * Time series downsample karna: downsampled = signal[::10]
#   * Odd/even indices lena: even = arr[::2], odd = arr[1::2]
#   * Arrays reverse karna: reversed_arr = arr[::-1]
#   * Alternating features select karna: selected = features[:, ::2]
#   * Stride ke saath training batches banana: batch = data[start::stride]
#   * Signal processing mein decimation: reduced = data[::decimation_factor]
# - Example:
#   sequence = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
#   sequence[::2]    # [0, 2, 4, 6, 8] - har 2nd element (even indices)
#   sequence[1::2]   # [1, 3, 5, 7, 9] - 1 se shuru karke har 2nd (odd indices)
#   sequence[::3]    # [0, 3, 6, 9] - har 3rd element
#   sequence[::-1]   # [9, 8, 7, 6, 5, 4, 3, 2, 1, 0] - reversed
#   sequence[::-2]   # [9, 7, 5, 3, 1] - har 2nd element, reversed
#   
#   # ML example: High-frequency sensor data downsample karna
#   sensor_data = np.sin(np.linspace(0, 10, 1000))  # 1000 samples
#   downsampled = sensor_data[::10]  # Har 10th sample rakho (100 samples)
#   
#   # Alternating samples ke saath train/val splits banana
#   indices = np.arange(1000)
#   train_indices = indices[::2]  # Training ke liye even indices
#   val_indices = indices[1::2]   # Validation ke liye odd indices
# -----------------------------------------------------------------------------

print(arr[::2])
# Output: [1 3 5 7]
# Extracts every 2nd element (step=2) from entire array
# Visual: [1, 2, 3, 4, 5, 6, 7, 8]
#          ↑     ↑     ↑     ↑
#       idx 0   2     4     6
# Returns 4 elements: indices 0,2,4,6
# Starts at index 0, then jumps by 2 each time

# =============================================================================
# ADVANCED SLICING PATTERNS
# =============================================================================
# ENGLISH:
# Combining start, stop, and step:
# 
# # Get every 2nd element between indices 1 and 7
# arr[1:7:2]  # [2, 4, 6] - indices 1,3,5
# 
# # Get every 3rd element from start to index 8
# arr[:8:3]  # [1, 4, 7] - indices 0,3,6
# 
# # Get every 2nd element from index 2 to end
# arr[2::2]  # [3, 5, 7] - indices 2,4,6
# 
# # Reverse entire array
# arr[::-1]  # [8, 7, 6, 5, 4, 3, 2, 1]
# 
# # Reverse portion of array (indices 2 to 6)
# arr[6:2:-1]  # [7, 6, 5, 4] - reverse from index 6 to 3
# 
# # Get last 4 elements in reverse
# arr[-1:-5:-1]  # [8, 7, 6, 5]
# 
# # Copy entire array (creates actual copy, not view)
# arr_copy = arr[:]  # Full copy
# 
# HINGLISH:
# Start, stop, aur step combine karna:
# 
# # Indices 1 aur 7 ke beech har 2nd element lo
# arr[1:7:2]  # [2, 4, 6] - indices 1,3,5
# 
# # Start se index 8 tak har 3rd element lo
# arr[:8:3]  # [1, 4, 7] - indices 0,3,6
# 
# # Index 2 se end tak har 2nd element lo
# arr[2::2]  # [3, 5, 7] - indices 2,4,6
# 
# # Poora array reverse karo
# arr[::-1]  # [8, 7, 6, 5, 4, 3, 2, 1]
# 
# # Array ke portion ko reverse karo (indices 2 se 6)
# arr[6:2:-1]  # [7, 6, 5, 4] - index 6 se 3 tak reverse
# 
# # Aakhri 4 elements reverse mein lo
# arr[-1:-5:-1]  # [8, 7, 6, 5]
# 
# # Poore array ki copy banao (actual copy, view nahi)
# arr_copy = arr[:]  # Full copy
# =============================================================================

# =============================================================================
# 2D ARRAY SLICING
# =============================================================================
# ENGLISH:
# Slicing works on each dimension independently:
# 
# matrix = np.array([[1, 2, 3, 4],
#                    [5, 6, 7, 8],
#                    [9, 10, 11, 12]])
# 
# # Get first 2 rows, all columns
# matrix[:2, :]  # [[1,2,3,4], [5,6,7,8]]
# 
# # Get all rows, first 3 columns
# matrix[:, :3]  # [[1,2,3], [5,6,7], [9,10,11]]
# 
# # Get rows 1-2, columns 1-3
# matrix[1:3, 1:3]  # [[6,7], [10,11]]
# 
# # Get every other row, every other column
# matrix[::2, ::2]  # [[1,3], [9,11]]
# 
# # Reverse rows, keep columns
# matrix[::-1, :]  # [[9,10,11,12], [5,6,7,8], [1,2,3,4]]
# 
# HINGLISH:
# Har dimension par independently slicing kaam karti hai:
# 
# matrix = np.array([[1, 2, 3, 4],
#                    [5, 6, 7, 8],
#                    [9, 10, 11, 12]])
# 
# # Pehli 2 rows lo, saare columns
# matrix[:2, :]  # [[1,2,3,4], [5,6,7,8]]
# 
# # Saari rows lo, pehle 3 columns
# matrix[:, :3]  # [[1,2,3], [5,6,7], [9,10,11]]
# 
# # Rows 1-2 lo, columns 1-3
# matrix[1:3, 1:3]  # [[6,7], [10,11]]
# 
# # Har doosri row, har doosra column
# matrix[::2, ::2]  # [[1,3], [9,11]]
# 
# # Rows reverse karo, columns same rakho
# matrix[::-1, :]  # [[9,10,11,12], [5,6,7,8], [1,2,3,4]]
# =============================================================================

# =============================================================================
# VIEW vs COPY - CRITICAL CONCEPT
# =============================================================================
# ENGLISH:
# Slicing creates VIEWS (not copies) - modifying slice affects original!
# 
# original = np.array([1, 2, 3, 4, 5])
# slice_view = original[1:4]  # [2, 3, 4] - this is a VIEW
# slice_view[0] = 999  # Modify the slice
# print(original)  # [1, 999, 3, 4, 5] - ORIGINAL CHANGED!
# 
# # To create independent copy, use .copy()
# original = np.array([1, 2, 3, 4, 5])
# slice_copy = original[1:4].copy()  # [2, 3, 4] - this is a COPY
# slice_copy[0] = 999  # Modify the copy
# print(original)  # [1, 2, 3, 4, 5] - original UNCHANGED
# 
# When to use copy():
# - When you need to modify slice without affecting original
# - When storing slices for later use
# - When passing slices to functions that might modify them
# 
# HINGLISH:
# Slicing VIEWS banata hai (copies nahi) - slice modify karne se original affect hota hai!
# 
# original = np.array([1, 2, 3, 4, 5])
# slice_view = original[1:4]  # [2, 3, 4] - yeh ek VIEW hai
# slice_view[0] = 999  # Slice ko modify karo
# print(original)  # [1, 999, 3, 4, 5] - ORIGINAL BADAL GAYA!
# 
# # Independent copy banane ke liye .copy() use karo
# original = np.array([1, 2, 3, 4, 5])
# slice_copy = original[1:4].copy()  # [2, 3, 4] - yeh ek COPY hai
# slice_copy[0] = 999  # Copy ko modify karo
# print(original)  # [1, 2, 3, 4, 5] - original UNCHANGED raha
# 
# copy() kab use karein:
# - Jab aapko slice modify karna ho bina original ko affect kiye
# - Jab slices ko baad mein use karne ke liye store karna ho
# - Jab functions ko slices pass karna ho jo unhe modify kar sakte hain
# =============================================================================

# =============================================================================
# PRACTICAL ML EXAMPLE: TRAIN-VAL-TEST SPLIT
# =============================================================================
# ENGLISH:
# Real-world scenario: Splitting dataset for ML (60% train, 20% val, 20% test)
#
# dataset = np.arange(1000)  # Simulated dataset with 1000 samples
# 
# # Calculate split indices
# train_end = int(0.6 * len(dataset))  # 600
# val_end = int(0.8 * len(dataset))    # 800
# 
# # Create splits using slicing
# train_data = dataset[:train_end]      # [0:600] - first 600 samples
# val_data = dataset[train_end:val_end] # [600:800] - next 200 samples
# test_data = dataset[val_end:]         # [800:] - last 200 samples
# 
# print(f"Train: {len(train_data)} samples")  # 600
# print(f"Val: {len(val_data)} samples")      # 200
# print(f"Test: {len(test_data)} samples")    # 200
#
# HINGLISH:
# Real-world scenario: ML ke liye dataset split karna (60% train, 20% val, 20% test)
#
# dataset = np.arange(1000)  # 1000 samples ke saath simulated dataset
# 
# # Split indices calculate karo
# train_end = int(0.6 * len(dataset))  # 600
# val_end = int(0.8 * len(dataset))    # 800
# 
# # Slicing use karke splits banao
# train_data = dataset[:train_end]      # [0:600] - pehle 600 samples
# val_data = dataset[train_end:val_end] # [600:800] - agle 200 samples
# test_data = dataset[val_end:]         # [800:] - aakhri 200 samples
# 
# print(f"Train: {len(train_data)} samples")  # 600
# print(f"Val: {len(val_data)} samples")      # 200
# print(f"Test: {len(test_data)} samples")    # 200
# =============================================================================


# =============================================================================
# MULTI-DIMENSIONAL ARRAYS AND AXIS-BASED OPERATIONS
# =============================================================================

# =============================================================================
# CREATING A 2D ARRAY (MATRIX) FOR AGGREGATION OPERATIONS
# =============================================================================
# ENGLISH:
# - What it does: Creates a 2D array (matrix) with 3 rows and 4 columns to
#   demonstrate axis-based operations and aggregations
# - Why use it: Understanding axis operations is CRUCIAL in ML for computing
#   statistics across different dimensions (e.g., mean per feature, sum per
#   sample, batch normalization, etc.)
# - Structure: 3×4 matrix = 3 rows (samples/observations), 4 columns (features)
# - Common use cases:
#   * Representing datasets: rows=samples, columns=features
#   * Batch operations: computing statistics across batch dimension
#   * Feature engineering: aggregating values across different axes
#   * Image processing: operations across height, width, or channels
#
# HINGLISH:
# - Yeh kya karta hai: Aggregation operations demonstrate karne ke liye 3 rows
#   aur 4 columns ke saath ek 2D array (matrix) banata hai
# - Kab use karein: ML mein axis operations samajhna BAHUT ZAROORI hai kyunki
#   different dimensions across statistics compute karna padta hai (jaise har
#   feature ka mean, har sample ka sum, batch normalization, etc.)
# - Structure: 3×4 matrix = 3 rows (samples/observations), 4 columns (features)
# - Common use cases:
#   * Datasets represent karna: rows=samples, columns=features
#   * Batch operations: batch dimension across statistics compute karna
#   * Feature engineering: alag axes across values aggregate karna
#   * Image processing: height, width, ya channels across operations
# -----------------------------------------------------------------------------

arr2D = np.array([[1, 22, 33, 44],     # Row 0 (Sample 0)
                  [22, 33, 44, 55],    # Row 1 (Sample 1)
                  [66, 77, 88, 99]])   # Row 2 (Sample 2)
#                  ↑   ↑   ↑   ↑
#               Col0 Col1 Col2 Col3
#             (Feature 0-3)

print(arr2D)
# Output:
# [[ 1 22 33 44]
#  [22 33 44 55]
#  [66 77 88 99]]
# 
# Matrix visualization:
#        Column 0  Column 1  Column 2  Column 3
# Row 0:    1        22        33        44
# Row 1:   22        33        44        55
# Row 2:   66        77        88        99

# =============================================================================
# OPERATION 1: np.sum() - TOTAL SUM (NO AXIS SPECIFIED)
# =============================================================================
# ENGLISH:
# - What it does: Computes the sum of ALL elements in the array, collapsing
#   all dimensions into a single scalar value
# - Syntax: np.sum(array) or array.sum()
# - Why use it: Useful for getting total of all values, computing loss
#   functions, or checking data magnitude
# - When axis is NOT specified:
#   * Treats entire array as flat/1D
#   * Returns single number (scalar)
#   * Sums every single element regardless of position
# - Formula: Returns a + b + c + ... for all elements
# - Common use cases:
#   * Computing total loss across all samples and features
#   * Getting overall dataset statistics
#   * Calculating total pixel intensity in images
#   * Verifying data preprocessing (e.g., ensuring probabilities sum to N)
#   * Computing total weights in neural networks
# - Example:
#   matrix = np.array([[1, 2, 3],
#                      [4, 5, 6]])
#   total = np.sum(matrix)  # 1+2+3+4+5+6 = 21
#   
#   # ML example: Total loss
#   batch_losses = np.array([[0.5, 0.3],
#                            [0.4, 0.2],
#                            [0.6, 0.1]])
#   total_loss = np.sum(batch_losses)  # Sum of all individual losses
#   
#   # Image example: Total pixel intensity
#   image = np.random.randint(0, 255, (100, 100))  # 100x100 grayscale
#   total_intensity = np.sum(image)  # Sum of all pixel values
#
# HINGLISH:
# - Yeh kya karta hai: Array ke SAARE elements ka sum compute karta hai, saare
#   dimensions ko ek single scalar value mein collapse karke
# - Syntax: np.sum(array) ya array.sum()
# - Kab use karein: Saari values ka total lene, loss functions compute karne,
#   ya data magnitude check karne ke liye useful hai
# - Jab axis specify NAHI kiya:
#   * Poore array ko flat/1D ki tarah treat karta hai
#   * Single number (scalar) return karta hai
#   * Position se koi fark nahi, har element ko sum karta hai
# - Formula: Saare elements ke liye a + b + c + ... return karta hai
# - Common use cases:
#   * Saare samples aur features across total loss compute karna
#   * Overall dataset statistics lena
#   * Images mein total pixel intensity calculate karna
#   * Data preprocessing verify karna (jaise probabilities ka sum N hai)
#   * Neural networks mein total weights compute karna
# - Example:
#   matrix = np.array([[1, 2, 3],
#                      [4, 5, 6]])
#   total = np.sum(matrix)  # 1+2+3+4+5+6 = 21
#   
#   # ML example: Total loss
#   batch_losses = np.array([[0.5, 0.3],
#                            [0.4, 0.2],
#                            [0.6, 0.1]])
#   total_loss = np.sum(batch_losses)  # Saare individual losses ka sum
#   
#   # Image example: Total pixel intensity
#   image = np.random.randint(0, 255, (100, 100))  # 100x100 grayscale
#   total_intensity = np.sum(image)  # Saare pixel values ka sum
# -----------------------------------------------------------------------------

print(np.sum(arr2D))
# Output: 589
# Calculation: 1+22+33+44+22+33+44+55+66+77+88+99 = 589
# Sums every single element in the matrix, regardless of row or column

# =============================================================================
# UNDERSTANDING AXES IN NUMPY - CRITICAL CONCEPT
# =============================================================================
# ENGLISH:
# AXIS CONCEPT - THE MOST IMPORTANT CONCEPT IN NUMPY:
# 
# In a 2D array:
# - axis=0 means "along rows" (vertically, top to bottom)
# - axis=1 means "along columns" (horizontally, left to right)
# 
# THINK OF IT AS: "Which dimension gets COLLAPSED?"
# - axis=0: Rows collapse → Result has same number of columns
# - axis=1: Columns collapse → Result has same number of rows
# 
# Visual representation for arr2D with shape (3, 4):
# 
#        axis=1 →
#        (along columns, horizontal)
#        Column 0  Column 1  Column 2  Column 3
#    ↓     [1        22        33        44]      Row 0
# axis=0   [22       33        44        55]      Row 1
# (along   [66       77        88        99]      Row 2
#  rows,
# vertical)
# 
# When you do np.sum(arr2D, axis=0):
# - You're summing DOWN the rows (collapsing row dimension)
# - For each column, add all row values
# - Result shape: (4,) - one sum per column
# - Column 0: 1+22+66=89, Column 1: 22+33+77=132, etc.
# 
# When you do np.sum(arr2D, axis=1):
# - You're summing ACROSS the columns (collapsing column dimension)
# - For each row, add all column values
# - Result shape: (3,) - one sum per row
# - Row 0: 1+22+33+44=100, Row 1: 22+33+44+55=154, etc.
# 
# MEMORY TRICK:
# axis=0 → "0 is vertical like standing | " → operates on ROWS (vertically)
# axis=1 → "1 is horizontal like lying —" → operates on COLUMNS (horizontally)
# 
# For 3D arrays (like RGB images):
# - axis=0: Along depth/batch (first dimension)
# - axis=1: Along height/rows (second dimension)  
# - axis=2: Along width/columns (third dimension)
#
# HINGLISH:
# AXIS CONCEPT - NUMPY MEIN SABSE IMPORTANT CONCEPT:
# 
# 2D array mein:
# - axis=0 matlab "rows ke along" (vertically, upar se neeche)
# - axis=1 matlab "columns ke along" (horizontally, left se right)
# 
# AISE SOCHO: "Kaun sa dimension COLLAPSE hota hai?"
# - axis=0: Rows collapse → Result mein same number of columns
# - axis=1: Columns collapse → Result mein same number of rows
# 
# arr2D ke liye visual representation with shape (3, 4):
# 
#        axis=1 →
#        (columns ke along, horizontal)
#        Column 0  Column 1  Column 2  Column 3
#    ↓     [1        22        33        44]      Row 0
# axis=0   [22       33        44        55]      Row 1
# (rows    [66       77        88        99]      Row 2
#  ke along,
# vertical)
# 
# Jab aap np.sum(arr2D, axis=0) karte hain:
# - Aap rows ko NEECHE sum kar rahe hain (row dimension collapse)
# - Har column ke liye, saari row values add karo
# - Result shape: (4,) - har column ka ek sum
# - Column 0: 1+22+66=89, Column 1: 22+33+77=132, etc.
# 
# Jab aap np.sum(arr2D, axis=1) karte hain:
# - Aap columns ko ACROSS sum kar rahe hain (column dimension collapse)
# - Har row ke liye, saari column values add karo
# - Result shape: (3,) - har row ka ek sum
# - Row 0: 1+22+33+44=100, Row 1: 22+33+44+55=154, etc.
# 
# YAAD RAKHNE KA TARIKA:
# axis=0 → "0 khada hai jaise | " → ROWS par operate (vertically)
# axis=1 → "1 leta hua hai jaise —" → COLUMNS par operate (horizontally)
# 
# 3D arrays ke liye (jaise RGB images):
# - axis=0: Depth/batch ke along (pehla dimension)
# - axis=1: Height/rows ke along (doosra dimension)  
# - axis=2: Width/columns ke along (teesra dimension)
# -----------------------------------------------------------------------------

# =============================================================================
# OPERATION 2: np.sum() WITH axis=0 - SUM ALONG ROWS (COLUMN-WISE SUM)
# =============================================================================
# ENGLISH:
# - What it does: Sums values DOWN each column, collapsing the row dimension.
#   Results in one sum per column.
# - Syntax: np.sum(array, axis=0) or array.sum(axis=0)
# - Why use it: Essential for computing per-feature statistics in ML datasets
#   where rows are samples and columns are features
# - axis=0 behavior:
#   * Operates along axis 0 (rows) - goes DOWN vertically
#   * Collapses row dimension: (3, 4) → (4,)
#   * Returns array with one value per column
#   * Each result is sum of all rows for that column
# - Common use cases:
#   * Computing feature sums across all samples
#   * Batch statistics: sum across batch dimension
#   * Image processing: sum across height (all rows)
#   * Column-wise aggregation in datasets
#   * Computing totals per feature for normalization
# - Example:
#   data = np.array([[10, 20, 30],
#                    [40, 50, 60],
#                    [70, 80, 90]])
#   column_sums = np.sum(data, axis=0)
#   # [120, 150, 180] = [10+40+70, 20+50+80, 30+60+90]
#   
#   # ML example: Feature sums across samples
#   X = np.array([[1.5, 2.3, 3.1],   # Sample 1
#                 [2.1, 1.9, 2.8],   # Sample 2
#                 [1.8, 2.5, 3.2]])  # Sample 3
#   feature_sums = np.sum(X, axis=0)
#   # [5.4, 6.7, 9.1] - sum for each of 3 features
#   feature_means = feature_sums / X.shape[0]  # Mean per feature
#   
#   # Batch processing: Sum across batch
#   batch_predictions = np.array([[0.1, 0.9],
#                                 [0.3, 0.7],
#                                 [0.2, 0.8]])  # 3 samples, 2 classes
#   class_totals = np.sum(batch_predictions, axis=0)
#   # [0.6, 2.4] - total probability for each class across batch
#
# HINGLISH:
# - Yeh kya karta hai: Har column ko NEECHE sum karta hai, row dimension
#   collapse karke. Har column ke liye ek sum milta hai.
# - Syntax: np.sum(array, axis=0) ya array.sum(axis=0)
# - Kab use karein: ML datasets mein per-feature statistics compute karne ke
#   liye essential hai jahan rows samples hain aur columns features hain
# - axis=0 behavior:
#   * Axis 0 (rows) ke along operate karta hai - vertically NEECHE jaata hai
#   * Row dimension collapse: (3, 4) → (4,)
#   * Har column ke liye ek value ka array return karta hai
#   * Har result us column ke saare rows ka sum hai
# - Common use cases:
#   * Saare samples across feature sums compute karna
#   * Batch statistics: batch dimension across sum
#   * Image processing: height across sum (saari rows)
#   * Datasets mein column-wise aggregation
#   * Normalization ke liye har feature ka total compute karna
# - Example:
#   data = np.array([[10, 20, 30],
#                    [40, 50, 60],
#                    [70, 80, 90]])
#   column_sums = np.sum(data, axis=0)
#   # [120, 150, 180] = [10+40+70, 20+50+80, 30+60+90]
#   
#   # ML example: Samples across feature sums
#   X = np.array([[1.5, 2.3, 3.1],   # Sample 1
#                 [2.1, 1.9, 2.8],   # Sample 2
#                 [1.8, 2.5, 3.2]])  # Sample 3
#   feature_sums = np.sum(X, axis=0)
#   # [5.4, 6.7, 9.1] - 3 features mein se har ek ka sum
#   feature_means = feature_sums / X.shape[0]  # Har feature ka mean
#   
#   # Batch processing: Batch across sum
#   batch_predictions = np.array([[0.1, 0.9],
#                                 [0.3, 0.7],
#                                 [0.2, 0.8]])  # 3 samples, 2 classes
#   class_totals = np.sum(batch_predictions, axis=0)
#   # [0.6, 2.4] - batch across har class ka total probability
# -----------------------------------------------------------------------------

sum_of_columns = np.sum(arr2D, axis=0)
print(sum_of_columns)
# Output: [ 89 132 165 198]
# 
# Calculation (summing DOWN each column):
# Column 0: 1 + 22 + 66 = 89
# Column 1: 22 + 33 + 77 = 132
# Column 2: 33 + 44 + 88 = 165
# Column 3: 44 + 55 + 99 = 198
# 
# Visual:
#    Col0  Col1  Col2  Col3
#     ↓     ↓     ↓     ↓
#   [ 1    22    33    44]
#   [22    33    44    55]
#   [66    77    88    99]
#    ↓     ↓     ↓     ↓
#   [89   132   165   198]
# 
# Result shape: (4,) - one sum for each of 4 columns
# Original shape: (3, 4) → axis=0 collapsed → (4,)

# =============================================================================
# OPERATION 3: np.sum() WITH axis=1 - SUM ALONG COLUMNS (ROW-WISE SUM)
# =============================================================================
# ENGLISH:
# - What it does: Sums values ACROSS each row, collapsing the column dimension.
#   Results in one sum per row.
# - Syntax: np.sum(array, axis=1) or array.sum(axis=1)
# - Why use it: Essential for computing per-sample statistics in ML datasets
#   where rows are samples and columns are features
# - axis=1 behavior:
#   * Operates along axis 1 (columns) - goes ACROSS horizontally
#   * Collapses column dimension: (3, 4) → (3,)
#   * Returns array with one value per row
#   * Each result is sum of all columns for that row
# - Common use cases:
#   * Computing total/sum per sample across all features
#   * Row-wise aggregation in datasets
#   * Calculating total score per student (rows=students, cols=subjects)
#   * Sum of predictions per sample across classes
#   * Image processing: sum across width (all columns)
# - Example:
#   data = np.array([[10, 20, 30],
#                    [40, 50, 60],
#                    [70, 80, 90]])
#   row_sums = np.sum(data, axis=1)
#   # [60, 150, 240] = [10+20+30, 40+50+60, 70+80+90]
#   
#   # ML example: Total feature value per sample
#   X = np.array([[1.5, 2.3, 3.1],   # Sample 1
#                 [2.1, 1.9, 2.8],   # Sample 2
#                 [1.8, 2.5, 3.2]])  # Sample 3
#   sample_totals = np.sum(X, axis=1)
#   # [6.9, 6.8, 7.5] - total for each sample across all features
#   
#   # Probability normalization
#   logits = np.array([[2.0, 1.0, 0.5],
#                      [1.5, 2.5, 1.0]])  # 2 samples, 3 classes
#   exp_logits = np.exp(logits)
#   row_sums = np.sum(exp_logits, axis=1, keepdims=True)
#   probabilities = exp_logits / row_sums  # Softmax normalization
#   
#   # Student scores: Total per student
#   scores = np.array([[85, 90, 78],   # Student 1: Math, Science, English
#                      [92, 88, 95],   # Student 2
#                      [78, 85, 82]])  # Student 3
#   total_scores = np.sum(scores, axis=1)
#   # [253, 275, 245] - each student's total score
#
# HINGLISH:
# - Yeh kya karta hai: Har row ko ACROSS sum karta hai, column dimension
#   collapse karke. Har row ke liye ek sum milta hai.
# - Syntax: np.sum(array, axis=1) ya array.sum(axis=1)
# - Kab use karein: ML datasets mein per-sample statistics compute karne ke
#   liye essential hai jahan rows samples hain aur columns features hain
# - axis=1 behavior:
#   * Axis 1 (columns) ke along operate karta hai - horizontally ACROSS jaata hai
#   * Column dimension collapse: (3, 4) → (3,)
#   * Har row ke liye ek value ka array return karta hai
#   * Har result us row ke saare columns ka sum hai
# - Common use cases:
#   * Saare features across har sample ka total/sum compute karna
#   * Datasets mein row-wise aggregation
#   * Har student ka total score calculate karna (rows=students, cols=subjects)
#   * Classes across har sample ke predictions ka sum
#   * Image processing: width across sum (saare columns)
# - Example:
#   data = np.array([[10, 20, 30],
#                    [40, 50, 60],
#                    [70, 80, 90]])
#   row_sums = np.sum(data, axis=1)
#   # [60, 150, 240] = [10+20+30, 40+50+60, 70+80+90]
#   
#   # ML example: Har sample ka total feature value
#   X = np.array([[1.5, 2.3, 3.1],   # Sample 1
#                 [2.1, 1.9, 2.8],   # Sample 2
#                 [1.8, 2.5, 3.2]])  # Sample 3
#   sample_totals = np.sum(X, axis=1)
#   # [6.9, 6.8, 7.5] - saare features across har sample ka total
#   
#   # Probability normalization
#   logits = np.array([[2.0, 1.0, 0.5],
#                      [1.5, 2.5, 1.0]])  # 2 samples, 3 classes
#   exp_logits = np.exp(logits)
#   row_sums = np.sum(exp_logits, axis=1, keepdims=True)
#   probabilities = exp_logits / row_sums  # Softmax normalization
#   
#   # Student scores: Har student ka total
#   scores = np.array([[85, 90, 78],   # Student 1: Math, Science, English
#                      [92, 88, 95],   # Student 2
#                      [78, 85, 82]])  # Student 3
#   total_scores = np.sum(scores, axis=1)
#   # [253, 275, 245] - har student ka total score
# -----------------------------------------------------------------------------

sum_of_rows = np.sum(arr2D, axis=1)
print(sum_of_rows)
# Output: [100 154 330]
# 
# Calculation (summing ACROSS each row):
# Row 0: 1 + 22 + 33 + 44 = 100
# Row 1: 22 + 33 + 44 + 55 = 154
# Row 2: 66 + 77 + 88 + 99 = 330
# 
# Visual:
#   Row 0: [ 1  22  33  44] → 100
#           ←───────────→
#   Row 1: [22  33  44  55] → 154
#           ←───────────→
#   Row 2: [66  77  88  99] → 330
#           ←───────────→
# 
# Result shape: (3,) - one sum for each of 3 rows
# Original shape: (3, 4) → axis=1 collapsed → (3,)

# =============================================================================
# OTHER COMMON AGGREGATION FUNCTIONS WITH AXIS
# =============================================================================
# ENGLISH:
# All these functions work the same way with axis parameter:
# 
# 1. np.mean(array, axis=None/0/1) - Average
#    mean_cols = np.mean(arr2D, axis=0)  # Mean of each column
#    mean_rows = np.mean(arr2D, axis=1)  # Mean of each row
# 
# 2. np.max(array, axis=None/0/1) - Maximum value
#    max_cols = np.max(arr2D, axis=0)  # Max in each column
#    max_rows = np.max(arr2D, axis=1)  # Max in each row
# 
# 3. np.min(array, axis=None/0/1) - Minimum value
#    min_cols = np.min(arr2D, axis=0)  # Min in each column
#    min_rows = np.min(arr2D, axis=1)  # Min in each row
# 
# 4. np.std(array, axis=None/0/1) - Standard deviation
#    std_cols = np.std(arr2D, axis=0)  # Std dev of each column (feature)
#    std_rows = np.std(arr2D, axis=1)  # Std dev of each row (sample)
# 
# 5. np.var(array, axis=None/0/1) - Variance
#    var_cols = np.var(arr2D, axis=0)  # Variance of each column
# 
# 6. np.median(array, axis=None/0/1) - Median value
#    median_cols = np.median(arr2D, axis=0)  # Median of each column
# 
# 7. np.argmax(array, axis=None/0/1) - Index of maximum
#    argmax_cols = np.argmax(arr2D, axis=0)  # Row index of max in each col
#    argmax_rows = np.argmax(arr2D, axis=1)  # Col index of max in each row
# 
# 8. np.argmin(array, axis=None/0/1) - Index of minimum
#    argmin_rows = np.argmin(arr2D, axis=1)  # Col index of min in each row
# 
# 9. np.prod(array, axis=None/0/1) - Product of elements
#    prod_rows = np.prod(arr2D, axis=1)  # Product across each row
# 
# 10. np.cumsum(array, axis=None/0/1) - Cumulative sum
#     cumsum_cols = np.cumsum(arr2D, axis=0)  # Running sum down columns
#
# HINGLISH:
# Yeh saare functions axis parameter ke saath same tarah kaam karte hain:
# 
# 1. np.mean(array, axis=None/0/1) - Average
#    mean_cols = np.mean(arr2D, axis=0)  # Har column ka mean
#    mean_rows = np.mean(arr2D, axis=1)  # Har row ka mean
# 
# 2. np.max(array, axis=None/0/1) - Maximum value
#    max_cols = np.max(arr2D, axis=0)  # Har column mein max
#    max_rows = np.max(arr2D, axis=1)  # Har row mein max
# 
# 3. np.min(array, axis=None/0/1) - Minimum value
#    min_cols = np.min(arr2D, axis=0)  # Har column mein min
#    min_rows = np.min(arr2D, axis=1)  # Har row mein min
# 
# 4. np.std(array, axis=None/0/1) - Standard deviation
#    std_cols = np.std(arr2D, axis=0)  # Har column (feature) ka std dev
#    std_rows = np.std(arr2D, axis=1)  # Har row (sample) ka std dev
# 
# 5. np.var(array, axis=None/0/1) - Variance
#    var_cols = np.var(arr2D, axis=0)  # Har column ka variance
# 
# 6. np.median(array, axis=None/0/1) - Median value
#    median_cols = np.median(arr2D, axis=0)  # Har column ka median
# 
# 7. np.argmax(array, axis=None/0/1) - Maximum ka index
#    argmax_cols = np.argmax(arr2D, axis=0)  # Har col mein max ka row index
#    argmax_rows = np.argmax(arr2D, axis=1)  # Har row mein max ka col index
# 
# 8. np.argmin(array, axis=None/0/1) - Minimum ka index
#    argmin_rows = np.argmin(arr2D, axis=1)  # Har row mein min ka col index
# 
# 9. np.prod(array, axis=None/0/1) - Elements ka product
#    prod_rows = np.prod(arr2D, axis=1)  # Har row across product
# 
# 10. np.cumsum(array, axis=None/0/1) - Cumulative sum
#     cumsum_cols = np.cumsum(arr2D, axis=0)  # Columns neeche running sum
# =============================================================================

# =============================================================================
# PRACTICAL ML EXAMPLE: FEATURE NORMALIZATION
# =============================================================================
# ENGLISH:
# Real-world scenario: Normalizing features (Z-score normalization)
#
# # Dataset: 4 samples, 3 features
# X = np.array([[1.0, 2.0, 3.0],
#               [4.0, 5.0, 6.0],
#               [7.0, 8.0, 9.0],
#               [10.0, 11.0, 12.0]])
# 
# # Compute mean and std for each feature (axis=0)
# feature_means = np.mean(X, axis=0)  # [5.5, 6.5, 7.5]
# feature_stds = np.std(X, axis=0)    # [3.416, 3.416, 3.416]
# 
# # Normalize: (X - mean) / std for each feature
# X_normalized = (X - feature_means) / feature_stds
# 
# # Verify normalization worked
# print(np.mean(X_normalized, axis=0))  # ~[0, 0, 0] - mean centered
# print(np.std(X_normalized, axis=0))   # ~[1, 1, 1] - unit variance
#
# HINGLISH:
# Real-world scenario: Features normalize karna (Z-score normalization)
#
# # Dataset: 4 samples, 3 features
# X = np.array([[1.0, 2.0, 3.0],
#               [4.0, 5.0, 6.0],
#               [7.0, 8.0, 9.0],
#               [10.0, 11.0, 12.0]])
# 
# # Har feature ke liye mean aur std compute karo (axis=0)
# feature_means = np.mean(X, axis=0)  # [5.5, 6.5, 7.5]
# feature_stds = np.std(X, axis=0)    # [3.416, 3.416, 3.416]
# 
# # Normalize karo: har feature ke liye (X - mean) / std
# X_normalized = (X - feature_means) / feature_stds
# 
# # Verify karo ki normalization kaam kiya
# print(np.mean(X_normalized, axis=0))  # ~[0, 0, 0] - mean centered
# print(np.std(X_normalized, axis=0))   # ~[1, 1, 1] - unit variance
# =============================================================================


# =============================================================================
# 3D ARRAYS (TENSORS) - MULTI-DIMENSIONAL DATA STRUCTURES
# =============================================================================

# =============================================================================
# CREATING A 3D ARRAY (TENSOR)
# =============================================================================
# ENGLISH:
# - What it does: Creates a 3-dimensional array (tensor) with shape (2, 3, 3).
#   This is essentially a stack of 2D matrices.
# - Why use it: 3D arrays are FUNDAMENTAL in deep learning for representing:
#   * RGB images: (height, width, channels) - e.g., (224, 224, 3)
#   * Video frames: (frames, height, width) or (frames, height, width, channels)
#   * Batches of data: (batch_size, samples, features)
#   * Convolutional layer outputs: (batch, height, width, filters)
#   * Time series batches: (batch, timesteps, features)
# - Structure breakdown:
#   * First dimension (axis 0): "depth" or "layers" - 2 matrices stacked
#   * Second dimension (axis 1): rows within each matrix - 3 rows
#   * Third dimension (axis 2): columns within each matrix - 3 columns
# - Shape (2, 3, 3) means:
#   * 2 "layers" or "matrices" or "channels"
#   * Each matrix has 3 rows
#   * Each matrix has 3 columns
#   * Total elements: 2 × 3 × 3 = 18 elements
# - Think of it as:
#   * A box containing 2 sheets of paper (layers/matrices)
#   * Each sheet has a 3×3 grid of numbers
#   * You need 3 indices to locate any number: [which_layer, which_row, which_column]
# - Common use cases:
#   * Color images: arr[height, width, RGB_channel]
#   * Batch of grayscale images: arr[image_num, height, width]
#   * Video data: arr[frame, height, width]
#   * CNN feature maps: arr[batch, feature_map_h, feature_map_w]
#   * Sequence batches: arr[batch, sequence_length, feature_dim]
# - Example:
#   # RGB image (100×100 pixels, 3 color channels)
#   image = np.zeros((100, 100, 3))  # shape: (100, 100, 3)
#   # Accessing pixel at row 50, col 30, red channel
#   red_value = image[50, 30, 0]
#   
#   # Batch of 32 grayscale images (28×28 each)
#   batch = np.zeros((32, 28, 28))  # shape: (32, 28, 28)
#   # Accessing 5th image in batch
#   fifth_image = batch[4, :, :]  # shape: (28, 28)
#   
#   # Time series: 64 samples, each with 100 timesteps, 10 features
#   timeseries = np.zeros((64, 100, 10))  # shape: (64, 100, 10)
#   # Accessing first sample's data at timestep 50
#   features_at_t50 = timeseries[0, 50, :]  # shape: (10,)
#
# HINGLISH:
# - Yeh kya karta hai: Shape (2, 3, 3) ke saath ek 3-dimensional array (tensor)
#   banata hai. Yeh basically 2D matrices ka stack hai.
# - Kab use karein: 3D arrays deep learning mein represent karne ke liye
#   FUNDAMENTAL hain:
#   * RGB images: (height, width, channels) - jaise (224, 224, 3)
#   * Video frames: (frames, height, width) ya (frames, height, width, channels)
#   * Data ke batches: (batch_size, samples, features)
#   * Convolutional layer outputs: (batch, height, width, filters)
#   * Time series batches: (batch, timesteps, features)
# - Structure breakdown:
#   * Pehla dimension (axis 0): "depth" ya "layers" - 2 matrices stacked
#   * Doosra dimension (axis 1): har matrix ke andar rows - 3 rows
#   * Teesra dimension (axis 2): har matrix ke andar columns - 3 columns
# - Shape (2, 3, 3) ka matlab:
#   * 2 "layers" ya "matrices" ya "channels"
#   * Har matrix mein 3 rows hain
#   * Har matrix mein 3 columns hain
#   * Total elements: 2 × 3 × 3 = 18 elements
# - Aise socho:
#   * Ek box jismein 2 sheets of paper hain (layers/matrices)
#   * Har sheet par 3×3 grid of numbers hai
#   * Kisi bhi number ko locate karne ke liye 3 indices chahiye: [kaun_si_layer, kaun_si_row, kaun_sa_column]
# - Common use cases:
#   * Color images: arr[height, width, RGB_channel]
#   * Grayscale images ka batch: arr[image_num, height, width]
#   * Video data: arr[frame, height, width]
#   * CNN feature maps: arr[batch, feature_map_h, feature_map_w]
#   * Sequence batches: arr[batch, sequence_length, feature_dim]
# - Example:
#   # RGB image (100×100 pixels, 3 color channels)
#   image = np.zeros((100, 100, 3))  # shape: (100, 100, 3)
#   # Row 50, col 30, red channel par pixel access karna
#   red_value = image[50, 30, 0]
#   
#   # 32 grayscale images ka batch (har ek 28×28)
#   batch = np.zeros((32, 28, 28))  # shape: (32, 28, 28)
#   # Batch mein 5th image access karna
#   fifth_image = batch[4, :, :]  # shape: (28, 28)
#   
#   # Time series: 64 samples, har ek mein 100 timesteps, 10 features
#   timeseries = np.zeros((64, 100, 10))  # shape: (64, 100, 10)
#   # Pehle sample ka data timestep 50 par access karna
#   features_at_t50 = timeseries[0, 50, :]  # shape: (10,)
# -----------------------------------------------------------------------------

arr3D = np.array([
    # Matrix 0 (First layer/depth) - axis 0, index 0
    [[1, 2, 3],      # Row 0 of Matrix 0
     [4, 5, 6],      # Row 1 of Matrix 0
     [7, 8, 9]],     # Row 2 of Matrix 0
    
    # Matrix 1 (Second layer/depth) - axis 0, index 1
    [[11, 22, 33],   # Row 0 of Matrix 1
     [44, 55, 66],   # Row 1 of Matrix 1
     [77, 88, 99]]   # Row 2 of Matrix 1
])

# =============================================================================
# VISUALIZING 3D ARRAY STRUCTURE
# =============================================================================
# ENGLISH:
# Visual representation of arr3D with shape (2, 3, 3):
#
# Think of it as 2 layers stacked on top of each other:
#
# Layer 0 (arr3D[0]):          Layer 1 (arr3D[1]):
# ┌─────────────┐              ┌─────────────┐
# │ 1   2   3  │              │ 11  22  33 │
# │ 4   5   6  │              │ 44  55  66 │
# │ 7   8   9  │              │ 77  88  99 │
# └─────────────┘              └─────────────┘
#
# 3D coordinates system:
# - arr3D[layer, row, column]
# - arr3D[axis_0, axis_1, axis_2]
# - arr3D[depth, height, width]
#
# Examples of accessing elements:
# arr3D[0, 0, 0] = 1   (Layer 0, Row 0, Col 0)
# arr3D[0, 1, 1] = 5   (Layer 0, Row 1, Col 1)
# arr3D[1, 0, 0] = 11  (Layer 1, Row 0, Col 0)
# arr3D[1, 1, 1] = 55  (Layer 1, Row 1, Col 1)
# arr3D[1, 2, 2] = 99  (Layer 1, Row 2, Col 2)
#
# HINGLISH:
# Shape (2, 3, 3) ke saath arr3D ka visual representation:
#
# Ise 2 layers ki tarah socho jo ek doosre ke upar stacked hain:
#
# Layer 0 (arr3D[0]):          Layer 1 (arr3D[1]):
# ┌─────────────┐              ┌─────────────┐
# │ 1   2   3  │              │ 11  22  33 │
# │ 4   5   6  │              │ 44  55  66 │
# │ 7   8   9  │              │ 77  88  99 │
# └─────────────┘              └─────────────┘
#
# 3D coordinates system:
# - arr3D[layer, row, column]
# - arr3D[axis_0, axis_1, axis_2]
# - arr3D[depth, height, width]
#
# Elements access karne ke examples:
# arr3D[0, 0, 0] = 1   (Layer 0, Row 0, Col 0)
# arr3D[0, 1, 1] = 5   (Layer 0, Row 1, Col 1)
# arr3D[1, 0, 0] = 11  (Layer 1, Row 0, Col 0)
# arr3D[1, 1, 1] = 55  (Layer 1, Row 1, Col 1)
# arr3D[1, 2, 2] = 99  (Layer 1, Row 2, Col 2)
# -----------------------------------------------------------------------------

print(arr3D, arr3D.shape)
# Output:
# [[[ 1  2  3]
#   [ 4  5  6]
#   [ 7  8  9]]
#
#  [[11 22 33]
#   [44 55 66]
#   [77 88 99]]] (2, 3, 3)
#
# Shape (2, 3, 3) breakdown:
# - First number (2): Number of 3×3 matrices (depth/layers)
# - Second number (3): Number of rows in each matrix
# - Third number (3): Number of columns in each matrix
# - Total elements: 2 × 3 × 3 = 18

# =============================================================================
# 3D ARRAY INDEXING - ACCESSING INDIVIDUAL ELEMENTS
# =============================================================================
# ENGLISH:
# - What it does: Accesses a single element in a 3D array using three indices:
#   [layer_index, row_index, column_index]
# - Syntax: arr3D[i, j, k] where:
#   * i = which layer/matrix (axis 0) - ranges from 0 to 1 in this case
#   * j = which row within that matrix (axis 1) - ranges from 0 to 2
#   * k = which column within that row (axis 2) - ranges from 0 to 2
# - Why use it: Essential for accessing specific values in multi-dimensional
#   data like individual pixel values in images, specific features in batches,
#   or particular timesteps in sequences
# - Indexing order matters:
#   * Always: [slowest-changing, ..., fastest-changing]
#   * For images: [batch, height, width, channel] or [height, width, channel]
#   * For video: [frame, height, width] or [frame, height, width, channel]
#   * For batches: [sample, feature_1, feature_2, ...]
# - Common use cases:
#   * Getting pixel value: image[y, x, channel]
#   * Accessing specific frame: video[frame_num, y, x]
#   * Getting element from batch: batch[sample_idx, row, col]
#   * Extracting feature value: data[sample, timestep, feature]
# - Example:
#   # RGB image (256×256, 3 channels)
#   img = np.random.rand(256, 256, 3)
#   red_pixel = img[100, 150, 0]    # Red value at pixel (100, 150)
#   green_pixel = img[100, 150, 1]  # Green value at same pixel
#   blue_pixel = img[100, 150, 2]   # Blue value at same pixel
#   
#   # Batch of images (32 images, 28×28 each)
#   batch = np.random.rand(32, 28, 28)
#   pixel_value = batch[5, 10, 15]  # Pixel at (10,15) in 6th image
#   
#   # Time series (10 samples, 100 timesteps, 5 features)
#   ts_data = np.random.rand(10, 100, 5)
#   feature_val = ts_data[0, 50, 2]  # 3rd feature at timestep 50, sample 0
#
# HINGLISH:
# - Yeh kya karta hai: Teen indices use karke 3D array mein ek single element
#   access karta hai: [layer_index, row_index, column_index]
# - Syntax: arr3D[i, j, k] jahan:
#   * i = kaun si layer/matrix (axis 0) - is case mein 0 se 1 tak
#   * j = us matrix mein kaun si row (axis 1) - 0 se 2 tak
#   * k = us row mein kaun sa column (axis 2) - 0 se 2 tak
# - Kab use karein: Multi-dimensional data mein specific values access karne
#   ke liye essential hai jaise images mein individual pixel values, batches
#   mein specific features, ya sequences mein particular timesteps
# - Indexing order important hai:
#   * Hamesha: [slowest-changing, ..., fastest-changing]
#   * Images ke liye: [batch, height, width, channel] ya [height, width, channel]
#   * Video ke liye: [frame, height, width] ya [frame, height, width, channel]
#   * Batches ke liye: [sample, feature_1, feature_2, ...]
# - Common use cases:
#   * Pixel value lena: image[y, x, channel]
#   * Specific frame access karna: video[frame_num, y, x]
#   * Batch se element lena: batch[sample_idx, row, col]
#   * Feature value extract karna: data[sample, timestep, feature]
# - Example:
#   # RGB image (256×256, 3 channels)
#   img = np.random.rand(256, 256, 3)
#   red_pixel = img[100, 150, 0]    # Pixel (100, 150) par red value
#   green_pixel = img[100, 150, 1]  # Same pixel par green value
#   blue_pixel = img[100, 150, 2]   # Same pixel par blue value
#   
#   # Images ka batch (32 images, har ek 28×28)
#   batch = np.random.rand(32, 28, 28)
#   pixel_value = batch[5, 10, 15]  # 6th image mein (10,15) par pixel
#   
#   # Time series (10 samples, 100 timesteps, 5 features)
#   ts_data = np.random.rand(10, 100, 5)
#   feature_val = ts_data[0, 50, 2]  # Sample 0 mein timestep 50 par 3rd feature
# -----------------------------------------------------------------------------

print(arr3D[1, 1, 1])
# Output: 55
#
# Breakdown of indexing arr3D[1, 1, 1]:
# - Index 0 (layer/matrix): 1 → Select the SECOND matrix (Matrix 1)
# - Index 1 (row): 1 → Select the SECOND row within Matrix 1
# - Index 2 (column): 1 → Select the SECOND column within that row
#
# Visual trace:
# Step 1: arr3D[1] → Select Matrix 1
#   [[11, 22, 33],
#    [44, 55, 66],  ← We want this row
#    [77, 88, 99]]
#
# Step 2: arr3D[1, 1] → Select Row 1 of Matrix 1
#   [44, 55, 66]
#        ↑
#     We want this column
#
# Step 3: arr3D[1, 1, 1] → Select Column 1
#   55 ← Final result

# =============================================================================
# MORE 3D INDEXING EXAMPLES
# =============================================================================
# ENGLISH:
# Accessing different elements to understand indexing:
#
# # Get first element (top-left of first matrix)
# print(arr3D[0, 0, 0])  # Output: 1
#
# # Get last element (bottom-right of second matrix)
# print(arr3D[1, 2, 2])  # Output: 99
#
# # Get entire first matrix (all rows and columns of layer 0)
# print(arr3D[0])        # Output: [[1,2,3], [4,5,6], [7,8,9]]
# # Or equivalently:
# print(arr3D[0, :, :])  # Same output
#
# # Get entire second matrix
# print(arr3D[1])        # Output: [[11,22,33], [44,55,66], [77,88,99]]
#
# # Get first row from first matrix
# print(arr3D[0, 0])     # Output: [1, 2, 3]
# # Or equivalently:
# print(arr3D[0, 0, :])  # Same output
#
# # Get middle row from second matrix
# print(arr3D[1, 1])     # Output: [44, 55, 66]
#
# # Get first column from all matrices (all layers, all rows, column 0)
# print(arr3D[:, :, 0])  # Output: [[1,4,7], [11,44,77]]
#
# # Get middle element from all matrices (all layers, row 1, col 1)
# print(arr3D[:, 1, 1])  # Output: [5, 55]
#
# # Using negative indices
# print(arr3D[-1, -1, -1])  # Output: 99 (last layer, last row, last column)
#
# HINGLISH:
# Indexing samajhne ke liye alag elements access karna:
#
# # Pehla element lo (pehli matrix ka top-left)
# print(arr3D[0, 0, 0])  # Output: 1
#
# # Aakhri element lo (doosri matrix ka bottom-right)
# print(arr3D[1, 2, 2])  # Output: 99
#
# # Poori pehli matrix lo (layer 0 ki saari rows aur columns)
# print(arr3D[0])        # Output: [[1,2,3], [4,5,6], [7,8,9]]
# # Ya equivalently:
# print(arr3D[0, :, :])  # Same output
#
# # Poori doosri matrix lo
# print(arr3D[1])        # Output: [[11,22,33], [44,55,66], [77,88,99]]
#
# # Pehli matrix se pehli row lo
# print(arr3D[0, 0])     # Output: [1, 2, 3]
# # Ya equivalently:
# print(arr3D[0, 0, :])  # Same output
#
# # Doosri matrix se middle row lo
# print(arr3D[1, 1])     # Output: [44, 55, 66]
#
# # Saari matrices se pehla column lo (saari layers, saari rows, column 0)
# print(arr3D[:, :, 0])  # Output: [[1,4,7], [11,44,77]]
#
# # Saari matrices se middle element lo (saari layers, row 1, col 1)
# print(arr3D[:, 1, 1])  # Output: [5, 55]
#
# # Negative indices use karna
# print(arr3D[-1, -1, -1])  # Output: 99 (last layer, last row, last column)
# =============================================================================

# =============================================================================
# 3D ARRAY AXIS OPERATIONS
# =============================================================================
# ENGLISH:
# Understanding axis operations in 3D arrays (CRITICAL for ML):
#
# For arr3D with shape (2, 3, 3):
# - axis=0: Operates along depth/layers (collapses layers)
# - axis=1: Operates along rows (collapses rows)
# - axis=2: Operates along columns (collapses columns)
#
# Examples:
#
# # Sum along axis 0 (collapse layers) - shape (2,3,3) → (3,3)
# sum_axis0 = np.sum(arr3D, axis=0)
# # Adds corresponding elements from both matrices
# # [[1+11, 2+22, 3+33],
# #  [4+44, 5+55, 6+66],
# #  [7+77, 8+88, 9+99]]
# # = [[12, 24, 36], [48, 60, 72], [84, 96, 108]]
#
# # Sum along axis 1 (collapse rows) - shape (2,3,3) → (2,3)
# sum_axis1 = np.sum(arr3D, axis=1)
# # For each matrix, sums down the rows (column-wise sums)
# # Matrix 0: [1+4+7, 2+5+8, 3+6+9] = [12, 15, 18]
# # Matrix 1: [11+44+77, 22+55+88, 33+66+99] = [132, 165, 198]
# # Result: [[12, 15, 18], [132, 165, 198]]
#
# # Sum along axis 2 (collapse columns) - shape (2,3,3) → (2,3)
# sum_axis2 = np.sum(arr3D, axis=2)
# # For each matrix, sums across the columns (row-wise sums)
# # Matrix 0: [1+2+3, 4+5+6, 7+8+9] = [6, 15, 24]
# # Matrix 1: [11+22+33, 44+55+66, 77+88+99] = [66, 165, 264]
# # Result: [[6, 15, 24], [66, 165, 264]]
#
# # Sum along multiple axes
# sum_axis_01 = np.sum(arr3D, axis=(0, 1))  # Collapse layers and rows → (3,)
# sum_all = np.sum(arr3D)  # Collapse all axes → scalar
#
# HINGLISH:
# 3D arrays mein axis operations samajhna (ML ke liye CRITICAL):
#
# Shape (2, 3, 3) wale arr3D ke liye:
# - axis=0: Depth/layers ke along operate (layers collapse)
# - axis=1: Rows ke along operate (rows collapse)
# - axis=2: Columns ke along operate (columns collapse)
#
# Examples:
#
# # axis 0 ke along sum (layers collapse) - shape (2,3,3) → (3,3)
# sum_axis0 = np.sum(arr3D, axis=0)
# # Dono matrices se corresponding elements add karta hai
# # [[1+11, 2+22, 3+33],
# #  [4+44, 5+55, 6+66],
# #  [7+77, 8+88, 9+99]]
# # = [[12, 24, 36], [48, 60, 72], [84, 96, 108]]
#
# # axis 1 ke along sum (rows collapse) - shape (2,3,3) → (2,3)
# sum_axis1 = np.sum(arr3D, axis=1)
# # Har matrix ke liye, rows ko neeche sum karta hai (column-wise sums)
# # Matrix 0: [1+4+7, 2+5+8, 3+6+9] = [12, 15, 18]
# # Matrix 1: [11+44+77, 22+55+88, 33+66+99] = [132, 165, 198]
# # Result: [[12, 15, 18], [132, 165, 198]]
#
# # axis 2 ke along sum (columns collapse) - shape (2,3,3) → (2,3)
# sum_axis2 = np.sum(arr3D, axis=2)
# # Har matrix ke liye, columns ko across sum karta hai (row-wise sums)
# # Matrix 0: [1+2+3, 4+5+6, 7+8+9] = [6, 15, 24]
# # Matrix 1: [11+22+33, 44+55+66, 77+88+99] = [66, 165, 264]
# # Result: [[6, 15, 24], [66, 165, 264]]
#
# # Multiple axes ke along sum
# sum_axis_01 = np.sum(arr3D, axis=(0, 1))  # Layers aur rows collapse → (3,)
# sum_all = np.sum(arr3D)  # Saare axes collapse → scalar
# =============================================================================

# =============================================================================
# PRACTICAL ML EXAMPLE: RGB IMAGE OPERATIONS
# =============================================================================
# ENGLISH:
# Real-world scenario: Processing an RGB image
#
# # Simulate RGB image: 4×4 pixels, 3 color channels (R, G, B)
# rgb_image = np.array([
#     # Pixel row 0
#     [[255, 0, 0],    [255, 0, 0],    [0, 255, 0],    [0, 255, 0]],
#     # Pixel row 1
#     [[255, 0, 0],    [255, 0, 0],    [0, 255, 0],    [0, 255, 0]],
#     # Pixel row 2
#     [[0, 0, 255],    [0, 0, 255],    [255, 255, 0],  [255, 255, 0]],
#     # Pixel row 3
#     [[0, 0, 255],    [0, 0, 255],    [255, 255, 0],  [255, 255, 0]]
# ])  # Shape: (4, 4, 3) - height=4, width=4, channels=3
#
# # Get red channel only (extract plane)
# red_channel = rgb_image[:, :, 0]  # Shape: (4, 4)
#
# # Get green channel
# green_channel = rgb_image[:, :, 1]  # Shape: (4, 4)
#
# # Get blue channel
# blue_channel = rgb_image[:, :, 2]  # Shape: (4, 4)
#
# # Convert to grayscale (average of R, G, B)
# grayscale = np.mean(rgb_image, axis=2)  # Shape: (4, 4)
#
# # Get pixel at position (2, 3) - all channels
# pixel_rgb = rgb_image[2, 3, :]  # Shape: (3,) - [R, G, B] values
#
# # Get average intensity per channel across entire image
# avg_red = np.mean(rgb_image[:, :, 0])
# avg_green = np.mean(rgb_image[:, :, 1])
# avg_blue = np.mean(rgb_image[:, :, 2])
# # Or more efficiently:
# channel_averages = np.mean(rgb_image, axis=(0, 1))  # Shape: (3,)
#
# HINGLISH:
# Real-world scenario: RGB image process karna
#
# # RGB image simulate karo: 4×4 pixels, 3 color channels (R, G, B)
# rgb_image = np.array([
#     # Pixel row 0
#     [[255, 0, 0],    [255, 0, 0],    [0, 255, 0],    [0, 255, 0]],
#     # Pixel row 1
#     [[255, 0, 0],    [255, 0, 0],    [0, 255, 0],    [0, 255, 0]],
#     # Pixel row 2
#     [[0, 0, 255],    [0, 0, 255],    [255, 255, 0],  [255, 255, 0]],
#     # Pixel row 3
#     [[0, 0, 255],    [0, 0, 255],    [255, 255, 0],  [255, 255, 0]]
# ])  # Shape: (4, 4, 3) - height=4, width=4, channels=3
#
# # Sirf red channel lo (plane extract karo)
# red_channel = rgb_image[:, :, 0]  # Shape: (4, 4)
#
# # Green channel lo
# green_channel = rgb_image[:, :, 1]  # Shape: (4, 4)
#
# # Blue channel lo
# blue_channel = rgb_image[:, :, 2]  # Shape: (4, 4)
#
# # Grayscale mein convert karo (R, G, B ka average)
# grayscale = np.mean(rgb_image, axis=2)  # Shape: (4, 4)
#
# # Position (2, 3) par pixel lo - saare channels
# pixel_rgb = rgb_image[2, 3, :]  # Shape: (3,) - [R, G, B] values
#
# # Poori image mein har channel ka average intensity lo
# avg_red = np.mean(rgb_image[:, :, 0])
# avg_green = np.mean(rgb_image[:, :, 1])
# avg_blue = np.mean(rgb_image[:, :, 2])
# # Ya zyada efficiently:
# channel_averages = np.mean(rgb_image, axis=(0, 1))  # Shape: (3,)
# =============================================================================

# =============================================================================
# 3D SLICING EXAMPLES
# =============================================================================
# ENGLISH:
# Slicing works independently on each axis:
#
# # Get both matrices, first 2 rows, all columns
# slice1 = arr3D[:, :2, :]
# # Shape: (2, 2, 3)
#
# # Get first matrix, all rows, last 2 columns
# slice2 = arr3D[0, :, -2:]
# # Shape: (3, 2)
#
# # Get second matrix, middle row only
# slice3 = arr3D[1, 1, :]
# # Shape: (3,) - [44, 55, 66]
#
# # Get diagonal elements from both matrices
# diag0 = arr3D[0, [0,1,2], [0,1,2]]  # [1, 5, 9]
# diag1 = arr3D[1, [0,1,2], [0,1,2]]  # [11, 55, 99]
#
# HINGLISH:
# Har axis par independently slicing kaam karti hai:
#
# # Dono matrices lo, pehli 2 rows, saare columns
# slice1 = arr3D[:, :2, :]
# # Shape: (2, 2, 3)
#
# # Pehli matrix lo, saari rows, aakhri 2 columns
# slice2 = arr3D[0, :, -2:]
# # Shape: (3, 2)
#
# # Doosri matrix lo, sirf middle row
# slice3 = arr3D[1, 1, :]
# # Shape: (3,) - [44, 55, 66]
#
# # Dono matrices se diagonal elements lo
# diag0 = arr3D[0, [0,1,2], [0,1,2]]  # [1, 5, 9]
# diag1 = arr3D[1, [0,1,2], [0,1,2]]  # [11, 55, 99]
# =============================================================================
```

**Quick Reference: 3D Array Structure**

| Dimension | Name | Common Interpretations | Index Range (this example) |
|-----------|------|------------------------|---------------------------|
| axis=0 | Depth/Layer/Batch | Matrices, Images, Samples | 0-1 (2 layers) |
| axis=1 | Height/Row | Vertical position | 0-2 (3 rows) |
| axis=2 | Width/Column | Horizontal position | 0-2 (3 columns) |

**Common 3D Shapes in ML:**
- **RGB Image**: `(height, width, 3)` - e.g., `(224, 224, 3)`
- **Batch of Grayscale**: `(batch, height, width)` - e.g., `(32, 28, 28)`
- **Batch of RGB**: `(batch, height, width, 3)` - e.g., `(32, 224, 224, 3)`
- **Time Series Batch**: `(batch, timesteps, features)` - e.g., `(64, 100, 10)`

**Indexing Pattern:**
```
arr3D[layer, row, column]
      ↑      ↑     ↑
   axis=0  axis=1 axis=2
   depth   height  width


# =============================================================================
# NUMPY VECTORIZATION - EFFICIENT ELEMENT-WISE OPERATIONS
# =============================================================================

# =============================================================================
# WHAT IS VECTORIZATION?
# =============================================================================
# ENGLISH:
# Vectorization is NumPy's ability to perform operations on entire arrays
# WITHOUT explicit Python loops. This is:
# - MUCH faster (10-100x) than Python loops because operations run in C
# - More readable and concise
# - Automatically parallelized on modern CPUs
# - The CORE reason NumPy exists and why it's essential for ML
#
# Instead of:
#   result = []
#   for i in range(len(arr)):
#       result.append(arr[i] ** 2)  # SLOW - Python loop
#
# You write:
#   result = arr ** 2  # FAST - vectorized operation
#
# HINGLISH:
# Vectorization NumPy ki ability hai poore arrays par operations perform karne
# ki BINA explicit Python loops ke. Yeh:
# - Python loops se BAHUT zyada fast hai (10-100x) kyunki operations C mein
#   run hote hain
# - Zyada readable aur concise hai
# - Modern CPUs par automatically parallelized hai
# - NumPy exist karne ki aur ML ke liye essential hone ki CORE reason hai
#
# Iske bajaye:
#   result = []
#   for i in range(len(arr)):
#       result.append(arr[i] ** 2)  # SLOW - Python loop
#
# Aap likhte hain:
#   result = arr ** 2  # FAST - vectorized operation
# -----------------------------------------------------------------------------

# =============================================================================
# CREATING ARRAYS FOR VECTORIZATION DEMONSTRATIONS
# =============================================================================
# ENGLISH:
# - What it does: Creates sample arrays to demonstrate vectorized operations
# - arr: 1D array with 5 elements
# - arr2: Another 1D array with 5 elements (same shape as arr)
# - arr3: 2D array with shape (2, 5) - demonstrates broadcasting
#
# HINGLISH:
# - Yeh kya karta hai: Vectorized operations demonstrate karne ke liye sample
#   arrays banata hai
# - arr: 5 elements wala 1D array
# - arr2: Doosra 1D array with 5 elements (arr jaisa hi shape)
# - arr3: Shape (2, 5) wala 2D array - broadcasting demonstrate karta hai
# -----------------------------------------------------------------------------

arr = np.array([1, 2, 3, 4, 5])
arr2 = np.array([5, 6, 7, 8, 9])
arr3 = np.array([[1, 2, 3, 4, 5],   # Row 0
                 [2, 3, 4, 5, 6]])  # Row 1

# =============================================================================
# OPERATION 1: ELEMENT-WISE EXPONENTIATION (arr ** 2)
# =============================================================================
# ENGLISH:
# - What it does: Squares each element in the array independently. The **
#   operator performs element-wise exponentiation.
# - Syntax: array ** power
# - Why use it: Common in ML for:
#   * Computing squared errors: (predictions - actual) ** 2
#   * Calculating variance and standard deviation
#   * Feature engineering: creating polynomial features
#   * L2 regularization: sum of squared weights
#   * Euclidean distance: sqrt(sum((x - y) ** 2))
# - How it works:
#   * Takes each element independently
#   * Raises it to the specified power
#   * Returns new array with same shape
#   * MUCH faster than Python loop
# - Performance comparison:
#   # Slow Python way (avoid this):
#   result = [x ** 2 for x in arr]  # List comprehension - slower
#   
#   # Fast NumPy way (use this):
#   result = arr ** 2  # Vectorized - 10-100x faster
# - Common use cases:
#   * Mean Squared Error (MSE): np.mean((y_pred - y_true) ** 2)
#   * L2 norm (magnitude): np.sqrt(np.sum(vector ** 2))
#   * Variance: np.mean((data - mean) ** 2)
#   * Polynomial features: X_squared = X ** 2
#   * Distance calculations in clustering/KNN
# - Example:
#   # Mean Squared Error calculation
#   predictions = np.array([2.5, 3.0, 4.2, 5.1])
#   actual = np.array([2.0, 3.5, 4.0, 5.0])
#   squared_errors = (predictions - actual) ** 2
#   # [0.25, 0.25, 0.04, 0.01]
#   mse = np.mean(squared_errors)  # 0.1375
#   
#   # L2 norm (Euclidean length of vector)
#   vector = np.array([3, 4])
#   magnitude = np.sqrt(np.sum(vector ** 2))  # 5.0
#   
#   # Creating polynomial features
#   X = np.array([1, 2, 3, 4, 5])
#   X_poly = np.column_stack([X, X**2, X**3])
#   # [[1, 1, 1], [2, 4, 8], [3, 9, 27], [4, 16, 64], [5, 25, 125]]
#
# HINGLISH:
# - Yeh kya karta hai: Array ke har element ko independently square karta hai.
#   ** operator element-wise exponentiation perform karta hai.
# - Syntax: array ** power
# - Kab use karein: ML mein common hai:
#   * Squared errors compute karna: (predictions - actual) ** 2
#   * Variance aur standard deviation calculate karna
#   * Feature engineering: polynomial features banana
#   * L2 regularization: squared weights ka sum
#   * Euclidean distance: sqrt(sum((x - y) ** 2))
# - Kaise kaam karta hai:
#   * Har element ko independently leta hai
#   * Use specified power tak raise karta hai
#   * Same shape wala naya array return karta hai
#   * Python loop se BAHUT zyada fast hai
# - Performance comparison:
#   # Slow Python tarika (isse avoid karo):
#   result = [x ** 2 for x in arr]  # List comprehension - slower
#   
#   # Fast NumPy tarika (isse use karo):
#   result = arr ** 2  # Vectorized - 10-100x faster
# - Common use cases:
#   * Mean Squared Error (MSE): np.mean((y_pred - y_true) ** 2)
#   * L2 norm (magnitude): np.sqrt(np.sum(vector ** 2))
#   * Variance: np.mean((data - mean) ** 2)
#   * Polynomial features: X_squared = X ** 2
#   * Clustering/KNN mein distance calculations
# - Example:
#   # Mean Squared Error calculation
#   predictions = np.array([2.5, 3.0, 4.2, 5.1])
#   actual = np.array([2.0, 3.5, 4.0, 5.0])
#   squared_errors = (predictions - actual) ** 2
#   # [0.25, 0.25, 0.04, 0.01]
#   mse = np.mean(squared_errors)  # 0.1375
#   
#   # L2 norm (vector ki Euclidean length)
#   vector = np.array([3, 4])
#   magnitude = np.sqrt(np.sum(vector ** 2))  # 5.0
#   
#   # Polynomial features banana
#   X = np.array([1, 2, 3, 4, 5])
#   X_poly = np.column_stack([X, X**2, X**3])
#   # [[1, 1, 1], [2, 4, 8], [3, 9, 27], [4, 16, 64], [5, 25, 125]]
# -----------------------------------------------------------------------------

print(arr ** 2)
# Output: [ 1  4  9 16 25]
#
# Element-wise calculation:
# [1, 2, 3, 4, 5] ** 2
#  ↓  ↓  ↓  ↓  ↓
# [1² 2² 3² 4² 5²]
#  ↓  ↓  ↓  ↓  ↓
# [1, 4, 9, 16, 25]
#
# Each element is squared independently
# No loops needed - operation is vectorized

# =============================================================================
# OPERATION 2: SCALAR ADDITION (arr + scalar)
# =============================================================================
# ENGLISH:
# - What it does: Adds a scalar (single number) to every element in the array.
#   This is called "broadcasting" - the scalar is automatically extended to
#   match the array shape.
# - Syntax: array + scalar (or scalar + array - commutative)
# - Why use it: Essential for:
#   * Data normalization: shifting data by a constant
#   * Bias addition in neural networks: output = weights @ input + bias
#   * Feature scaling: adjusting ranges
#   * Temperature conversion: celsius + 273.15 = kelvin
#   * Offset corrections in data preprocessing
# - Broadcasting behavior:
#   * Scalar is treated as array of same shape with repeated value
#   * [1, 2, 3] + 2 becomes [1, 2, 3] + [2, 2, 2]
#   * Then element-wise addition is performed
# - All arithmetic operators work this way:
#   * Addition: arr + 5
#   * Subtraction: arr - 3
#   * Multiplication: arr * 2
#   * Division: arr / 4
#   * Power: arr ** 0.5
#   * Modulo: arr % 2
# - Common use cases:
#   * Data centering: data - mean
#   * Adding bias in neural networks
#   * Temperature conversions
#   * Shifting probability distributions
#   * Adjusting pixel intensities in images
# - Example:
#   # Centering data (zero-mean normalization)
#   data = np.array([10, 20, 30, 40, 50])
#   mean = np.mean(data)  # 30
#   centered = data - mean  # [-20, -10, 0, 10, 20]
#   
#   # Temperature conversion
#   celsius = np.array([0, 10, 20, 30, 100])
#   fahrenheit = celsius * 9/5 + 32  # [32, 50, 68, 86, 212]
#   
#   # Neural network bias addition
#   weighted_sum = np.array([0.5, -0.3, 0.8])
#   bias = 0.1
#   output = weighted_sum + bias  # [0.6, -0.2, 0.9]
#   
#   # Image brightness adjustment
#   image = np.random.rand(256, 256) * 255  # Grayscale image
#   brighter = image + 50  # Increase brightness by 50
#   brighter = np.clip(brighter, 0, 255)  # Keep in valid range
#
# HINGLISH:
# - Yeh kya karta hai: Array ke har element mein ek scalar (single number)
#   add karta hai. Ise "broadcasting" kehte hain - scalar automatically array
#   shape match karne ke liye extend ho jata hai.
# - Syntax: array + scalar (ya scalar + array - commutative hai)
# - Kab use karein: Zaroori hai:
#   * Data normalization: constant se data shift karna
#   * Neural networks mein bias addition: output = weights @ input + bias
#   * Feature scaling: ranges adjust karna
#   * Temperature conversion: celsius + 273.15 = kelvin
#   * Data preprocessing mein offset corrections
# - Broadcasting behavior:
#   * Scalar ko same shape ke array ki tarah treat kiya jata hai with repeated value
#   * [1, 2, 3] + 2 ban jata hai [1, 2, 3] + [2, 2, 2]
#   * Phir element-wise addition perform hota hai
# - Saare arithmetic operators aise hi kaam karte hain:
#   * Addition: arr + 5
#   * Subtraction: arr - 3
#   * Multiplication: arr * 2
#   * Division: arr / 4
#   * Power: arr ** 0.5
#   * Modulo: arr % 2
# - Common use cases:
#   * Data centering: data - mean
#   * Neural networks mein bias add karna
#   * Temperature conversions
#   * Probability distributions shift karna
#   * Images mein pixel intensities adjust karna
# - Example:
#   # Data center karna (zero-mean normalization)
#   data = np.array([10, 20, 30, 40, 50])
#   mean = np.mean(data)  # 30
#   centered = data - mean  # [-20, -10, 0, 10, 20]
#   
#   # Temperature conversion
#   celsius = np.array([0, 10, 20, 30, 100])
#   fahrenheit = celsius * 9/5 + 32  # [32, 50, 68, 86, 212]
#   
#   # Neural network mein bias addition
#   weighted_sum = np.array([0.5, -0.3, 0.8])
#   bias = 0.1
#   output = weighted_sum + bias  # [0.6, -0.2, 0.9]
#   
#   # Image brightness adjustment
#   image = np.random.rand(256, 256) * 255  # Grayscale image
#   brighter = image + 50  # Brightness 50 se increase karo
#   brighter = np.clip(brighter, 0, 255)  # Valid range mein rakho
# -----------------------------------------------------------------------------

print(arr + 2)
# Output: [3 4 5 6 7]
#
# Broadcasting visualization:
# [1, 2, 3, 4, 5] + 2
#  ↓  ↓  ↓  ↓  ↓     ↓ (scalar broadcasts to match array)
# [1, 2, 3, 4, 5] + [2, 2, 2, 2, 2]
#  ↓  ↓  ↓  ↓  ↓     ↓  ↓  ↓  ↓  ↓
# [3, 4, 5, 6, 7]
#
# The scalar 2 is added to each element

# =============================================================================
# OPERATION 3: ARRAY + ARRAY (SAME SHAPE)
# =============================================================================
# ENGLISH:
# - What it does: Performs element-wise addition of two arrays with the SAME
#   shape. Corresponding elements are added together.
# - Syntax: array1 + array2 (arrays must have compatible shapes)
# - Why use it: Fundamental operation in ML for:
#   * Combining predictions from multiple models (ensemble learning)
#   * Adding gradients in backpropagation
#   * Vector addition in linear algebra
#   * Merging feature vectors
#   * Computing residuals: actual - prediction
# - How it works:
#   * Arrays must have same shape or be broadcastable
#   * Elements at same positions are added
#   * Returns new array with same shape
#   * Much faster than Python loops
# - All element-wise operators work similarly:
#   * Addition: arr1 + arr2
#   * Subtraction: arr1 - arr2
#   * Multiplication: arr1 * arr2 (NOT matrix multiplication!)
#   * Division: arr1 / arr2
#   * Power: arr1 ** arr2
#   * Comparison: arr1 > arr2, arr1 == arr2
# - Common use cases:
#   * Ensemble predictions: (model1_pred + model2_pred) / 2
#   * Gradient accumulation in training
#   * Computing errors: predictions - targets
#   * Feature combinations: feature1 + feature2
#   * Vector arithmetic in embeddings
# - Example:
#   # Ensemble learning - averaging two models
#   model1_pred = np.array([0.8, 0.2, 0.6, 0.9])
#   model2_pred = np.array([0.7, 0.3, 0.5, 0.8])
#   ensemble_pred = (model1_pred + model2_pred) / 2
#   # [0.75, 0.25, 0.55, 0.85]
#   
#   # Computing prediction errors
#   predictions = np.array([2.5, 3.2, 4.1, 5.0])
#   actual = np.array([2.0, 3.5, 4.0, 5.2])
#   errors = predictions - actual  # [0.5, -0.3, 0.1, -0.2]
#   
#   # Gradient accumulation
#   grad_batch1 = np.array([0.1, 0.2, 0.3])
#   grad_batch2 = np.array([0.15, 0.18, 0.25])
#   total_grad = grad_batch1 + grad_batch2  # [0.25, 0.38, 0.55]
#   
#   # Vector addition
#   vec1 = np.array([1, 2, 3])
#   vec2 = np.array([4, 5, 6])
#   result = vec1 + vec2  # [5, 7, 9]
#
# HINGLISH:
# - Yeh kya karta hai: SAME shape wale do arrays ka element-wise addition
#   perform karta hai. Corresponding elements ko ek saath add kiya jata hai.
# - Syntax: array1 + array2 (arrays ka shape compatible hona chahiye)
# - Kab use karein: ML mein fundamental operation hai:
#   * Multiple models se predictions combine karna (ensemble learning)
#   * Backpropagation mein gradients add karna
#   * Linear algebra mein vector addition
#   * Feature vectors merge karna
#   * Residuals compute karna: actual - prediction
# - Kaise kaam karta hai:
#   * Arrays ka same shape hona chahiye ya broadcastable hona chahiye
#   * Same positions par elements add hote hain
#   * Same shape wala naya array return hota hai
#   * Python loops se bahut zyada fast hai
# - Saare element-wise operators aise hi kaam karte hain:
#   * Addition: arr1 + arr2
#   * Subtraction: arr1 - arr2
#   * Multiplication: arr1 * arr2 (matrix multiplication NAHI!)
#   * Division: arr1 / arr2
#   * Power: arr1 ** arr2
#   * Comparison: arr1 > arr2, arr1 == arr2
# - Common use cases:
#   * Ensemble predictions: (model1_pred + model2_pred) / 2
#   * Training mein gradient accumulation
#   * Errors compute karna: predictions - targets
#   * Feature combinations: feature1 + feature2
#   * Embeddings mein vector arithmetic
# - Example:
#   # Ensemble learning - do models average karna
#   model1_pred = np.array([0.8, 0.2, 0.6, 0.9])
#   model2_pred = np.array([0.7, 0.3, 0.5, 0.8])
#   ensemble_pred = (model1_pred + model2_pred) / 2
#   # [0.75, 0.25, 0.55, 0.85]
#   
#   # Prediction errors compute karna
#   predictions = np.array([2.5, 3.2, 4.1, 5.0])
#   actual = np.array([2.0, 3.5, 4.0, 5.2])
#   errors = predictions - actual  # [0.5, -0.3, 0.1, -0.2]
#   
#   # Gradient accumulation
#   grad_batch1 = np.array([0.1, 0.2, 0.3])
#   grad_batch2 = np.array([0.15, 0.18, 0.25])
#   total_grad = grad_batch1 + grad_batch2  # [0.25, 0.38, 0.55]
#   
#   # Vector addition
#   vec1 = np.array([1, 2, 3])
#   vec2 = np.array([4, 5, 6])
#   result = vec1 + vec2  # [5, 7, 9]
# -----------------------------------------------------------------------------

print(arr + arr2)
# Output: [ 6  8 10 12 14]
#
# Element-wise addition:
# [1, 2, 3, 4,  5]  (arr)
# +
# [5, 6, 7, 8,  9]  (arr2)
# ↓  ↓  ↓  ↓   ↓
# [6, 8, 10, 12, 14]
#
# Each pair of corresponding elements is added:
# 1+5=6, 2+6=8, 3+7=10, 4+8=12, 5+9=14

# =============================================================================
# OPERATION 4: BROADCASTING - ARRAY + ARRAY (DIFFERENT SHAPES)
# =============================================================================
# ENGLISH:
# - What it does: Adds arrays of DIFFERENT but compatible shapes by
#   automatically "broadcasting" the smaller array to match the larger one.
# - Syntax: array1 + array2 (where shapes are compatible for broadcasting)
# - Why use it: EXTREMELY powerful feature that makes code concise and fast:
#   * Adding bias to each sample in a batch
#   * Normalizing each feature independently
#   * Image operations across channels
#   * Batch processing without explicit loops
# - Broadcasting rules (NumPy automatically applies):
#   1. If arrays have different number of dimensions, prepend 1s to smaller
#   2. Arrays are compatible if for each dimension:
#      - They are equal, OR
#      - One of them is 1
#   3. The smaller array is "stretched" to match larger array's shape
# - How it works for arr (shape 5,) + arr3 (shape 2,5):
#   Step 1: arr shape (5,) becomes (1, 5) - prepend dimension
#   Step 2: arr (1, 5) broadcasts to (2, 5) - repeat along axis 0
#   Step 3: Element-wise addition with arr3 (2, 5)
# - Broadcasting examples with different shapes:
#   (3, 1) + (3, 4) → (3, 4)  ✓ Compatible
#   (5,) + (5, 3) → (5, 3)    ✓ Compatible
#   (3, 1) + (1, 4) → (3, 4)  ✓ Compatible
#   (3, 2) + (3, 4) → Error   ✗ Incompatible
# - Common use cases:
#   * Adding bias to batch: batch_output + bias_vector
#   * Feature normalization: (X - mean) / std (mean & std per feature)
#   * Image processing: img + color_adjustment (per channel)
#   * Matrix operations with row/column vectors
# - Example:
#   # Adding bias to each sample in batch
#   batch = np.array([[1, 2, 3],
#                     [4, 5, 6],
#                     [7, 8, 9]])  # shape: (3, 3) - 3 samples, 3 features
#   bias = np.array([0.1, 0.2, 0.3])  # shape: (3,) - one bias per feature
#   output = batch + bias  # Broadcasting: (3,3) + (3,) → (3,3)
#   # [[1.1, 2.2, 3.3],
#   #  [4.1, 5.2, 6.3],
#   #  [7.1, 8.2, 9.3]]
#   
#   # Feature normalization (mean & std per feature)
#   X = np.array([[1, 10, 100],
#                 [2, 20, 200],
#                 [3, 30, 300]])  # shape: (3, 3)
#   mean = np.array([2, 20, 200])  # shape: (3,) - mean per feature
#   std = np.array([1, 10, 100])   # shape: (3,) - std per feature
#   X_normalized = (X - mean) / std  # Broadcasting both operations
#   
#   # Image RGB adjustment
#   img = np.random.rand(256, 256, 3)  # shape: (256, 256, 3)
#   rgb_adjust = np.array([1.1, 1.0, 0.9])  # shape: (3,) - R,G,B adjustments
#   adjusted = img * rgb_adjust  # Broadcasting: (256,256,3) * (3,)
#
# HINGLISH:
# - Yeh kya karta hai: DIFFERENT lekin compatible shapes wale arrays ko
#   automatically chhote array ko bade array ke shape mein "broadcast" karke
#   add karta hai.
# - Syntax: array1 + array2 (jahan shapes broadcasting ke liye compatible hain)
# - Kab use karein: BAHUT powerful feature jo code ko concise aur fast banata hai:
#   * Batch mein har sample mein bias add karna
#   * Har feature ko independently normalize karna
#   * Channels across image operations
#   * Explicit loops ke bina batch processing
# - Broadcasting rules (NumPy automatically apply karta hai):
#   1. Agar arrays ke dimensions alag hain, to chhote mein 1s prepend karo
#   2. Arrays compatible hain agar har dimension ke liye:
#      - Wo equal hain, YA
#      - Unme se ek 1 hai
#   3. Chhota array bade array ke shape match karne ke liye "stretched" hota hai
# - arr (shape 5,) + arr3 (shape 2,5) ke liye kaise kaam karta hai:
#   Step 1: arr ka shape (5,) ban jata hai (1, 5) - dimension prepend
#   Step 2: arr (1, 5) broadcast hota hai (2, 5) mein - axis 0 ke along repeat
#   Step 3: arr3 (2, 5) ke saath element-wise addition
# - Alag shapes ke saath broadcasting examples:
#   (3, 1) + (3, 4) → (3, 4)  ✓ Compatible
#   (5,) + (5, 3) → (5, 3)    ✓ Compatible
#   (3, 1) + (1, 4) → (3, 4)  ✓ Compatible
#   (3, 2) + (3, 4) → Error   ✗ Incompatible
# - Common use cases:
#   * Batch mein bias add karna: batch_output + bias_vector
#   * Feature normalization: (X - mean) / std (har feature ke liye mean & std)
#   * Image processing: img + color_adjustment (har channel ke liye)
#   * Row/column vectors ke saath matrix operations
# - Example:
#   # Batch mein har sample mein bias add karna
#   batch = np.array([[1, 2, 3],
#                     [4, 5, 6],
#                     [7, 8, 9]])  # shape: (3, 3) - 3 samples, 3 features
#   bias = np.array([0.1, 0.2, 0.3])  # shape: (3,) - har feature ke liye ek bias
#   output = batch + bias  # Broadcasting: (3,3) + (3,) → (3,3)
#   # [[1.1, 2.2, 3.3],
#   #  [4.1, 5.2, 6.3],
#   #  [7.1, 8.2, 9.3]]
#   
#   # Feature normalization (har feature ke liye mean & std)
#   X = np.array([[1, 10, 100],
#                 [2, 20, 200],
#                 [3, 30, 300]])  # shape: (3, 3)
#   mean = np.array([2, 20, 200])  # shape: (3,) - har feature ka mean
#   std = np.array([1, 10, 100])   # shape: (3,) - har feature ka std
#   X_normalized = (X - mean) / std  # Dono operations broadcast hote hain
#   
#   # Image RGB adjustment
#   img = np.random.rand(256, 256, 3)  # shape: (256, 256, 3)
#   rgb_adjust = np.array([1.1, 1.0, 0.9])  # shape: (3,) - R,G,B adjustments
#   adjusted = img * rgb_adjust  # Broadcasting: (256,256,3) * (3,)
# -----------------------------------------------------------------------------

print(arr + arr3)
# Output:
# [[2 4 6 8 10]
#  [3 5 7 9 11]]
#
# Broadcasting visualization:
# arr shape:  (5,)   → treated as (1, 5) → broadcast to (2, 5)
# arr3 shape: (2, 5) → stays as (2, 5)
#
# Step-by-step:
# Original arr:  [1, 2, 3, 4, 5]  (shape: 5,)
# 
# After broadcasting arr to (2, 5):
# [[1, 2, 3, 4, 5],   ← arr broadcasted (repeated)
#  [1, 2, 3, 4, 5]]   ← same row repeated
# 
# arr3:
# [[1, 2, 3, 4, 5],   ← Row 0
#  [2, 3, 4, 5, 6]]   ← Row 1
# 
# Element-wise addition:
# [[1+1, 2+2, 3+3, 4+4, 5+5],
#  [1+2, 2+3, 3+4, 4+5, 5+6]]
# 
# Result:
# [[2, 4, 6, 8, 10],
#  [3, 5, 7, 9, 11]]

# =============================================================================
# ALL VECTORIZED ARITHMETIC OPERATIONS
# =============================================================================
# ENGLISH:
# NumPy supports all arithmetic operations in vectorized form:
#
# # Addition
# arr + 5          # Add scalar to all elements
# arr1 + arr2      # Element-wise addition
#
# # Subtraction
# arr - 3          # Subtract scalar from all elements
# arr1 - arr2      # Element-wise subtraction
#
# # Multiplication (element-wise, NOT matrix multiplication)
# arr * 2          # Multiply all elements by scalar
# arr1 * arr2      # Element-wise multiplication
#
# # Division
# arr / 4          # Divide all elements by scalar
# arr1 / arr2      # Element-wise division
#
# # Floor division
# arr // 3         # Integer division
#
# # Modulo
# arr % 2          # Remainder after division
#
# # Exponentiation
# arr ** 2         # Square all elements
# arr ** 0.5       # Square root (same as np.sqrt(arr))
# arr1 ** arr2     # Element-wise power
#
# # Comparison operations (return boolean arrays)
# arr > 3          # [False, False, False, True, True]
# arr == 3         # [False, False, True, False, False]
# arr1 < arr2      # Element-wise comparison
#
# # Logical operations
# (arr > 2) & (arr < 5)   # AND operation
# (arr < 2) | (arr > 4)   # OR operation
# ~(arr > 3)              # NOT operation
#
# HINGLISH:
# NumPy saare arithmetic operations ko vectorized form mein support karta hai:
#
# # Addition
# arr + 5          # Saare elements mein scalar add karo
# arr1 + arr2      # Element-wise addition
#
# # Subtraction
# arr - 3          # Saare elements se scalar subtract karo
# arr1 - arr2      # Element-wise subtraction
#
# # Multiplication (element-wise, matrix multiplication NAHI)
# arr * 2          # Saare elements ko scalar se multiply karo
# arr1 * arr2      # Element-wise multiplication
#
# # Division
# arr / 4          # Saare elements ko scalar se divide karo
# arr1 / arr2      # Element-wise division
#
# # Floor division
# arr // 3         # Integer division
#
# # Modulo
# arr % 2          # Division ke baad remainder
#
# # Exponentiation
# arr ** 2         # Saare elements ko square karo
# arr ** 0.5       # Square root (np.sqrt(arr) jaisa hi)
# arr1 ** arr2     # Element-wise power
#
# # Comparison operations (boolean arrays return karte hain)
# arr > 3          # [False, False, False, True, True]
# arr == 3         # [False, False, True, False, False]
# arr1 < arr2      # Element-wise comparison
#
# # Logical operations
# (arr > 2) & (arr < 5)   # AND operation
# (arr < 2) | (arr > 4)   # OR operation
# ~(arr > 3)              # NOT operation
# =============================================================================

# =============================================================================
# PERFORMANCE COMPARISON: LOOP vs VECTORIZATION
# =============================================================================
# ENGLISH:
# Demonstrating why vectorization is crucial for ML:
#
# import time
# 
# # Create large array
# large_arr = np.arange(1000000)
# 
# # Method 1: Python loop (SLOW)
# start = time.time()
# result_loop = []
# for x in large_arr:
#     result_loop.append(x ** 2)
# loop_time = time.time() - start
# print(f"Loop time: {loop_time:.4f} seconds")
# 
# # Method 2: List comprehension (STILL SLOW)
# start = time.time()
# result_comp = [x ** 2 for x in large_arr]
# comp_time = time.time() - start
# print(f"Comprehension time: {comp_time:.4f} seconds")
# 
# # Method 3: NumPy vectorization (FAST!)
# start = time.time()
# result_vec = large_arr ** 2
# vec_time = time.time() - start
# print(f"Vectorization time: {vec_time:.4f} seconds")
# 
# # Typical results:
# # Loop time: 0.2500 seconds
# # Comprehension time: 0.1800 seconds
# # Vectorization time: 0.0020 seconds  ← 100x faster!
#
# HINGLISH:
# Demonstrate kar rahe hain ki vectorization ML ke liye kyun crucial hai:
#
# import time
# 
# # Bada array banao
# large_arr = np.arange(1000000)
# 
# # Method 1: Python loop (SLOW)
# start = time.time()
# result_loop = []
# for x in large_arr:
#     result_loop.append(x ** 2)
# loop_time = time.time() - start
# print(f"Loop time: {loop_time:.4f} seconds")
# 
# # Method 2: List comprehension (ABHI BHI SLOW)
# start = time.time()
# result_comp = [x ** 2 for x in large_arr]
# comp_time = time.time() - start
# print(f"Comprehension time: {comp_time:.4f} seconds")
# 
# # Method 3: NumPy vectorization (FAST!)
# start = time.time()
# result_vec = large_arr ** 2
# vec_time = time.time() - start
# print(f"Vectorization time: {vec_time:.4f} seconds")
# 
# # Typical results:
# # Loop time: 0.2500 seconds
# # Comprehension time: 0.1800 seconds
# # Vectorization time: 0.0020 seconds  ← 100x faster!
# =============================================================================

# =============================================================================
# PRACTICAL ML EXAMPLE: GRADIENT DESCENT STEP
# =============================================================================
# ENGLISH:
# Real-world scenario: One step of gradient descent (vectorized)
#
# # Hyperparameters
# learning_rate = 0.01
# 
# # Model parameters (weights)
# weights = np.array([0.5, -0.3, 0.8, 0.2])
# 
# # Computed gradients from backpropagation
# gradients = np.array([0.1, -0.05, 0.15, 0.02])
# 
# # Gradient descent update (fully vectorized - no loops!)
# weights = weights - learning_rate * gradients
# # [0.499, -0.2995, 0.7985, 0.1998]
# 
# # Without vectorization, you'd need:
# # for i in range(len(weights)):
# #     weights[i] = weights[i] - learning_rate * gradients[i]
# # Much slower and less readable!
#
# HINGLISH:
# Real-world scenario: Gradient descent ka ek step (vectorized)
#
# # Hyperparameters
# learning_rate = 0.01
# 
# # Model parameters (weights)
# weights = np.array([0.5, -0.3, 0.8, 0.2])
# 
# # Backpropagation se computed gradients
# gradients = np.array([0.1, -0.05, 0.15, 0.02])
# 
# # Gradient descent update (fully vectorized - loops nahi!)
# weights = weights - learning_rate * gradients
# # [0.499, -0.2995, 0.7985, 0.1998]
# 
# # Vectorization ke bina, aapko yeh karna padega:
# # for i in range(len(weights)):
# #     weights[i] = weights[i] - learning_rate * gradients[i]
# # Bahut slower aur kam readable!
# =============================================================================


# normalization

arr = np.array([[1,2], 
               [3,4]])
mean = np.mean(arr)
std_dev = np.std(arr)
normalized_arr = (arr - mean)/ std_dev
print(normalized_arr)
print(mean)
print(std_dev)



# mathematical fnx

# aggregation

arr = np.array([1,2,3,4,5,6])

print(np.sum(arr))
print(np.prod(arr))
print(np.min(arr))
print(np.max(arr))
print(np.mean(arr))
print(np.argmin(arr))
print(np.argmax(arr))
print(np.std(arr))
print(np.median(arr))
print(np.var(arr))



#power fun
arr = np.array([1,2,3,4,5,6])
print(np.square(arr))
print(np.sqrt(arr))







#log
arr = np.array([1,2,3,4,5,6])
print(np.log(arr))
print(np.exp(arr))



#rounding
print(np.round(3.13))
print(np.floor(3.44))
print(np.trunc(3.22))

arr = np.array([1,2,3,4,5,6,6,-2])
print(np.unique(arr))
print(np.sort(arr))
print(np.abs(arr))




